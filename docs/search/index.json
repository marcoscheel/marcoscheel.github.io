[{"content":"With the Message Center Notification MC234048 Microsoft announced a change to the Microsoft Teams App \u0026ldquo;Incoming Webhook\u0026rdquo;. The URL currently used will be deprecated by mid of April 2021. The exact wording is:\n We will begin transitioning to the new webhook URLs on Monday January 11, 2021; however, existing webhooks URLs will continue to work for three (3) months to allow for migration time Source (as of 2021-01-26): https://admin.microsoft.com/Adminportal/Home?#/MessageCenter/:/messages/MC234048\n If you created a webhook prior January 11, 2021 you will need to update your existing connector configuration!\nThis app is in regular use by most companies, if not disabled by a Teams App permission policy in the tenant. The app is a very easy option to post a message to a team. The URI of a webhook is cryptic and the only security in place. If you send a well-crafted HTTP message to the endpoint, you will create a Teams post in the channel the app is connected to. Here is the Microsoft documentation and a great community article.\nCurrently Microsoft is using a non-tenant specific URI (outlook.office.com). The new URI will be tenant related (YOURTENANT.webhook.office.com).\nThis feature is communicated for Microsoft Teams, but it is also a Microsoft 365 Group Connector feature so these might also affected.\nCheck if the app is used It could be a good idea to check, if the app is active in your tenant. As a Teams administrator you can check the application in your admin center.\nEven if you checked the app and the Teams App Permission policy you could still have the app installed prior this configuration. It is easy to check if the application is installed in a Microsoft Team. To query for installed apps we will need to use the preview version of the MicrosoftTeams module (as of writing 1.1.10-preview). Using the Teams PowerShell you can get a list of Teams the app is installed in.\nGet the application ID and more details:\nGet-TeamsApp | Where-Object { $_.DisplayName -eq \u0026quot;Incoming Webhook\u0026quot;}\r Result:\nExternalId Id DisplayName DistributionMethod\r---------- -- ----------- ------------------\r203a1e2c-26cc-47ca-83ae-be98f960b6b2 Incoming Webhook store\r With the application id we now can query all teams and check if the app is installed:\nGet-Team | ForEach-Object {\r$team = $_;\r$apps = Get-TeamsAppInstallation -TeamId $team.GroupId | Where-Object { $_.TeamsAppId -eq \u0026quot;203a1e2c-26cc-47ca-83ae-be98f960b6b2\u0026quot;};\rif ($apps -ne $null){\r$team;\r}\r}\r Result for my two teams with the app installed:\nGroupId DisplayName Visibility Archived MailNickName Description\r------- ----------- ---------- -------- ------------ -----------\ra6687ed4-c1a6-4c7b-9171-2d625a60b76e GK Malachor MSDN Public False GKMalachorMSDN Check here for or…\r75366f42-6fc6-4857-90d1-3283236789b6 20200906 Demo Acc… Private False 20200906DemoAcces… 20200906 Demo Acc…\r Based on this information we now can contact the owners/members of a team and make them check if they use the app and need to update the URI. Currently I am not aware of a method to get the specific channel the webhook is attached to. The user needs to check all the channels to find the connectors.\nHow to fix the problem The user needs to navigate to the team and check for the connector of all channels:\nOpen the \u0026ldquo;x configured\u0026rdquo; (1) if available and click on the \u0026ldquo;Manage\u0026rdquo; (2) button for the specific implementation:\nThis will show you the current configuration of the webhook:\nYou need to click on \u0026ldquo;Update URL\u0026rdquo; and you will receive a new URI with the tenant specific part. The connector page did not refresh automatically. I quite the page and reopened the dialog. Now the page is not complaining about a required update and I could copy the new webhook URI:\nNow you just need to remember and find the app you integrated the webhook in :)\nNOTE: I was not able to update the incoming webhook, if the account that created the webhook is not the account updating the webhook. You can see the account that did the setup in the connector list and you will notice the \u0026ldquo;Save\u0026rdquo; button is disabled. In this case an easy option is to delete webhook and recreate it with the same name.\nSummary Check your tenant (admin) or teams (power users) for the configuration of incoming webhook. Remember as soon as you update the URL the webhook for this will stop working and not accept messages. Updating the URL is only solving 50% of the problem. You also need to update your Power Automate flows, Azure Functions, Azure Automation Runbooks or your PowerShell scripts in your on-prem servers task scheduler.\nBonus Get the owners of the groups to send an email:\nGet-Team | ForEach-Object {\r$team = $_;\r$apps = Get-TeamsAppInstallation -TeamId $team.GroupId | Where-Object { $_.TeamsAppId -eq \u0026quot;203a1e2c-26cc-47ca-83ae-be98f960b6b2\u0026quot;};\rif ($apps -ne $null){\rGet-TeamUser -GroupId $team.GroupId -Role Owner | ForEach-Object {\r$owner = $_;\r$fields = @{\rTeam = $team.DisplayName\rOwnerEmail = $owner.User\r}\rNew-Object -TypeName PSObject -Property $fields;\r}\r}\r}\r Result: ","date":"2021-01-27","permalink":"https://marcoscheel.de/post/2021/01/20210127-microsoftteams-webhookupdate/","tags":["Microsoft 365","Microsoft Teams","Development"],"title":"Microsoft Teams Incoming Webhook update required"},{"content":"If you are using Azure AD authentication for your scripts, apps, or other scenarios at some point you will end up creating your own application in your directory. Normally you open the Azure portal and navigate to the \u0026ldquo;App registrations\u0026rdquo; part of AAD. This is fine during development, but if you want to share the solution or a customer wants to run the software in their own tenant, things get complicated and error prone. For my Microsoft Teams backup solution this is very real because you need to hit all required permissions and configure the public client part otherwise the solution will not run.\nThis post provides you will all the needed information to create your own script. I\u0026rsquo;m using my M365 Teams Backup solution as a reference. The key components are:\n Install the Azure CLI or the Azure AD Preview PowerShell module The scripts to setup the app  create-aadapp-cli.ps1 via Azure CLI or create-aadapp.ps1 via PowerShell    Choose a scripting environment (Azure CLI vs Azure AD PS) During my day job I created some applications based on Microsoft Graph and I tried a few approaches to script the Azure AD app creation. It is important to understand that an Azure AD application consists of two parts. The application registration is like a blueprint for your app. The enterprise application is the implementation of your blueprint.\nThe application permissions are defined in the \u0026ldquo;App registration\u0026rdquo;. Here you select the permissions that your app will request from users in the tenant. Without a consent the permissions are not in effect. If your only have an app registered but not received consent the app will not be able to use the requested permissions. Check the Microsoft documentation for a deeper look at the consent framework.\nMost of my applications leverage application permissions or require admin consent for delegate permissions. The �M365.TeamsBackup� solution is using a bunch of Microsoft Graph permissions and some of them are pretty powerful. If you have an application with this kind of permission requirements it is needed to have admin consent given by a (best case) global administrator.\nIf your apps are like mine, it might be the best to use the Azure CLI because this is as of my knowledge the only way to script the admin consent. I am not a CLI guy. I am a PowerShell fan. I struggled in the past integrating the CLI and its output into my scripting flow. That is why I wanted to show you what and how it can be done. If you are OK with opening the portal to give admin consent or you don\u0026rsquo;t want to give admin consent during application setup, I also have an Azure AD PowerShell version of the script.\nSetup Azure CLI and connect The Azure CLI is not purely targeted at Azure AD. It is the other way around because the CLI is used to script all the Azure things available. There is great Microsoft docs on installing the Azure CLI. I\u0026rsquo;m running on Windows, so I typically go the MSI route:\n Download the release version of the MSI (that is what I\u0026rsquo;m running) Install the MSI (bring some extra time because the installation is slow) After the download open a new PowerShell (this ensured the path is set and available) You can check if the installation worked using the \u0026lsquo;az \u0026ndash;version\u0026rsquo; command  As you can see my version is not up to date. As with most tools you need to keep these tools at the latest version. The Azure CLI can be updated by installing the newest MSI or by running an admin command using this command \u0026lsquo;az upgrade\u0026rsquo;. The upgrade command will download the MSI and start the installation for you.\nInstallation is finish and now it is time to login to your tenant. The CLI is different from your normal \u0026ldquo;PowerShell Connect-SERVICE\u0026rdquo; (SharePoint, AD, Teams, \u0026hellip;) command. The Azure CLI will remember your last login. If you close and open your terminal you will still be logged in. If you use the Azure CLI just for the one-time setup, please consider a logout after you finish any script. But first lets login. I\u0026rsquo;m a big fan of device code authentication where possible. Azure CLI is supporting this flow so that is how I roll:\n Login:  az login \u0026ndash;use-device-code \u0026ndash;allow-no-subscriptions   Check current login:  az account show   Logout:  az logout    Check my script using the Azure CLI:\n create-aadapp-cli.ps1  Setup Azure AD PowerShell and connect Azure AD PowerShell versioning is complicated. For my job (M365 Modern Collaboration) I am always using the AzureADPreview module and this is what I recommend in most cases. The AzureADPreview cannot be installed side by side with the AzureADPreview, so at some point you will have to move to the AzureADPreview. As the Azure CLI is not PowerShell based I am using my Windows Terminal default that is PowerShell 7. The Azure AD modules are not yet ready for PowerShell 7 so you will need to open your old school PowerShell 5.\nInstalling the Azure AD module is like most modern modules and relies on the Powershell gallery.\n Open your PowerShell as an administrator and execute  Install-module AzureADPreview   Check your version opening a non admin session  Get-Module AzureADPreview -ListAvailable    If you are not on the latest version, you need to upgrade the module like any other module:\n Open your PowerShell as an administrator and execute  Update-Module AzureADPreview    To connect to Azure AD you cannot rely on device authentication and you will need to login directly on the script execution. If you need to execute multiple scripts, check if you want to disable the login command \u0026ldquo;Connect-AzureAD\u0026rdquo; in the script to prevent multiple logins (incl. MFA).\n Open your PowerShell and execute  Connect-AzureAD    Check my script using the Azure CLI:\n create-aadapp.ps1  Script the creation process Now we are prepared, and we can create our application. The easy part is to create an \u0026ldquo;App registration\u0026rdquo;. If your app need permissions the trouble begins. There are two challenges:\n Setting the permission in the two scripts Getting the permission definition in the first place  Getting the permission translated from the nice Azure AD portal UX to a script-ready solution is harder to research than expected. I\u0026rsquo;ve done a blog post (in German) about this in the past. In the next section I will show you how to get the ID of the Microsoft Graph application and the IDs of the required permissions.\nMy Teams Backup solution requires many permissions from the Microsoft Graph (this time delegation because app permissions for some require Microsoft approval) so let�s have a look at a non-error prone implementation that is also easy to read and extend.\nAzure CLI For reference: create-aadapp-cli.ps1 Microsoft docs: az command overview\nUse the Azure CLI to query the Azure Active Directory for the service principal with the name of the Microsoft Graph.\n$servicePrincipalName = \u0026quot;Microsoft Graph\u0026quot;;\r$servicePrincipalId = az ad sp list --filter \u0026quot;displayname eq '$servicePrincipalName'\u0026quot; --query '[0].appId' | ConvertFrom-Json\r Using the query parameter we select the first result (there is only one Microsoft Graph) and \u0026ldquo;cast\u0026rdquo; the app ID. Using the ConvertFrom-JSON makes it easy to parse the result and we receive \u0026ldquo;00000003-0000-0000-c000-000000000000\u0026rdquo; as the value for the app id.\nNext, we need to get the ID for each required permission. This info is part of the \u0026ldquo;oauth2Permissions\u0026rdquo; property from the MS Graph service principal:\n$servicePrincipalNameOauth2Permissions = @(\u0026quot;Channel.ReadBasic.All\u0026quot;, \u0026quot;ChannelMember.Read.All\u0026quot;, \u0026quot;ChannelMessage.Read.All\u0026quot;, \u0026quot;ChannelSettings.Read.All\u0026quot;, \u0026quot;Group.Read.All\u0026quot;, \u0026quot;GroupMember.Read.All\u0026quot;, \u0026quot;Team.ReadBasic.All\u0026quot;, \u0026quot;TeamMember.Read.All\u0026quot;, \u0026quot;TeamSettings.Read.All\u0026quot;, \u0026quot;TeamsTab.Read.All\u0026quot;);\r(az ad sp show --id $servicePrincipalId --query oauth2Permissions | ConvertFrom-Json) | ? { $_.value -in $servicePrincipalNameOauth2Permissions} | % {\r$permission = $_\r$delPermission = @{\rid = $permission.Id\rtype = \u0026quot;Scope\u0026quot;\r}\r$reqGraph.resourceAccess += $delPermission\r}\r Using the \u0026ldquo;-in\u0026rdquo; filter we receive all specified entries for the array we need. To use the IDs in the next command the script creates a hashtable that can be converted in the needed JSON file (correct a file). The permissions are added as \u0026ldquo;Scope\u0026rdquo; representing \u0026ldquo;Delegation\u0026rdquo; permission. The \u0026ldquo;az ad app create\u0026rdquo; command will require a file with the permissions.\nSet-Content ./required_resource_accesses.json -Value (\u0026quot;[\u0026quot; + ($reqGraph | ConvertTo-Json) + \u0026quot;]\u0026quot;)\r$newapp = az ad app create --display-name $appName --available-to-other-tenants false --native-app true --required-resource-accesses `@required_resource_accesses.json | ConvertFrom-Json\r This creates an app that is only valid in your tenant \u0026ldquo;\u0026ndash;available-to-other-tenants false\u0026rdquo; and allows the login as a public client \u0026ldquo;\u0026ndash;native-app true\u0026rdquo;. The result is a JSON representing the new application.\nThe benefit of using the Azure CLI is the possibility to grant admin consent for the newly created app\naz ad app permission admin-consent --id $newapp.appId\r PowerShell with Azure AD For reference: create-aadapp.ps1 Microsoft docs: Azure AD Application command overview\nTo get the ID for the Microsoft Graph Service principal we query the current directory and filter to the display name.\n$servicePrincipalName = \u0026quot;Microsoft Graph\u0026quot;;\r$servicePrincipal = Get-AzureADServicePrincipal -All $true | ? { $_.DisplayName -eq $servicePrincipalName };\r Where the Azure CLI requires a file to setup permission, the PowerShell version requires a .NET object. The Microsoft Graph service principal ID is the ResourceAppID.\n$reqGraph = New-Object -TypeName \u0026quot;Microsoft.Open.AzureAD.Model.RequiredResourceAccess\u0026quot;;\r$reqGraph.ResourceAppId = $servicePrincipal.AppId;\r From the returned object we can select the \u0026ldquo;Oauth2Permissions\u0026rdquo; property to filter on our array with the required permissions. For each permission another .NET object is created and added to the collection named \u0026ldquo;ResourceAccess\u0026rdquo;.\n$servicePrincipalNameOauth2Permissions = @(\u0026quot;Channel.ReadBasic.All\u0026quot;, \u0026quot;ChannelMember.Read.All\u0026quot;, \u0026quot;ChannelMessage.Read.All\u0026quot;, \u0026quot;ChannelSettings.Read.All\u0026quot;, \u0026quot;Group.Read.All\u0026quot;, \u0026quot;GroupMember.Read.All\u0026quot;, \u0026quot;Team.ReadBasic.All\u0026quot;, \u0026quot;TeamMember.Read.All\u0026quot;, \u0026quot;TeamSettings.Read.All\u0026quot;, \u0026quot;TeamsTab.Read.All\u0026quot;);\r$servicePrincipal.Oauth2Permissions | ? { $_.Value -in $servicePrincipalNameOauth2Permissions} | % {\r$permission = $_\r$delPermission = New-Object -TypeName \u0026quot;Microsoft.Open.AzureAD.Model.ResourceAccess\u0026quot; -ArgumentList $permission.Id,\u0026quot;Scope\u0026quot; #delegate permission (oauth) are always \u0026quot;Scope\u0026quot;\r$reqGraph.ResourceAccess += $delPermission\r}\r Now it is time to setup the application (only in this directory and as public client) and retrieve the ID of the new app:\nNew-AzureADApplication -DisplayName $appName -AvailableToOtherTenants:$false -PublicClient:$true -RequiredResourceAccess $reqGraph;\r\u0026quot;ClientId: \u0026quot; + $newapp.AppId;\r\u0026quot;TenantId: \u0026quot; + (Get-AzureADTenantDetail).ObjectId;\r\u0026quot;Check AAD app: https://portal.azure.com/#blade/Microsoft_AAD_RegisteredApps/ApplicationMenuBlade/CallAnAPI/appId/\u0026quot; + $newapp.AppId + \u0026quot;/objectId/\u0026quot; + $newapp.ObjectId + \u0026quot;/isMSAApp/\u0026quot;;\r The last line creates a link to the Azure AD to grant admin consent.\nSummary You can check out my linked solution to get the full picture from the client code using the app to the setup required for authentication. I would recommend checking out the Azure CLI because it is the most complete solution even though it does not feel natural to me as a PowerShell guy. The example should give you an idea how to get the needed IDs and how to constructed the required objects/file to create the app. Let me know how you setup Azure AD apps and if there are other options. I\u0026rsquo;ve ignored the PowerShell AZ module because you are not able to grant admin consent too and chances are higher you may have AzureAD PowerShell installed already.\n","date":"2021-01-25","permalink":"https://marcoscheel.de/post/2021/01/20210124-m365teamsbackup-aadapp/","tags":["Microsoft 365","Microsoft Teams","Development","Microsoft Graph","Azure AD"],"title":"Create your Azure AD application via script - M365.TeamsBackup"},{"content":"I have sat down for four weekends in a row to come up with a solution for two problem I encountered in the past:\n I wanted to save all images from a beautiful Teams channel message. Teams is saving these inline images not to SharePoint. The only way to download is to click on each image. We are doing a Tenant to Tenant migration at glueckkanja-gab after our merger. Most of the migration tools will support the migration of Teams chat, but not all tools are available, and some implementations are lacking features for a more flexible approach.  I started my career as a developer and my heart is still thinking Visual Basic. Do not be afraid, I migrated my dev skills to C# a long time ago and my array starts at 0 not 1.\nI really wanted to try a few things regarding Microsoft Graph, the Microsoft Graph SDK and Microsoft Teams. The Microsoft Teamwork part of the API has a solid starting point (if you look at the beta version). The API is getting very mature set of capabilities. I\u0026rsquo;m a huge fan of Azure Functions and I\u0026rsquo;ve done quite a few projects that are talking to the Microsoft Graph using Application Permissions. I\u0026rsquo;ve checked the documentation and if I wanted to go this route I would have to request special permissions from Microsoft to access the content without a real user. For now I decided to go with a console application and a Azure AD Device Code flow.\nI have published the source code at GitHub. Maybe this will get your own solution a kickstart. Just a quick disclaimer: A lot of this stuff is first time code for me (DI in a console, Graph Auth provider, logging, \u0026hellip;). I think at some points I over-engineered the solution and I got distracted from my real business problems ;)\nhttps://github.com/marcoscheel/M365.TeamsBackup\nIn the following sections I will show you how I approached the problem, how the result of the backup looks, how to setup and how to run it for yourself.\nCheck your migration vendor of choice first! I have only limited experience with the following tools. But these vendors are the big players, and you can easily get in touch with them to get a demo or further information.\n AvePoint Quest ShareGate  It is ok if you stop here if your migration needs are satisfied by one of these vendors. If you are interested in my approach it might still be worth reading on ;)\nMy goal Since the beginning of the first lockdown in April we at Glück \u0026amp; Kanja did a cooking event for a social online gathering. We are still doing it and we created a lot of content since the first meeting. The event is hosted on Microsoft Teams Channel meeting every Monday. Check out our blog post (german). The event created a ton of beautifully pictures but they were stuck inside a Teams thread without easy access like SharePoint (if you consider SharePoint an easy access method). The backup should save all the attachments to the file system. Sneak preview of the result: For our migration to the new tenant, we are running a set of tools. The tooling we have at hand has some limits regarding channel message migration. We wanted to preserve that chat messages for a Team without polluting the new target team. In some cases, we also want to archive the Team to a central location (a single SharePoint site collection) for archiving purposes. For this case we would need to export the chat to some readable and searchable format preserving most of the content.\nMy goal is not to provide a ready to use backup solution. For some of your scenarios this could work. I wanted to also provide a code base to accommodate your more specific requirements. We don\u0026rsquo;t rely on extensive app integration in our teams, so handling the adaptive cards in a chat message is not really a part of the HTML generation solution. But the JSON from the Microsoft Graph will have all or some information included. If you need to handle adaptive card content this solution could be a great kickstart so you don\u0026rsquo;t have to write a lot of boilerplate code to get to the message attachment properties.\nThe solution will try to preserve as much information as needed from the Microsoft Graph and dump it to disk. From this data you can run multiple HTML conversions to address the need for the content representation. The HTML generation is no longer interacting with the Microsoft Graph and could be reparsed month after the original content was deleted (decommissioning of the old tenant).\nMy current focus is on the following data:\n Basic team metadata including members Channel metadata including members for private channels Messages and replies with author and dates Message body and inline pictures (hosted content)  My approach I mentioned it a few times. I split the solution in two parts. The first part is talking to the Microsoft Graph and storing the response to a JSON file. For every Team I create a folder with the ID of the group. The response of the team request will be stored in the folder with the name \u0026ldquo;team.json\u0026rdquo;. For every channel I create a folder with the channel ID and the channel response will be stored as \u0026ldquo;channel.json\u0026rdquo;. For every message (entry point of a thread if replies are available) I create a folder with the message ID and the message response will be stored as \u0026ldquo;message.json\u0026rdquo;. For every reply to a message I create another file with the pattern \u0026ldquo;message.{messageid}.json\u0026rdquo;. Every message will be checked if inline content is available. This is called hosted content and I treat every item as an inline image represented as an PNG-file. Because the ID for a hosted content item is very long, I decided to use an MD5 hash of the ID to use in the filename. For the root message the file is named \u0026ldquo;hostedcontent.{hostedcontenidMD5}.png\u0026rdquo; and for replies \u0026ldquo;hostedcontent.{messageid}.{hostedcontenidMD5}.png\u0026rdquo;\nOf cause all this needs to happen with some kind of authentication. As mentioned, I\u0026rsquo;m big fan of application permission because there is no user involved. But the current graph implementation only offers access to chat messages if you apply for an app with protected API access. Read more about this here. I tried to apply but 4 weeks later I still don\u0026rsquo;t have feedback for my request (single tenant app should be easy) and I implemented the access using delegate permission based on the device flow. Based on this approach I made this into a feature. The tool will backup (by default) all teams the account is a member of. This way you can add the account to a Team as an admin or even the owner of Team can do this to \u0026ldquo;request\u0026rdquo; chat backup.\nThe other console application is taking care of building the HTML from the JSON files. Because all the heavy lifting is already done in the first part, the generation of HTML is really fast. The application needs the source directory and the HTML template for the output. The HTML templates currently has some inline styles to make it easy to move around and keep the dependencies low. The application is creating a HTML file for each channel and optionally a HTML file for every thread. Based on the configuration the HTML file will contain all images inline or as a separate file. If you go with the inline images you get a very portable version of the backup, but also a big file if you have many images or a long chat history. The combination of inline images and every thread as an HTML file will give you a great choice out of the box. If you would like to customize the HTML look and feel have a peak at the template file. With a few tweaks in the CSS styles you can enhance readability and change it to your preferences. I hopefully selected some easy-to-understand selectors.\nThe code Let’s have a quick look at the code and what it takes to get it compiled, if you pull it from GitHub. I am doing all my development in Visual Studio \u0026ldquo;proper\u0026rdquo;. The solution was created with Visual Studio 2019 Enterprise (Preview) but the community non preview version should also be fine. I write most of my code in C# and so is this code. As .NET 5 is now available this is my first solution using this version, but I think you can get it running on .NET Core 3.1 if you downgrade the solution and packages. The final solution is hosted on GitHub and you are welcome to open an issue or create a pull request. Just have a little bit of patience as this is just a side hustle for me.\nNormally I write Azure Functions and dependency injection is not yet in my DNA. If you look at the code this might feel a little awkward. I spend way too much time to get the console app make use of the DI concepts. For a start it feels OK and I had a lot of fun learning to code this, but I\u0026rsquo;m not sure this is 100% correct. Let’s put it this way: It works! Also, a thing I love about Azure Functions is the native Microsoft Extension Logging integration. So, it was natural to also rely on my normal code to write logs. In the cloud logging to a file on disk is not really a thing and that might be the reason why Microsoft does not have an out of the box solution for that. That’s my excuse why all of my logs are only on the console for the moment. I am looking into NLog or Serilog. The biggest benefit is to configure the logs levels very easy. Check out the application.json for a sample. The configuration system is also based on Microsoft standards. The console is loading the application.json settings and during development an argument can be passed in to respected the correct JSON file. A Special thanks to David Feldman for his blog post which got me started on the console DI thing.\nAs of now we didn\u0026rsquo;t write any business relevant code (developer love to write non relevant code 😁) so let’s get our hands dirty. Getting the data from Microsoft Teams is done via the Microsoft Graph. I\u0026rsquo;m using the beta SDK because for the Teams workload some feature are only available in the non-production endpoint. Also, I like to play with fire. The authentication (as mentioned: I had to go with the device code) is provided by the MSAL libraries. For my \u0026ldquo;daemon apps” (Azure Functions) I rely on the pure MSAL implementation, but for this application I tried something new and used the Microsoft Graph Auth libraries. The NuGet is still in preview, but this was the easiest solution to get the device code up and running in minutes. I plan to use this library in the future for my other projects. One thing was missing and was really annoying: I had to authenticate on every debug run, so I copied some code to persist the token to a file. This code is also standard Microsoft code, and it will put the token in a locally protected file. For the generation of the HTML file I visited an old friend: HTML Agility Pack (HAP). This is an awesome library and working with the HTML DOM is a breeze! The HTML from the Teams chat message can contain images (hosted content) pointing to the Microsoft Graph endpoint. Using the HTML Agility Pack, I search the images and replace the src with as base64 encoded version or a local file reference.\nThe result Here is a side-by-side comparison. The original channel in Microsoft Teams and the HTML backup generated from the Microsoft Graph API.\nThe HTML file will contain the Team name, creation date and the member count. The Channel name is also part of every file and if it is a private channel also the member count. Every post will contain the creation (+ edit if available) time, the author and the body including images. Links to documents are not modified and the file will not be downloaded. The content of an adaptive card is available in the JSON, but currently the output is not rendered in HTML. The adaptive cards rendering will be added later.\nAfter the complete run you have a set of JSON files representing Microsoft Graph SDK classes. I recommend putting them in to a ZIP file (there are a lot of files) and place them next to the HTML output. Based on your data and configuration the output can be stored in a SharePoint library or on a file based archive.\nYour setup Currently the setup is not download and ready to run. First I don\u0026rsquo;t want to offer a multi-tenant application in my Azure AD. Therefor you need to talk to your admin to register the application in the first place. Based on the permission the app needs the admin consent in any way. Please check out the project Readme.md to get the needed permissions and put the needed details in the application settings file. The binaries can be downloaded from the release page of the GitHub project. You will need to have the .NET 5 runtime installed.\nSummary I had a blast writing the solution from start to end. I will tweak the solution in the near future. These are the next ideas I want to code:\n Save images with the date in the name and add EXIF data with the author information Allow the use of a non-interactive AAD login (confidential client) Integrate features in an Azure Function and maybe trigger from a Teams Message Extension for an ad-hoc export for every user  I created the solution for a very specific use case (our tenant migration), but I hope in making the source code available you will be able to solve your problems. If you have feedback, please let me know. Create an issue, hit me up on Twitter or LinkedIn.\n","date":"2020-12-13","permalink":"https://marcoscheel.de/post/2020/12/20201130-m365teamsbackup/","tags":["Microsoft 365","Microsoft Teams","Development","Microsoft Graph"],"title":"Microsoft Teams backup your channel messages with Microsoft Graph"},{"content":"Microsoft hat im Mai angekündigt, dass man in Kürze auf Microsoft definierten Templates bei der Anlage zurückgreifen kann und in Zukunft auch eigene Templates im Admin-Center erstellen kann. In der Vergangenheit brauchte man eine Teams Provisioning Lösung und konnte nicht auf die eingebauten Dialoge zurückgreifen. Hier ein Beispiel, wie man über ein Site Design ein Microsoft Flow startet, um mit Teams zu interagieren.\nIn meinem Lab-Tenant ist nun endlich die Erstellung eigener Templates angekommen. Ich zeige euch, was es mit den Templates von Microsoft auf sich hat und was ihr mit den eigenen Templates erreichen könnt.\nMicrosoft Templates Microsoft bietet aktuell 13 Templates an, die man im Tenant auswählen kann. Die Templates werden mit einer \u0026ldquo;Industry\u0026rdquo; Information versehen. Auch wenn euer Unternehmen nicht aus dem Bereich stammt, sind die Templates trotzdem sinnvoll. Hier findet ihr die Dokumentation, was das einzelne Template ausmacht:\nGet started with Teams templates using Microsoft Graph - Teams template capabilities\nAus der User-Sicht startet ihr über den normalen Dialog zum Erstellen eines Teams. Habt ihr die Self-Service Creation abgeschaltet, dann sollten wir mal ein ernstes Wort reden. Wenn ihr alles \u0026ldquo;Richtig\u0026rdquo; gemacht habt, dann sieht der User folgendes:\nAuf dieser Seite kann der Benutzer sich über eine kurze Beschreibung über das Template informieren. Ist die Entscheidung gefallen, dann kommen beim Klick weitere Details zum Template:\nAb hier geht es wie gewohnt weiter. Klassifizierung + Privacy auswählen und den Namen für das Team festlegen:\nJetzt kommen die ersten Unterschiede. Ohne Template ist das Team in Sekunden erstellt und ich kann weitere Benutzer auswählen. Bei der Verwendung einer Vorlage dauert das Erstellen deutlich länger. Es geht jetzt nicht um Stunden aber der Prozess ist ab jetzt asynchron. Der Benutzer sieht folgende Meldung, welche in meinem Test sich auch nach Minuten nicht verändert hat. Das Schließen des Dialoges ist also nicht optional, wie angedeutet.\nDas fertige Team sieht dann so aus:\nIn meinem Test hat das System am Ende immer \u0026ldquo;undefinded\u0026rdquo; an den gewählten Namen angehängt, aber als Owner kann man den Namen ja jederzeit ändern.\nIn der aktuellen Implementierung kann man keine der Microsoft Templates ausblenden. Es gibt auch keine Möglichkeit Templates an eine Zielgruppe zu verteilen. Alle Templates sind immer für alle Benutzer zu sehen. Ich bin gespannt, wie es hier weiter geht.\nTemplates administrieren Als Microsoft Teams Administrator kann ich ab sofort eigene Templates erstellen und in den gezeigten Dialog integrieren. Die eigenen Templates werden immer vor den Microsoft Templates angezeigt. Hier könnt ihr die Microsoft Dokumentation einsehen:\nGet started with Teams templates in the admin center\nIn der aktuelle Implementierung ist der Funktionsumfang recht überschaubar. Ihr könnt Kanäle vordefinieren, Tabs in die Kanäle hängen und generell Apps in das Team einfügen.\nIm Admin Center gibt es im Bereich \u0026ldquo;Teams\u0026rdquo; einen neuen Navigationspunkt:\nHier kann man ein neues Template erstellen. Aktuell hat man drei Optionen:\n Create a custom team template Create a team template from an existing team template Create a template from an existing team  Eigene Template definieren Die erste Option \u0026ldquo;Create a custom team template\u0026rdquo; führt uns durch den folgenden Dialog. Name des Template, Beschreibung für den Endbenutzer und Hinweise für die Kollegen im Admin Team sind verpflichtend:\nAuf der folgenden Seite kann ich jetzt einen Kanal anlegen und gleichzeitig auch Tabs (Apps) hinzufügen:\nSollte eine App nicht über einen Tab hinzugefügt werden, dann kann ich diese auch direkt in das Team einfügen:\nIhr könnt den erstellten Kanälen (z.b. dem Standard-Kanal) nachträglich Apps hinzufügen:\nHier seht ihr die fertige Definition des Templates:\nEigenes Template von einem bestehenden Template Die zweite Option \u0026ldquo;Create a team template from an existing team template\u0026rdquo; schaltete vor die eigentliche Definition des Templates die Auswahl eines anderen (Microsoft oder eigenes) Template vor:\nDann geht es weiter wie bei der Definition eins leeren Template.\nEigenes Template von einem bestehendem Team Die dritte Option \u0026ldquo;Create a template from an existing team\u0026rdquo; erlaubt es ein bestehendes Team auszuwählen:\nDann geht es weiter wie bei der Definition eins leeren Template.\nIn dieser Auswahl werden aber auch nur die Channels, Tabs und die Apps übernommen. Inhalte wie Chat, Tab-Konfigurationen oder Dateien kommen nicht mit.\nZusammenfassung Es ist eine willkommene Option mit viel Potential für die Zukunft. Wenn ihr kein eigenes Teams Provisioning machen wollt, ist das die erste Möglichkeit euren Usern vorgefertigte Strukturen an die Hand zu geben. Es fehlen aber ganz essenzielle Dinge. Ich kann keine Dateien vorprovisionieren und zum Beispiel als Tab integrieren. Tabs können nicht mit Inhalten gefüllt werden. Ein \u0026ldquo;Website\u0026rdquo; Tab bekommt einen Namen, aber man kann nicht die URL hinterlegen.\nEs ist gut zu wissen, dass man hier „out-of-the-box“ Funktionen hat, aber solange Microsoft folgende Punkte nicht adressiert, wird es eine Nischenlösung bleiben:\n Teams Eigenschaften verändern (Moderation, etc.) Erstellungsdialog muss schneller werden Kein User/Group Targeting für Templates -\u0026gt; Heute sehen alle User alle Templates Die Verwendung eines Templates kann nicht erzwungen werden, der User kann immer noch \u0026ldquo;From scratch\u0026rdquo; wählen und sein eigenes Ding machen Microsoft Templates können nicht ausgeblendet werden Keine Inhalte (Tabs, Dateien, \u0026hellip;) ","date":"2020-11-08","permalink":"https://marcoscheel.de/post/2020/11/20201103-microsoftteams-customtemplates/","tags":["Microsoft 365","Microsoft Teams"],"title":"Eigene Vorlagen für Microsoft Teams"},{"content":"Microsoft arbeitet an einer neuen Version von Microsoft Stream. Microsoft Teams nutzte bis vor kurzem genau dieses Video-Backend für die Ablage der Meeting Recordings. Seit heute (01.11.2020) beginnt der Rollout für alle Tenants, es sei denn ihr habt per Meeting Policy ein Opt-Out für eure Benutzer gesetzt. Was das Recording in OneDrive/Teams bedeutet habe ich euch in folgendem Blogpost demonstriert.\nDas Meeting Recording liegt also im SharePoint (für Channel Meetings) oder im OneDrive (für alle anderen Meetings). Ist damit der Blog bereits zu Ende? Teilen auf SharePoint kann doch jeder, oder? Hier gibt es die Doku von Microsoft. Natürlich kann ich die Datei einfach über einen neuen Sharing Link teilen. Wenn der externe Benutzer aber im Meeting-Chat auf das Recording klickt, dann gibt es folgenden Fehler, den wir aber \u0026ldquo;einfach\u0026rdquo; Lösen können.\nMeeting Recordings richtig berechtigen Microsoft nutzt die normalen Share-Features für das Teilen des Recordings für interne Benutzer. Wenn ihr jetzt einfach den Share-Dialog am Video nutzt, dann kann der externe über den Link aus diesem Sharing (wird normal per E-Mail verschickt) zugreifen. Versucht er aber vielleicht später über Teams und den Meeting-Chat zuzugreifen, dann kommt es wieder zum Fehler, da dort ein anderer Sharing-Link hinterlegt ist:\nJetzt zeige ich euch, wie man den Link im Meeting-Chat auch für Externe konfiguriert. Wenn ihr die Datei im SharePoint oder OneDrive geöffnet habt, dann könnt ihr über \u0026ldquo;Manage access\u0026rdquo; die aktuellen Sharing Links einsehen. Es gibt viele Möglichkeiten \u0026ldquo;Manage access\u0026rdquo; zu erreichen. In der Ordneransicht klickt ihr \u0026ldquo;\u0026hellip;\u0026rdquo; auf der entsprechenden Videodatei und wählt dann \u0026ldquo;Manage access\u0026rdquo; aus:\nIhr seht die aktuell konfigurierten Freigaben:\nDie Freigabe für die Anzeige der Datei (\u0026ldquo;View\u0026rdquo;-Berechtigung) könnt ihr über die \u0026ldquo;\u0026hellip;\u0026rdquo; Option aufrufen:\nGebt die E-Mail des Gasts ein und bestätigt mit \u0026ldquo;Save\u0026rdquo;:\nWenn ich (als Gast der Besprechung) nun über den Meeting-Chat den Link aufrufe, dann bekomme ich keine Fehlermeldung beim Zugriff:\nZusammenfassung Die meisten Benutzer werden einfach im SharePoint auf \u0026ldquo;Share\u0026rdquo; klicken. Der Zugriff für externe Benutzer ist ein tolles Feature und ich will nicht meckern. Sollten eure Benutzer über das Verhalten meckern, dann könnt ihr sie nun über den richtigen Weg aufklären und das Nutzererlebnis verbessern.\n","date":"2020-11-01","permalink":"https://marcoscheel.de/post/2020/11/20201101-microsoftteams-recording-sharing/","tags":["Microsoft 365","Microsoft Teams"],"title":"Microsoft Teams Recording mit Externen teilen"},{"content":"Auf der Ignite 2020 wurde angekündigt, dass man Microsoft Stream einstellen neu erfinden wird. Ich bin Feuer und Flamme für die Idee, wie ihr hier sehen könnt:\nMigration vom @SharePoint Video Portal schon begonnen? Alle Videos in @MicrosoftStream angekommen? Jetzt geht\u0026#39;s wieder zurück 🤣 Aber ich finde das super. Endlich eine Video API natürlich mit @microsoftgraph! Und Meetingrecording von @MicrosoftTeams! https://t.co/ZKZsjWaDhP\n\u0026mdash; Marco Scheel (@marcoscheel) September 22, 2020  Den Microsoft Blogpost mit allen Details findet ihr hier. Heute wollen wir uns die Auswirkungen auf die Meeting Recordings in Microsoft Teams anschauen. In der \u0026ldquo;Vergangenheit\u0026rdquo; hatten wir folgende Probleme mit der Ablage in Microsoft Stream:\n Externe Teilnehmer konnte nie auf die Aufnahmen zugreifen, da Microsoft Steam (Classic) keinen Zugang für externe ermöglicht und es immer ein Konto aus dem Unternehmen für den Zugriff brauchte. Wer die Aufnahme starten wollte, brauchte nicht nur die Teams Meeting Policy dazu, sondern auch das Recht Videos in Stream hochzuladen. Es gibt keine offizielle API  Mit dem neuen Microsoft Stream gehören diese Probleme der Vergangenheit an und es werden noch viele Funktionen in der nahen Zukunft ergänzt. Zum Start bekommen wir aber eine sehr rudimentäre Implementierung mit ihren eigenen Problemen. Wir schauen einmal auf die entsprechende Implementierung Stand Oktober 2020. Microsoft hat zur Ignite eine dedizierte Session zum Thema Besprechungsaufzeichnung erstellt in der ihr viele Details findet.\nIm Meeting Ich habe für euch ein Meeting dokumentiert und zeige wo die Unterschiede liegen. Im Microsoft Teams Client bleibt während der Besprechung alles beim Alten. Über die erweiterten Funktionen (\u0026hellip;) kann jeder Moderator aus dem einladenden Unternehmen das Recording starten \u0026ldquo;Start recording\u0026rdquo;: Im Meeting sehen wir:\n Luke (Meeting Organizer) - luke ät gkmm.org Leia - leia ät gkmm.org Rey - rey ät gkmm.org Kylo - kylo ät gkmm.org Marco (Gast) - marco.scheel ät glueckkanja.com  Die Benutzer werden wie üblich mit einem Banner über den Start der Aufnahme informiert: Wird die Aufzeichnung während des Meetings beendet, dann werden die Benutzer über das Speichern informiert: Die Aufzeichnung wird im Meeting Chat verlinkt. Recording ansehen Hier kommt die erste Neuerung! Das Video ist deutlich schneller verfügbar. Microsoft Teams erzeugt das Video als MP4 in der Cloud und hat es in der Vergangenheit an Microsoft Steam übergeben. Stream hat dann die Azure Media Services bemüht, um das Video aufzubereiten und für ein adaptives Streaming auszuliefern. Simple gesagt: Stream hat das Video in verschiedenen Auflösungen gerechnet und kann nahtlos zwischen den Bitraten hin und her wechseln. Das neue Stream wirft einfach das MP4 in SharePoint (oder OneDrive for Business) und stellt es dann über einen einfachen HTML Player zur Verfügung. Es sollte klar werden, dass ohne die Integration der Azure Media Services in dieser ersten Version einige Funktionen entfallen:\n Wiedergabegeschwindigkeit. Nicht jeder redet so schnell wie ich und dann kann man schon ein 60 Minuten Video auf 45 Minuten reduzieren, wenn man es in 1,5x wiedergibt. Transkription (Sprache zu Text). Für ein Meeting Recording ziemlich relevant, um zum Beispiel im 2 Stunden Meeting den Moment zu finden, als es um Produkt X ging. Bandbreiten- und Performanceabhängiges Adaptives Streaming (wechseln von 1.1 Mbps bis 58Kbps). Für ein Meeting Recording nicht besonders relevant.   Anfang 2020 hatten unsere Video Recordings in Stream noch alle Bitraten. Aktuell sind auch alle Stream Videos (Meeting Recording) nur in der Original 1.1 Mbps (1080p) Auflösung verfügbar und macht somit kein adaptives Streaming. Corona lässt grüßen?\n Der folgende Screenshot zeigt Lukes Browser bei der Wiedergabe, nachdem er im Chat auf \u0026ldquo;Open in OneDrive\u0026rdquo; klickt. Genau so wird heute bereits jedes andere Video in SharePoint und OneDrive dargestellt. In der Zukunft wird der Content Typ für Video im SharePoint aufgewertet und die Darstellung, die Metadaten und der Lifecycle werden optimiert.\nFür alle Meeting Teilnehmer aus der einladenden Organisation wird das Sharing der Datei automatisch eingerichtet: Teams vergibt immer zwei Berechtigungen für die Video Datei. Es wird unterschieden in Moderator und Teilnehmer. Moderatoren erhalten Edit Berechtigungen. Teilnehmer des Meetings erhalten nur View Berechtigungen.\nGäste werden nicht berücksichtig. Für mich als Gast endet der Versuch das Recording anzusehen so: In diesem Blogpost gehe ich auf den richtig Umgang mit Gästen und dem Recording ein.\nDie Videodatei Es bleibt zu beantworten, wo die Datei dann eigentlich liegt! Wenn man im Chat ein Video angeklickt, dann öffnet sich in Teams die Video Datei und wird abgespielt. Keine gute Idee, denn jede Interaktion mit Teams führt zum Abbruch der Wiedergabe und man muss von vorne anfangen. Also am besten gleich die \u0026ldquo;\u0026hellip;\u0026rdquo; anklicken und \u0026ldquo;Open in OneDrive\u0026rdquo; auswählen: Die Datei liegt also im OneDrive und wird in einem Ordner mit dem Namen \u0026ldquo;Recordings\u0026rdquo; (bestimmt auch irgendwo \u0026ldquo;Aufzeichungen\u0026rdquo; oder \u0026ldquo;grabación\u0026rdquo;). OneDrive ist immer eine persönliche Ablage. Sie gehört einem Benutzer! Keine schöne Lösung. Also ein \u0026ldquo;Meet now\u0026rdquo; oder ein geplanter Termin landen im OneDrive eines Benutzers. Welcher Benutzer? Meeting Organizer oder der Benutzer der \u0026ldquo;Start recording\u0026rdquo; klickt. Es ist tatsächlich das OneDrive des Benutzers, der am schnellsten Klicken kann. Ich hätte den Meeting Organizer bevorzugt, da er auch der Besitzer des Termins ist. Hier hatte Stream mit seiner \u0026ldquo;neutralen\u0026rdquo; Ablageplattform klare Vorteile. Verlässt ein Benutzer das Unternehmen und sein OneDrive wird gelöscht, verschwinden alle Meeting Recordings mit seinem OneDrive! Microsoft hat angekündigt, dass es Mechanismen geben wird, welche automatisch \u0026ldquo;alte\u0026rdquo; Aufzeichnungen löschen wird. Der Speicherverbrauch auf SharePoint soll so reduziert werden. Eventuell kommen hier Retention Policies zum Einsatz. Diese Policies können nicht nur Dateien löschen, sondern auch für einen bestimmten Zeitraum garantiert Vorhalten. Es kann also sein, dass es hier auch eine Lösung für das Löschen eines OneDrives geben wird.\nMeetings können aber auch alternativ in einem Teams Kanal geplant werden. Diese Channel-Meetings speichern ihr Recording in dem entsprechenden Ordner im Kanal. Wenn ich also im \u0026ldquo;General\u0026rdquo; Kanal aufzeichne, dann liegt die Datei hier \u0026ldquo;/sites/YOURTEAMSITE/Shared Documents/General/Recordings\u0026rdquo;. Ich bin semi-zufrieden. Die Ablage im Team löst die Probleme beim Zugriff und die Frage wem die Datei gehört. Solltet ihr aber zum Beispiel den Ordner General per OneDrive Client auf eurem Rechner syncen und immer alles Offline wollen\u0026hellip; dann kommen jetzt auch größere Videodateien mit. Aber man kann nicht alles haben und ich hoffe, dass Microsoft in Zukunft diese Szenarien weiter optimiert.\nDas Label der Schaltfläche im Meeting Chat heißt immer \u0026ldquo;Open in OneDrive\u0026rdquo; und ändert sich für ein Channel Meeting NICHT in \u0026ldquo;Open in SharePoint\u0026rdquo;. Auch hier gibt es die Chance auf Besserung in der Zukunft.\nSchauen wir mal direkt auf die Datei. Hier ein Screenshot der erweiterten MP4 Eigenschaften: Die Aufzeichnung ist etwas unter 5 Minuten lang und verbraucht ca. 25 MB. Das Video besitzt eine FullHD (1080p) Auflösung. Wenn ich mir den Dateinamen anschaue, dann bin ich super happy. In Stream waren besonders die Titel der Channel Meetings oft Nichtssagend z.B. \u0026ldquo;Meeting in General\u0026rdquo;. So baut sich der Dateiname im neuen Stream auf:\n Demo Luke and Leia-20201024_152149-Meeting Recording.mp4  TitelDesMeeting-yyyyMMdd_HHmmss-Meeting Recording TitelDesMeeting = Demo Luke and Leia yyyyMMdd_HHmmss = Start der Aufnahme (Lokale Zeit und nicht UTC), also der Moment in dem \u0026ldquo;Start recording\u0026rdquo; geklickt wird   Meeting in _Workplace_-20201023_100348-Meeting Recording.mp4  MeetingInKanal\u0026ndash;yyyyMMdd_HHmmss-Meeting Recording MeetingInKanal = Der Kanal heißt Workplace und in dem Fall wurde leider der Titel des Meetings nicht übernommen. yyyyMMdd_HHmmss = Start der Aufnahme (Lokale Zeit und nicht UTC), also der Moment in dem \u0026ldquo;Start recording\u0026rdquo; geklickt wird    Einschalten im Tenant Microsoft hat aktuelle Dokumentation wann es wie weiter geht. Aktuell kann ein Tenant Admin den Opt-In durchführen und in wenigen Stunden ist der Tenant bereit und alle neuen Meeting Recordings landen direkt im OneDrive/SharePoint. Wenn der Admin kein Opt-In oder Opt-Out macht, dann wird ab Mitte Q4 2020 die Funktion für den Tenant konfiguriert. Habt ihr für euren Tenant ein Opt-Out konfiguriert, dann ist trotzdem im Q1 2021 Schluss und das Recording auf OneDrive/SharePoint wird auch für euren Tenant umgestellt. Bedeutet, dass mit Start Q2 2021 alle Meeting Recordings nur noch im neuen Microsoft Stream (aka SharePoint) laden.\nFür den Opt-In braucht ihr die Skype for Business PowerShell, um die entsprechende Meeting Policy zu setzen. Seit der Ignite 2020 sind die CommandLets aber auch in das Microsoft Teams PowerShell Modul integriert.\nImport-Module SkypeOnlineConnector\r$sfbSession = New-CsOnlineSession\rImport-PSSession $sfbSession\rSet-CsTeamsMeetingPolicy -Identity Global -RecordingStorageMode \u0026quot;OneDriveForBusiness\u0026quot;\r Da es eine Meeting Policy ist, kann man das Feature auch erstmal nur für einzelne Benutzer freischalten.\nFür den Op-Out setzt ihr den Wert einfach auf \u0026ldquo;Stream\u0026rdquo;:\nSet-CsTeamsMeetingPolicy -Identity Global -RecordingStorageMode \u0026quot;Stream\u0026quot;\r Zusammenfassung Microsoft Teams läutet eine neue Video Ära in Microsoft 365 ein. Die Meeting Recordings können ab sofort für all oder einzelne Benutzer nach SharePoint/OneDrive umgeleitet werden. Der Funktionsverlust ist angesichts der neuen Freiheit beim Teilen der Aufzeichnungen zu verschmerzen. Folgende Punkte sind also zu beachten:\n Aufzeichnungen im Benutzer OneDrive können verloren gehen, wenn das OneDrive gelöscht wird (Mitarbeiter verlässt das Unternehmen) Kein x-fache Wiedergabegeschwindigkeit Keine adaptive Bandbreitenanpassung (aktuell auch in Stream nicht gegeben) Keine Integration in die aktuelle Stream Mobile App Wird eine Datei umbenannt, verschoben oder gelöscht, dann geht der Link im Meeting Chat kaputt Das richtige Teilen mit Externen erfordert einige Klicks (Blogpost coming soon) Keine Möglichkeit alle Meeting Recordings auf einen Blick zu sehen, man muss jedes Mal den Meeting Chat suchen ","date":"2020-10-24","permalink":"https://marcoscheel.de/post/2020/10/20201020-microsoftteams-recording-onedrive/","tags":["Microsoft 365","Microsoft Teams","SharePoint"],"title":"Microsoft Teams Recording jetzt in SharePoint statt Microsoft Stream"},{"content":"Wer kennt es nicht: Das Meeting startet und plötzlich \u0026ldquo;piiiiiiiiep-piiiiiep-piiep-piep\u0026rdquo; \u0026hellip; der Projektleiter parkt rückwärts ein. Die Flexibilität in der Teilnahme an einem Teams Meeting ist \u0026ldquo;nahezu\u0026rdquo; unbegrenzt. Die Freiheitsgrade sind aber für viele neu und ungeübt. Der geübte Umgang mit dem Meeting Equipment ist noch in weiter Ferne. Im optimalen Fall setzen wir alle nur Microsoft Teams zertifizierte Geräte ein und Hardware + Software arbeiten in Harmonie. Wenn ich zu spät in ein Meeting komme, dann hoffentlich \u0026ldquo;on mute\u0026rdquo;. Die Realität sieht noch immer anders aus. Besonders in großen Meetings war es ein Problem, dass Teilnehmer jederzeit das Mikrofon öffnen konnten. Ein Microsoft Teams Live-Events waren oft keine Lösung, da die Interaktivität in einem späteren Moment fehlte. Durch die Einführung des Roadmap Item 66575 ist das Problem lösbar:\n Prevent attendees from unmuting in Teams Meetings\nGives meeting organizers the ability to mute one to many meeting participants while removing the muted participants' ability to unmute themselves.\n Hier der Screenshot zum Feature:\nTeilnehmer (Attendee) vs Moderator(Presenter) Microsoft Teams kennt in einem Meeting drei wesentliche Rollen. Für unser Szenario sind nur zwei Rollen interessant. Das Unternehmen kann vorgeben, wie strikt die Moderator-Rolle (Presenter) gehandhabt wird. Im Standard ist jeder Benutzer dieser Rolle zugeordnet und kann damit das Meeting unterstützen oder empfindlich stören. Im Meeting selbst kann jeder Moderator andere Benutzer zum Teilnehmer herabstufen. Teilnehmer können nicht präsentieren und keine Benutzer aus der Konferenz werfen. Für die genau Übersicht checkt diesen Link.\nWelche Optionen das Unternehmen zur Vorgabe hat, findet ihr mit allen Details hier.\n Jeder Jeder aus dem Unternehmen Nur der Organisator  Meeting planen Wenn ihr ein Meeting fertig geplant habt, könnt ihr nachträglich in die Meeting Optionen schauen. In Skype for Business konnte man das schon direkt beim Planen, aber Microsoft hat es sich hier \u0026ldquo;einfach\u0026rdquo; gemacht und springt einfach im Browser auf diese Optionen. Im Beschreibungstext des Termins neben dem Teilnahme-Link, kann der Organisator auch aus allen anderen Programmen auf diese Optionen zugreifen:\nHier die Meeting Optionen im Browser:\nWer kann präsentieren? Hier ist der Unternhemnsstandard vorgegeben und ihr könnt den Wert nach euren Vorlieben anpassen.\nSolltet ihr die Option \u0026ldquo;Specific people\u0026rdquo; auswählen, konnte ich in meinem Test nur eingeladenen Personen aus dem Unternehmen selbst auswählen. Je nachdem wer den Termin tatsächlich steuert, müsst ihr hier aufpassen, wenn ihr zum Beispiel durch einen externen Projektleiter unterstützt werden, der normalerweise das Meeting leitet.\nJetzt kommen wir zum eigentlichen Feature, wenn ihr schon zur Planung die Entscheidung treffen könnt, dann stellt ihr hier die Option \u0026ldquo;Allow attendees to unmute\u0026rdquo; aus.\nIm Meeting Während des Teams Meetings kann ein Moderator die Option zum \u0026ldquo;Stumm-schalten\u0026rdquo; der Teilnehmer direkt im Client bedienen. In der Teilnehmerliste kann man über das \u0026ldquo;\u0026hellip;\u0026quot;-Menü die Benutzergruppe stumm schalten.\nFür den Teilnehmer wird es im Client kurz signalisiert, dass Sie aktuell stumm sind. Ein \u0026ldquo;Unmute\u0026rdquo; über zum Beispiel zertifizierte Hardware-Button wird sofort wieder zurückgesetzt und das Headset (Dongle) zeigt weiterhin Mute (Rot) an.\nFür mich als Teilnehmer ist das Symbol zum \u0026ldquo;Unmuten\u0026rdquo; ausgegraut und nicht bedienbar. Der Rest der Teilnehmer wird ebenfalls als deaktiviert angezeigt.\nIn meinem Test kann man übrigens sehen, dass noch nicht alles rund läuft. Für ein kurzen Moment war mein Mikrofon noch offen (wie beim berühmten Double-Mute) aber Teams hatte mich im Client schon stumm geschaltet. In dem Moment hat mich der Client dann drauf hingewiesen, dass ich noch mute bin, obwohl ich es ja nicht ändern kann :) Wird schon noch werden.\nEin Moderator kann jederzeit die Option wieder für alle entfernen oder einzelne Teilnehmer zum Moderator befördern.\nAbschluss Ich kannte die Funktion bisher nur von WebEx und wurde schon das eine oder andere Mal darauf angesprochen. Bisher war meine Antwort eine Kombination aus Live-Event und Teams Meeting im Anschluss. Durch diese neue Funktion wird es für alle viel einfacher. Ich würde trotzdem sparsam mit dem Setting umgehen, da die Tücke im Detail liegt. Aktuell können zum Beispiel nur die Web- und Desktop-Versionen damit umgehen. Auf einem Microsoft Teams Room System oder dem Mobile Clients (Android/iOS) gibt es die Funktion Stand Oktober 2020 noch nicht. Es kann also passieren, dass ein Meeting nicht stattfinden kann, weil die Teilnehmer nicht zu Wort kommen können. Wie so oft kann man durch gute User-Erziehung mehr erreichen als durch harte Regulation.\nMicrosoft hat wie so oft in letzter Zeit hervorragen Dokumentation zum Feature Online. Schaut also selbst nochmal rein.\n Verwalten von audioberechtigungen für Teilnehmer in Teams-Besprechungen Manage attendee audio permissions in Teams meetings ","date":"2020-10-17","permalink":"https://marcoscheel.de/post/2020/10/20201017-unmute-attendee/","tags":["Microsoft 365","Microsoft Teams"],"title":"Teilnehmer in Microsoft Teams für immer stumm schalten"},{"content":"My blog now has a new home. It is no longer hosted on Tumblr.com and it is now hosted on GitHub Pages. The main reason to get of Tumblr is the poor image handling. The overall experience was OK. I liked the editor and best of all it is all free including running on your own domain! Having my own name was a key driver. I was running my blog on my own v-server back in the days. I tried a lot of platforms (blogger.com, wordpress.com and prior Tumblr I ran on a \u0026ldquo;self\u0026rdquo; hosted WordPress instance). The only constant was and will be my RSS hosting. Believe it or not I\u0026rsquo;m still running Feedburner from Google. One service that is still not (yet?) killed by search giant (RIP Google Reader). With all the previous choices there was also on driving factor: I\u0026rsquo;m cheap, can I get it for free? Yes and it will stay 100% free for you and me!\nToday is the day I switched to a static website! It is 2020 and the hipster way to go. So, what does it take to run a blog on a static site generator?\n A static site generator: https://gohugo.io A place to host the source files: https://github.com A place to host the generated website: https://pages.github.com A method to publish after I committed a new blog post: This but later https://github.com/features/actions  Main benefits:\n Still free I own my content 100% Better image handling (high-res with zoom) Better inline code handling and highlighting Learning new stuff  HUGO  Hugo is one of the most popular open-source static site generators. With its amazing speed and flexibility, Hugo makes building websites fun again.\n Why Hugo and not Jekyll? Because there are blogs out there that I\u0026rsquo;m reading, and I liked the idea of being one of them :) Who?\n Andrew Connell: https://www.andrewconnell.com/blog/hosting-hugo-on-azure Maximilian Melcher: https://melcher.dev/2017/11/bye-bye-wordpress-hello-hugo Matthew McDermott: https://www.ableblue.com/blog/archive/2019/02/20/learning-hugo-on-azure-part-1 Jason Hand: https://dev.to/jasonhand/10-tips-for-building-and-deploying-hugo-websites-on-azure-static-web-apps-307l Aaron Wislang: https://www.aaronmsft.com/posts/static-sites-hugo-azure-cloudflare  There is even content on Microsoft Docs on hosting Hugo on Azure Static Websites: https://docs.microsoft.com/en-us/azure/static-web-apps/publish-hugo\nIt was easy to start. Just follow the steps on the Getting started using choco installation for Windows users.\nI\u0026rsquo;ve chosen the Fuji theme as a great staring point and integrated it as a git submodule. As mentioned in the docs I copied the settings into my config.toml and I was ready to go.\nhugo new post/hugo/2020/10/my-blog-has-moved.md\rhugo server -D\r Open localhost:1313 in your browser of choice and check the result.\nMy tweaks To get result in the picture above I needed some tweaks. Also, some other settings are notable if you are like me :)\nThe chosen theme is not very colorful and I really wanted a site image. I\u0026rsquo;m sure it is my missing knowledge about Hugo and theming but I ended up messing this the CSS to get a header image. I have put a classic CSS file in my \u0026ldquo;static/css\u0026rdquo; folder.\nheader {\rbackground-image: url(/bg-skate-2020.jpg);\rbackground-size: cover;\r}\rbody header a {\rcolor: #ffffff;\rwhite-space: normal;\r}\rbody header .title-sub{\rcolor: #ffffff;\rwhite-space: normal;\r}\rbody .markdown-body strong{\rcolor: #000000;\r}\r To integrate this into your theme we use partials. To not mess with my theme (it is a submodule and controlled by the original author) I had to copy the \u0026ldquo;head.html\u0026rdquo; from my theme into \u0026ldquo;layouts/_partials\u0026rdquo; and I added the link to my CSS at the end of the file. While I\u0026rsquo;m in here I will also add the RSS tag to my FeedBurner account.\n...\r\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;https://cdn.jsdelivr.net/npm/disqusjs@1.3/dist/disqusjs.css\u0026quot; /\u0026gt;\r{{ end }}\r{{ partial \u0026quot;analytic-gtag.html\u0026quot; . }}\r{{ partial \u0026quot;analytic-cfga.html\u0026quot; . }}\r\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;/css/custom.css\u0026quot;\u0026gt;\r\u0026lt;link rel=\u0026quot;alternate\u0026quot; type=\u0026quot;application/rss+xml\u0026quot; href=\u0026quot;http://feeds.marcoscheel.de/marcoscheel\u0026quot;\u0026gt;\r I also modified the Google Analytics integration in the same way. I copied the \u0026ldquo;analytic-gtag.html\u0026rdquo; file to my partials folder and added the attribute \u0026ldquo;anonymize_ip\u0026rdquo; to anonymize the IP address.\n...\rdataLayer.push(arguments);\r}\rgtag('js', new Date());\rgtag('config', '{{ . }}', {'anonymize_ip': true});\r\u0026lt;/script\u0026gt;\r\u0026lt;script async src=\u0026quot;https://www.googletagmanager.com/gtag/js?id={{ . }}\u0026quot;\u0026gt;\u0026lt;/script\u0026gt;\r To get a favicon I followed the instructions on my theme site doc.\nBy default, the RSS feed generated will include only a summary (I HATE THAT) and return all items. I\u0026rsquo;ve found this post about solving my RSS \u0026ldquo;problem\u0026rdquo;. This time we had to grab the content from the Hugo website and copy the file into \u0026ldquo;layouts/_default/rss.xml\u0026rdquo;. Switch from \u0026ldquo;.Summary\u0026rdquo; to \u0026ldquo;.Content\u0026rdquo; and switched the description of the RSS feed to my site description. Also, I configured the XML feed to only return 25 items.\n...\r\u0026lt;description\u0026gt;{{.Site.Params.subTitle}}\u0026lt;/description\u0026gt;\r...\r\u0026lt;pubDate\u0026gt;{{ .Date.Format \u0026quot;Mon, 02 Jan 2006 15:04:05 -0700\u0026quot; | safeHTML }}\u0026lt;/pubDate\u0026gt;\r{{ with .Site.Author.email }}\u0026lt;author\u0026gt;{{.}}{{ with $.Site.Author.name }} ({{.}}){{end}}\u0026lt;/author\u0026gt;{{end}}\r\u0026lt;guid\u0026gt;{{ .Permalink }}\u0026lt;/guid\u0026gt;\r\u0026lt;description\u0026gt;{{ .Content | html }}\u0026lt;/description\u0026gt;\r config.toml\nrssLimit = 25\r Content migration I also need to take care about my old content living on Tumblr and if possible, on WordPress. It was kind of easy. I checked the migration article on the Hugo docs site.\nTumblr: https://gohugo.io/tools/migrations/#tumblr All of the solutions require a Tumblr app registration, so I created on. To not mess with my fresh Windows install I enabled WSL2 and used the Ubuntu distro. This way I was able to clone the tumblr-importr repo and build my application. The important part was to place the GO binary into the right location. Otherwise the command was unknown. After that I was able to generate the needed files.\ngit clone https://github.com/carlmjohnson/tumblr-importr\rcd tumblr-importr\rgo build\rsudo cp tumblr-importr $GOPATH/bin\rtumblr-importr -api-key 'MYAPIKEYHERE' -blog 'marcoscheel.de'\r I copied the files into a subfolder named \u0026ldquo;tmblr\u0026rdquo; in my \u0026ldquo;content/post\u0026rdquo; folder. My main problem was that the content was not markdown. The files used HTML. I ended up opening all the blog posts on Tumblr in edit mode and switched to markdown mode and copied the source to the corresponding .md file. I only had 12 posts, so the work was doable and the result is clean. The main benefit of the conversion was that the front-matter attributes were pre-generated I did not have to recreate those (title, old URL as alias, tags, date, \u0026hellip;)\ndate = 2019-08-02T19:41:30Z\rtitle = \u0026quot;Manage Microsoft Teams membership with Azure AD Access Review\u0026quot;\rslug = \u0026quot;manage-microsoft-teams-membership-with-azure-ad\u0026quot;\rid = \u0026quot;186728523052\u0026quot;\raliases = [ \u0026quot;/post/186728523052/manage-microsoft-teams-membership-with-azure-ad\u0026quot; ]\rtags = [ \u0026quot;Microsoft 365\u0026quot;, \u0026quot;Azure AD\u0026quot;, \u0026quot;Microsoft Teams\u0026quot;]\r The Tumblr export generated an image mapping JSON. I used the JSON (converted to a CSV) to rewrite my images to the downloaded (still to small) version.\n\u0026quot;OldURI\u0026quot;:\u0026quot;NewURI\u0026quot;\r\u0026quot;https://64.media.tumblr.com/023c5bd633c51521feede1808ee7fc20/eb22dd4fa3026290-d8/s540x810/36e4547d82122343bec6a09acf4075bb15eae1c1.png\u0026quot;: \u0026quot;tmblr/6b/23/64d506172093d1d548651e196cf7.png\u0026quot;\r $images = Import-Csv -Delimiter \u0026quot;:\u0026quot; -Path \u0026quot;.\\image-rewrites.csv\u0026quot;;\rGet-ChildItem -Filter \u0026quot;*.md\u0026quot; -Recurse | ForEach-Object {\r$file = $_;\r$content = get-content -Path $file.FullName -Raw\rforeach ($image in $images) {\r$content = $content -replace $image.OldURI, $image.NewURI\r}\rSet-Content -Value $content -Path ($file.FullName)\r}\r WordPress: https://gohugo.io/tools/migrations/#wordpress Once again, I used my handy WSL2 instance to not mess with not loved language. So a save route was to use the WordPress export feature and the repo exitwp-for-hugo. I cloned the repo and a few \u0026ldquo;sudo apt-get\u0026rdquo; later I was ready to run the python script. I placed my downloaded XML into the \u0026ldquo;wordpress-xml\u0026rdquo; folder. I ended up changing the exitwp.py file to ignore all tags and replace it with a single \u0026ldquo;xArchived\u0026rdquo;.\ngit clone https://github.com/wooni005/exitwp-for-hugo.git\rcd exitwp-for-hugo\r./exitwp.py\r At the end, my \u0026ldquo;content/post\u0026rdquo; folder looks like to following.\nGithub Now that the content is available on my local drive and I\u0026rsquo;m able to generate the static files. It is already a git repo so where to host the primary authority? So, the Hugo site with all config and logic will go to GitHub. There are only two choices for me. GitHub or Azure DevOps. Microsoft is owning both services. Private repos are free in both services. It looks like in the future Azure DevOps will not get all the love and that is why my website source code is hosted on GitHub: https://github.com/marcoscheel/marcoscheel-de\nGitHub Pages Next up is to generate the final HTML and put it out there on the internet. Generating the content is as easy as running this command.\nNow we need to decide how to host the content. My first try was to setup a new Azure Pay-As-You-Go subscription with a 200$ starting budget for the first month and my personal credit card from here. Based on Andrew Connell blog I setup a storage account and enabled the static website. I could setup a custom domain for the blob store, but I created a Azure CDN (MS Standard) to optimize traffic and reduce potential cost. I also checked for Cloudflare CDN. All options allowed to have a custom domain and easy HTTPS with build in certificates. At the end it was my credit card and if something went really wrong (too much traffic due to non-paid internet fame?) I would be paying a life lesson with real money. I took the easy route instead. GitHub Pages to the rescue.\n Websites for you and your projects. Hosted directly from your GitHub repository. Just edit, push, and your changes are live.\n For every account GitHub is offering one GitHub Pages repository. I created the repository at: https://github.com/marcoscheel/marcoscheel.github.io\nNormally the content will be server on the github.io domain, but through the settings we can add a CNAME to the site. To achieve this we need to put a file called \u0026ldquo;CNAME\u0026rdquo; into the root directory. For my Hugo site and the publish process I placed the file into the \u0026ldquo;static\u0026rdquo; folder so every time the site is generated the file will be copied to the root of the site. Once the CNAME is in place we configure the HTTPS redirect.\nCustom domain. HTTPS. No credit card. Everything is good.\nPublishing In the future I\u0026rsquo;m looking forward to enable GitHub Actions to publish my site. For the moment I rely on my local environment pushing content from my Hugo site to the GitHub Pages repository. I integrated the GitHub Pages repo as a submodule and with the publish process I put the files into \u0026ldquo;public/docs\u0026rdquo;.\npublishDir = \u0026quot;public/docs\u0026quot;\r A quick \u0026ldquo;hugo\u0026rdquo; on the Windows Terminal and a fresh version is ready to be pushed into the right repo.\nhugo\rcd public\rgit add -A \u0026amp;\u0026amp; git commit -m \u0026quot;Ready for go live\u0026quot;\rgit push\r","date":"2020-10-11","permalink":"https://marcoscheel.de/post/2020/10/20201011-my-blog-has-moved/","tags":["Meta"],"title":"My blog has moved - From Tumblr to Hugo on GitHub Pages"},{"content":"Microsoft announced end of June 2020 the “General Availability” of the Microsoft Information Protection integration for Group labeling. Unified labeling is now available für all Microsoft 365 Groups (Teams, SharePoint, \u0026hellip;).\n Microsoft Information Protection is a built-in, intelligent, unified, and extensible solution to protect sensitive data across your enterprise – in Microsoft 365 cloud services, on-premises, third-party SaaS applications, and more. Microsoft Information Protection provides a unified set of capabilities to know your data, protect your data, and prevent data loss across Microsoft 365 apps (e.g. Word, PowerPoint, Excel, Outlook) and services (e.g. Teams, SharePoint, and Exchange).\n Source: https://techcommunity.microsoft.com/t5/microsoft-security-and/general-availability-microsoft-information-protection/ba-p/1497769\nThe feature is currently an opt-in solution. The previous Azure AD based group classification is still available and supported. If you want to switch to the new solution to apply sensitivity labels to your groups you need to run some lines of PowerShell. This is the Microsoft documentation:\nhttps://docs.microsoft.com/en-us/azure/active-directory/users-groups-roles/groups-assign-sensitivity-labels#enable-sensitivity-label-support-in-powershell\nThe feature is configured with the same commands as the AAD based classification. You have to set the value for “EnableMIPLabels“ to true.The documentation is expecting that you already have Azure AD directory settings for the template “Group.Unified“. If this is not the case you can also follow the instructions on the Azure AD directory settings for Groups:\nhttps://docs.microsoft.com/en-us/azure/active-directory/users-groups-roles/groups-settings-cmdlets#create-settings-at-the-directory-level\nTo make it easier for my customers and for you, I’ve created a PowerShell that will help and work in any configuration. Check out the latest version of my script in this GitHub repository:\nhttps://github.com/marcoscheel/snippets/blob/master/m365-enable-ul-groups/enable-ulclassification.ps1\n$tenantdetail = $null; $tenantdetail = Get-AzureADTenantDetail -ErrorAction SilentlyContinue; if ($null -eq $tenantdetail) { #connect as gloabl admin Connect-AzureAD $tenantdetail = Get-AzureADTenantDetail -ErrorAction SilentlyContinue; } if ($null -eq $tenantdetail) { Write-Host \u0026quot;Error connecting to tenant\u0026quot; -ForegroundColor Red; Exit } $settingIsNew = $false; $setting = Get-AzureADDirectorySetting | Where-Object { $_.DisplayName -eq \u0026quot;Group.Unified\u0026quot;}; if ($null -eq $setting){ Write-Host \u0026quot;Not directory settings for Group.Unified found. Create new!\u0026quot; -ForegroundColor Green; $settingIsNew = $true; $aaddirtempid = (Get-AzureADDirectorySettingTemplate | Where-Object { $_.DisplayName -eq \u0026quot;Group.Unified\u0026quot; }Id; $template = Get-AzureADDirectorySettingTemplate -Id $aaddirtempid; $setting = $template.CreateDirectorySetting(); } else{ Write-Host \u0026quot;Directory settings for Group.Unified found. Current value for EnableMIPLabels:\u0026quot; oregroundColor Green; Write-Host $setting[\u0026quot;EnableMIPLabels\u0026quot;]; } $setting[\u0026quot;EnableMIPLabels\u0026quot;] = \u0026quot;true\u0026quot;; if (-not $settingIsNew){ #Reset AAD based classsification? #$setting[\u0026quot;ClassificationList\u0026quot;] = \u0026quot;\u0026quot;; #$setting[\u0026quot;DefaultClassification\u0026quot;] = \u0026quot;\u0026quot;; #$setting[\u0026quot;ClassificationDescriptions\u0026quot;] = \u0026quot;\u0026quot;; } if ($settingIsNew){ New-AzureADDirectorySetting -DirectorySetting $setting; Write-Host \u0026quot;New directory settings for Group.Unified applied.\u0026quot; -ForegroundColor Green; $setting = Get-AzureADDirectorySetting | Where-Object { $_.DisplayName -eq \u0026quot;Group.Unified\u0026quot;}; } else{ Set-AzureADDirectorySetting -Id $setting.Id -DirectorySetting $setting; Write-Host \u0026quot;Updated directory settings for Group.Unified.\u0026quot; -ForegroundColor Green; $setting = Get-AzureADDirectorySetting | Where-Object { $_.DisplayName -eq \u0026quot;Group.Unified\u0026quot;}; } $setting.Values; ","date":"2020-08-17","permalink":"https://marcoscheel.de/post/2020/08/626684661029044224-enable-unified-labeling-for-microsoft-365-groups/","tags":["Microsoft 365","MIP"],"title":"Enable Unified Labeling for Microsoft 365 Groups (and Teams) in your tenant via PowerShell script"},{"content":"The Microsoft Teams Admin Center can be used to create a new Team. The initial dialog allows you to set multiple owners for the Team. This feature was added over time and is a welcome addition to make the life of an administrator easier. But the implementation has a big shortcoming: The specified owners in this dialog will not become a member of the underlying Microsoft 365 Group in Azure Active Directory. As a result all Microsoft 365 group services checking for members will not behave as expected. For example: These owners will not be able to access Planner. Other services like Teams and SharePoint work by accident. Let’s start with some basic information. Office Microsoft 365 Groups use a special AAD group type. But it is still a group in Azure Active Directory. A group in AAD is very similar to the old-school AD group in our on-premises directories. The group is made of members. In most on-premises cases these groups are managed by your directory admins. But also in AD you can specify owners of a group that will then be able to manage these groups\u0026hellip; if they have the right tool (dsa.msc, \u0026hellip;). In the cloud Microsoft (and myself) is pushing towards self-service for group management. This “self-service-first approach” is obvious since the introduction of now Office Microsoft 365 Groups. Teams being one of the most famous M365 Group services is also pushing to the owner/member model where end users are owners of a Team (therefor an AAD group). All end user facing UX from Microsoft is abstracting away how the underlying AAD group is managed. Teams for example will group owners and members in two sections: But if you look at the underlying AAD group you will find, that every owner is also a member:\nAnd the Azure AD portal shows a dedicated section to manage owners of the group: The Microsoft Admin portal also has dedicated sections for members and owners: In general it is important that your administrative staff is aware how group membership (including ownership) is working. The problem with the Teams Admin portal as mentioned in the beginning is the result of this initial dialog leaving the group in an inconsistent state without making that admin aware of this misalignment. Lets check the group created in the initial screenshot using the Teams admin center. We specified 5 admin users and one member after the initial dialog. Non of the admin users was added as a member in AAD (Microsoft 365 Admin portal screenshot): But looking at the teams Admin center won’t show this “misconfiguration”: If Leia wants to access the associated Planner service for this Team/Group the following error will show: Planner is checking against group membership. Only is the user is a member Planner (and other services) will check if the user is also an owner and then show administrative controls. Teams itself looks ok. In SharePoint implemented a hack and the owners of the AAD group are granted Site Collection Admin permission so every item is accessible, because that is what Site Collection Admins are for. If your users are reporting problem like this and based on the Teams Admin center everything looks ok, go check “a better” portal like AAD.\nTo fix the problem in the Teams Admin center the owner has to be demoted to member state and then promoted to being a owner again.\nA quick test with the SharePoint admin center shows that the system is managing membership as intended and every owner will also be a member. SharePoint requires one owner (Leia) in the first dialog and the second dialog allows additional owners and members: The result for members in AAD is correct as all owners are also added as members: Until this bug design flaw is fixed we recommend to not add more owners in the initial dialog. The currently logged-in admin user will be added as the only owner of the created group. In the next step remove your admin and add the requested users as members and promote them to owners. If you rely on self-service this should is not a problem. If you have a custom creation process automated hopefully you add all owner as member, but you should be good most of the time. We at glueckkanja-gab are supporting many customers with our custom lifecycle solution. We are adding a option to report this problem and a optional config switch to fix any detected misalignments.\nI’ve created a UserVoice “idea” for this “bug”. So please lets go: vote!\nhttps://microsoftteams.uservoice.com/forums/555103-public/suggestions/40951714-add-owners-also-as-members-aad-group-in-the-init\n","date":"2020-07-21","permalink":"https://marcoscheel.de/post/2020/07/624267623460454400-beware-of-the-teams-admin-center-to-create-new/","tags":["Microsoft 365","Microsoft Teams","Groups"],"title":"Beware of the Teams Admin center to create new teams (and assign owners)"},{"content":"Online Meetings sind aus den heutigen Unternehmen nicht mehr wegzudenken. Microsoft Teams ist die Meeting-Lösung im Microsoft 365 Service. Ein Teams Meeting ermöglicht es allen Teilnehmern, aktiv an der Besprechung teilzunehmen. Es gibt nur wenig Kontrolle für den Besprechungsleiter. Die Teilnehmer müssen die Disziplin aufbringen und sich im Meeting “korrekt” verhalten. Durch die aktuelle Corona-Krise wird aber deutlich, dass sich viele Teilnehmer schwerer damit tun als gedacht. Besonders für Neulinge sind die verschiedenen Optionen in der Software ungewohnt und Grundregeln für ein gutes Meeting eventuell unbekannt. Es wäre toll, wenn die Software hier besser unterstützt, aber der aktuelle Stand (”Mute all”, \u0026hellip;) wird sich kurzfristig nicht ändern.\nIch möchte euch heute eine Alternative zum klassischen Meeting zeigen. Es wird nicht für jede Situation passen, aber ihr solltet es kennenlernen und selbst entscheiden. Ich wurde von einer Schule angesprochen, welche Optionen es gibt, die Schüler Online zu unterrichten. Die Zielgruppe ist nicht besonders gut ausgebildet, wenn es um eine gute Meeting-Kultur geht :) Die Eltern und die Schüler dürften oft Anfänger sein und da passieren automatisch Fehler. In dieser Situation kann es sin machen auf ein Microsoft Teams Live Event auszuweichen.\nMicrosoft Teams Live Event Ein Live Event kann über den Teams Client geplant und durchgeführt werden. Anders als in einem normalen Meeting gibt es eine klare Trennung zwischen Meeting Organisator und Teilnehmern. Der Organisator wird zum Producer des Events (Meetings) und muss den Microsoft Team Client nutzen. Er kann weitere Personen als Producer einladen und das macht auch Sinn, damit man so ein Meeting professionelle über die Bühne bekommt. Die Teilnehmer verwenden den Browser (ohne Plugins oder ähnlicher) oder falls vorhanden den Teams Client. Im schulischen Szenario kann man sehr wenig Rückschlüsse über die technischen Möglichkeiten der Teilnehmer ziehen. Es ist also gut, dass die Lösung eigentlich mit allen Optionen zurechtkommt.\nFür die Teilnahme am Meeting gibt es zwei Links. Einmal der Link für den Producer (so ähnlich wie ein normaler Teams Meeting-Link) und nur über den Teams Client kommt man an den Teilnehmerlink. Das Meeting (besser Live Event) muss nach dem Beitritt des Producers nochmal über die Software gestartet werden, sonst sehen die Teilnehmer nichts.\nDas Meeting wird mit einem Zeitversatz (10-40 Sekunde) an die Teilnehmer gestreamt. Die Teilnehmer können nur über die eingebaute Frage und Antwort-Funktion (Q\u0026amp;A) mit den Producern kommunizieren. Die Zuschauer können das Meeting jederzeit **pausieren und zurück spulen. Es stehen mehr als 50 Sprachen für optionalen Live-Untertitel **zur Verfügung.\nÜbersicht der Limits\n Microsoft Teams Meeting Microsoft Teams Live Events  Live Event vorbereiten Der Benutzer benötigt eine Office 365 Lizenz (E1 oder höher, A3 oder höher) und der Administrator kann über eine Live Event Policy im Admin-Center festlegen, wer Events erstellen kann. Im Standard sind externe Benutzer nicht erlaubt. Für das Schul-Szenario ist es also wichtig, dass dieses Setting verändert wird.\nTeams Admin Center - Meetings - Live event policies - Global (Org-wide default)\n“Who can join scheduled live events” = “Everyone” statt “Everyone in your organization“.\nLive Event Planen In der Teams App wechselt ihr auf den Kalender und könnt dann oben rechts das DropDown auslösen und “Live event” wählen: Über Yammer kann man ebenfalls ein Live Event starten. Begebt euch in die entsprechende Gruppe/Community und dann findet ihr rechts im Bereich “Group Actions” die Möglichkeit ein Live Meeting zu planen. Über Microsoft Stream kann man zwar Live Events starten, allerdings muss man hier spezielle Software verwenden und kann nicht Microsoft Teams als Producer wählen. Wir wählen den Weg über Microsoft Teams und erstellen ein Live Event. Hier findet ihr die Anleitung von Microsoft. Es muss wie bei jedem Meeting ein **Titel **angegeben werden und die Start- sowie Endzeit. Die optionale Ortsangabe (Location) könnt ihr offenlassen, Microsoft Teams oder Online eintragen. An dieser Stelle ladet ihr auch die weiteren Presenter ein. Diese Personen stammen aus eurer Organisation und unterstützen euch später beim Erzeugen der Inhalte im Meeting. Auf der nächsten Seite werdet ihr nach den **Berechtigungen **gefragt. Wenn euer Administrator die richtigen Voreinstellungen getroffen hat, dann könnt ihr “Public” (Öffentlich) auswählen. Für das Scenario Schule ist **Public **zwingend erforderlich. Setzt ihr ein Live Event für eure Kollegen auf, dann könnt ihr natürlich auch Org-wide oder einzelne Personen auswählen. In beiden letzten Fällen wird ein Login des Unternehmens benötigt. Wichtig: Hier geht es um Berechtigungen und nicht darum, wer eingeladen wird. Teilnehmen kann an einem Org-wide Meeting nur wer auch den Teilnahme-Link erhalten hat! Das Live Event ist bereits als Teams Event gekennzeichnet und erlaubt so auch nur die Teams relevanten Einstellungen. Trefft eure Entscheidung, ob die Aufzeichnung für alle Teilnehmer verfügbar gemacht werden soll. Je nach Publikum bietet es sich an, dass “Live-Untertitel” angeboten werden. Es stehen mehr als 50 Sprachen zur Verfügung, aber ein Event kann immer nur 6 gleichzeitig anbieten. Ihr müsst hier entscheiden, welche Sprache gesprochen wird und in welche Sprache übersetzt werden kann. Die Konfiguration des Teilnehmer-Berichtes erlaubt es nach dem Meeting zu sehen (CSV), wer teilgenommen hat sowie wann und wie oft beigetreten wurde. Die Option für die “Frage\u0026amp;Antwort” Funktion (Q\u0026amp;A) sollte immer gewählt sein, damit Teilnehmer und Producer interagieren können. Das Erstellen des Live Events ist abgeschlossen und wird angezeigt. Über die Anzeige kann man dann auch den Teilnehmerlink kopieren. Dieser link muss dann an alle externen (oder internen) Teilnehmer weitergeleitet werden. Am besten wählt man hier den üblichen Kommunikationskanal (Teams, Email, WhatsApp, \u0026hellip;) für die Teilnehmergruppe. Der Link zum Teilnehmen sieht ungefähr so aus wie jeder andere Meeting Link: https://teams.microsoft.com/l/meetup-join/19%3ameeting_... aber für die Teilnehmer-Links endet der Link mit IsBroadcastMeeting=true\nLive Event produzieren Rechtzeitig vor Beginn des eigentlichen Live Events sollten sich alle Producer ins Meeting einwählen und die nötigen Vorbereitungen zum Start des Events durchführen. Für den Producer sieht der Beitritt so aus wie bei jeden anderen Meeting (Abweichung über dem Titel: “Als Produzent teilnehmen”). Wollt ihr euer Video übermitteln, so müsst ihr natürlich die Webcam aktivieren. Hinweis: Leider gibt es kein Background-Blur in Live Meetings. Achtet also auf eure Umgebung! Über die **Einstellungen **(Zahnrad mit dem aktuellen Audiogerät als Name) könnt ihr das passende Audiogerät verwenden. Es ist extrem wichtig hier die hochwertigsten Komponenten zu verwenden. Laptop Mikrophone und Lautsprecher erzeugen in der Regel das schlechteste Ergebnis. Ein Kopfhörer mit entsprechendem Mikro macht es für alle einfacher und reduzieren Nebengeräusche. Der Auditorium-Modus sollte in Zeiten von Social Distancing nicht relevant sein, aber eventuell für zukünftige Ereignisse von Interesse sein, um Publikum nicht durch Teams rausrechnen zu lassen. Hinweis: Wenn ihr unsicher seid, dann testet eure Optionen mit einem Testanruf! Seid ihr dem Live Event beigetreten, dann seht ihr die Producer Oberfläche und spätestens jetzt wird klar, dass es kein normales Meeting ist und warum ihr das vorher üben solltet :) Es gibt zwei Inhaltsanzeigen. Links befindet sich der Inhalt, der noch nicht Live gesendet wird. Hier bereitet ihr die Szene vor, welche dann Live geschaltet wird. Rechts seht ihr, was die Teilnehmer sehen werden und sich gerade “On-Air” befindet. Mit dem Beitritt zum Live Event wird das Meeting also noch nicht gestartet! Hierfür muss man erstmal die Inhalte anordnen.\nWenn ihr eure Webcam übertragen wollt, dann schaltet auf das entsprechende Layout unten links um. Jetzt könnt ihr aus dem unteren Bereich eure Webcam nach oben in den kleinen Bereich des linken Fensters ziehen. Genau so geht das auch mit den Inhalten, die ihr teilen wollt. Ihr könnt ein einzelnes Fenster freigeben oder den ganzen Desktop. Ich empfehle immer den **Desktop **freizugeben, da es kompliziert wird, wenn man auch andere Dinge zeigen möchte. Es kommt schneller als man denkt. Oben: Fenster freigeben | Unten: Desktop freigeben Wie in jedem anderen Meeting wird nun das geteilte Element rot umrandet und es gibt im oberen Bereich die Möglichkeit weitere Kontrollen einzustellen oder abzubrechen. In der Producer-Ansicht habt ihr nun Video und Inhalt ausgerichtet. Wenn die Zeit gekommen ist, dann muss man die Inhalte “Live schalten”. Das Live Event ist aber noch immer nicht für die Teilnehmer sichtbar. Erst mit einem weiteren Klick auf “Start” werden die Inhalte (Audio und Video) an die Teilnehmer übermittelt. Auf der linken Seite, kann zum Beispiel ein weiterer Producer das nächste Layout (Speaker, Speaker-Runde, \u0026hellip;) vorbereitet und gegebenenfalls sofort “Live schalten\u0026quot; Es ist ein Microsoft Produkt also wird nochmal nachgefragt! In der Producer-Ansicht wird der sichtbare Teil für die Teilnehmer rot umrandet. Stoppt der aktuelle Benutzer seine Freigabe, dann wird sein Video in Vollbilddarstellung gebracht. In der rechten Leiste der Produceransicht gibt es verschiedene Reiter mit Informationen zum Meeting.\nStaus und Leistung Frage und Antwort Teilnehmer können hier Fragen stellen und Producer können diese privat beantworten. ist eine Frage/Antwort für alle Teilnehmer interessant, dann kann der Producer die Frage (samt Antwort) veröffentlichen und so allen zugänglich machen.\nBesprechungsnotizen Die Notizen werden wie aus Meetings bekannt, über die Wiki-Funktion abgewickelt. Es gibt eine rudimentäre Struktur und Formatierung der Inhalte für die Teilnehmer. Die Notizen können nur vom Producer erstellt werde und sind sofort für die Teilnehmer sichtbar.\nBesprechungschat (nur Producer) Der Chat ist nur für Producer und kann nicht von Teilnehmern eingesehen werden. Teilnehmer können nur über die Frage\u0026amp;Antwort Funktion interagieren. Eine Interaktion unter Teilnehmern ist ausgeschlossen.\nKontakte Hier werden alle Producer aufgelistet. Teilnehmer sind hier nicht sichtbar.\nGeräteeinstellungen Hier kann das Audio und Webcam Setup (Producer) verändert werden. Es können auch nachträglich die Live-Untertitel abgeschaltet werden.\nBesprechungsdetails Wie in jedem Meeting werden auch hier die Einwahlinformationen für das Meeting angezeigt. Hinweis: Diese Einladung ist nur für Producer und nicht für Teilnehmer geeignet!\nLive Event teilnehmen Die Teilnahme an einem Live Event kann mit einem Browser oder Microsoft Teams Client durchgeführt werden. Hier findet ihr die Systemanforderungen. Mit dem Teilnehmerlink kommt man auf eine Website, welcher dem normalen Meeting Beitritt sehr ähnelt. Über “Watch on the web instead” kann man über alle modernen Browser teilnehmen, ohne extra Software zu installieren. Wenn man über ein Teams Account (Azure AD) verfügt, dann kann dieser genutzt werden oder ihr tretet anonym bei. Nach dem Beitritt seht ihr den aktuell geteilten Inhalt. Die Ansicht ähnelt einem normalen Meeting. Der Teilnehmer kann aber jederzeit Pause machen oder sogar zurückspulen.\nÜber die Einstellungen im Wiedergabefenster können die optionalen Untertitel eingeschaltet werden. Die Auswahl entspricht den Einstellungen, welche zum Setup des Live Events gemacht wurden. Hier sehen wir den englischen Untertitel. Frage \u0026amp; Antwort\nHier haben wir die Teilnehmersicht der Q\u0026amp;A Funktion. Der Teilnehmer kann seine Fragen stellen und bekommt gegebenenfalls Antworten vom Producer Team. Der Teilnehmer kann sich für die Frage einen Namen geben oder aber anonym schreiben. Die Fragen können weiter “verfeinert” werden. Hier sehen wir die Producer Ansicht für die gestellt Frage. Die Frage kann ignoriert oder veröffentlicht werden (sichtbar für alle Teilnehmer). Wird die Frage (inclusive Antwort) veröffentlicht, dann wird sie im Bereich “Featured” angezeigt. Hier gibt es die Möglichkeit für die Teilnehmer eine Frage zu “liken” und damit Feedback in die Runde zu geben. Ist das Live Event beendet, dann wird es entsprechend im Stream angezeigt. Zusammenfassung Live Events sind eine sehr spezielle Form von Meetings in Microsoft 365. Der Einsatz macht nur in wenigen Szenarien Sinn, aber es ist wichtig, diese Option zu kennen. Besonders mit vielen “Meeting” Anfängern kann es die Situation vereinfachen, wenn eventuell zwei Meetings aufgesetzt werden. Ein Live Meeting für den Transport der Inhalte mit absoluter Kontrolle über die Präsentation ohne störende Zwischenrufe und ein anschließendes Ad-Hoc Q\u0026amp;A Meeting im normalen Teams Meeting Modus, damit jeder sprechen kann und sich auch per Video präsentiert.\n","date":"2020-03-30","permalink":"https://marcoscheel.de/post/2020/03/614015532121931776-microsoft-teams-live-events-f%C3%BCr-krisen-vortr%C3%A4ge/","tags":["Microsoft 365","Microsoft Teams"],"title":"Microsoft Teams Live Events für (Krisen-)Vorträge"},{"content":"My buddy Oliver Kieselbach did a blog post about the capabilities of Microsoft Quick Assist (as part of the current operation system). In his post he raised the question if Microsoft Teams is not enough for this kind of IT support scenarios. Check out his blog to see it live in action and what the biggest shortcoming is. Microsoft Teams is not a good option for anything UAC related. Even without the so called secure desktop feature Microsoft Teams will not allow the support staff to enter admin credentials if needed. I also would suggest (for the most customers) to pick a proper IT support tool for these scenarios.\nMicrosoft Teams is the hub for teamwork, but that doesn’t mean you could not support your colleagues, if they are experiencing non admin related issues. But first things first, we should check, if your tenant settings are ready to allow remote control during a desktop sharing session. In quite a few customer environments I’ve noticed that remote sharing is restricted or completely disabled.\nDesktop sharing and remote control is configured through the tenant meeting policies. Check out your teams admin center: Meeting policies - Pick a policy (”Global - Org Wide Default”) - Content sharing Ensure that “Screen sharing mode“ is set to “Entire screen” (1) otherwise remote control will be limited to a single window. I’ve seen people struggle using the single app sharing mode more than once. For a remote support scenario please enable “Allow a participant to give or request control” (2)! If your users are asking for support from a valuable and skilled colleague, don’t waste their time yelling what button to press next. The last option needs a decision, if you want to allow the same privilege to be granted for people outside of your organization. I’m a fan of enabling “Allow an external participant to give or request control” (3), because I’m often the external user trying to help, but please align with your corporate security requirements. By the way: Settings (1) and (2) are configured like shown by default in any tenant!\n1:1 Now that we have setup the prerequisites let’s have a look at the user experience. In my scenario Luke is trying to organize a new funding round to order some spaceships. Now he tries to prepare a nice Excel to present at the next procurement meeting, but he is not happy with the visual display of one of his charts. He needs help from an expert and he is in contact with Leia (she is running the rebellion so she is awesome in Excel!). Luke is starting a chat to make his point:\nTo start the screen sharing in a 1:1 session you will find the icon for screen sharing (1) in the top right corner. Luke needs to start it and share the complete screen (2):\nLeia will receive a request to accept the screen sharing session. You should only accept a request if you talked/chatted with the person! Otherwise you could end up seeing things you don’t want to see. If you started through chat the system will asked you if it is a good idea to add audio to the conversation. Normally this is a good idea. Especially if you are not willing to give control to the person you request help from. Because Leia is a busy employee and has lots of stuff to do she will request control, because the particular action to execute is hard to describe and maybe takes some poking around with various settings. So Leia requests to have control over the screen/application that was shared. In the far right part of the call control bar you will find the option to request control. If this option is missing, talk to your Teams admins! They didn’t execute all prerequisites as described above: Luke will see the request at the top of the shared content and has the option to accept or deny sharing. Sometimes meeting participants accidently hit the button so think twice if this is what you want: Now comes a really impressive upgrade from previous Skype for Business based screen sharing. Both parties are represented by their Teams profile avatar (in my case the Office 365 profile pictures). You will always see what the other is pointing at or clicking. I often prefer in a support case that the requesting party will do all the clicks and I’m just advising what\u0026rsquo;s up next and where to find it on screen. With this solution I think it is a great learning opportunity and I can gently show where to click instead of yelling where not to click :)\nHere are the two screens side by side (Luke on the left vs Leia on the right): Leia fixed the request by switching to a logarithmic scale and Luke is happy and the session can be ended. Leia can click on “Stop control”:\nOr Luke can end the session (”Cancel Control”):\nJust for completeness this is the way to give control if the supported doesn’t find the “Request control” option: Extra: Meetings In a normal meeting everything is working alike, but the UX is looking a bit different. If you are in a scheduled meeting the share button is located in the call control bar: If you select the icon a new screen will appear from the bottom of the teams app: (1) Share the complete screen. If you have more than one screen, you can only share one at a time. (2) Share a window. (3) You can also upload a PowerPoint presentation, but this is beyond a remote control/support session. (4) Open a whiteboard, but this is beyond a remote control/support session. (5) While sharing your screen or an application also include audio (for example a Microsoft Stream video that you want to trim/edit) Note: This option is only available in planed meetings and not 1:1 support session started from chat.\nConclusion The capabilities for remote support in Microsoft Teams are available and very useful. Thinks like the AAD picture next to the mouse cursor is a great addition and helping a lot. Is Teams a better remote support tool then Quick Assist?\nFor your IT staff: No! A proper Remote Assist tool will be a better choice.\nFor your typical Information Worker: Yes! Sharing your desktop to get support from a colleague is a quick and proper solution! No need to walk to someone’s desk and touch a maybe filthy mouse/keyboard. Not always do you have the right person in the same building.\nI definitely know that Microsoft Quick Assist is not a proper collaboration solution. Try to co-author a Excel document for the next #FreeCookiesFriday campaign ;) ","date":"2020-03-22","permalink":"https://marcoscheel.de/post/2020/03/613300443898068992-is-microsoft-teams-a-remote-support-tool/","tags":["Microsoft 365","Microsoft Teams"],"title":"Is Microsoft Teams a remote support tool?"},{"content":"Microsoft is documenting a list of things (about Microsoft Teams) that are not working as expected by Microsoft or customers. The list has been around for quite some time. New things are added, but I noticed things on the list that are clearly working as of this writing are not removed. So let’s have a look together and maybe you support my pull request.\nThe article: Known issues for Microsoft Teams\nMy pull request:\nKnown issues that are no longer known issues\nAudit logs may report an incorrect username as initiator when someone has been removed from a Team\n Teams team is a modern group in AAD. When you add/remove a member through the Teams user interface, the flow knows exactly which user initiated the change, and the Audit log reflects the correct info. However, if a user adds/removes a member through AAD, the change is synced to the Teams backend without telling Teams who initiated the action. Microsoft Teams picks the first owner of team as the initiator, which is eventually reflected in the Audit log as well. https://docs.microsoft.com/en-us/microsoftteams/known-issues#administration\nRelated Issue: [Audit logs may report an incorrect username as initiator when someone has been removed from a Team\n](https://docs.microsoft.com/en-us/microsoftteams/known-issues#teams)\n I validated group membership edits from the new Microsoft Admin Portal (https://admin.microsoft.com) and from the Azure Portal (https://portal.azure.com). In both cases the audit logs showed the correct user. In both cases I used my cloudadmin account that is not part of the team and the audit logs documented the operation as the executing user.  Unable to delete connectors as a team owner\n Attempting to delete a connector as an owner, that can otherwise add a connector, while \u0026ldquo;Allow members to create, update, and remove connectors\u0026rdquo; is disabled throws an error indicating the user does not have permission to do so. Source: https://docs.microsoft.com/en-us/microsoftteams/known-issues#apps\n I tested this in my lab environment in various combinations and I did not run into this issue. For example:\n Leia created a team and added Luke as a member Luke added an Incoming Webhook as a connector Leia didn’t like Luke’s webhook so she decided to remove the member permission to configure connectors for the team Luke wanted to add another connector, but the option is now missing from his context menu for the channel Leia deleted the Incoming Webhook that look created without a problem   Planner on single sign-on (SSO) build\n SSO does not apply to Planner. You will have to sign in again the first time you use Planner on each client. Source: https://docs.microsoft.com/en-us/microsoftteams/known-issues#authentication\n I’m using planner within Microsoft Teams on a weekly basis (not on a daily basis as some of my colleagues would like me to use it) and it is working as expected.\n Wiki not created for channels created by guests\n When a guest creates a new channel, the Wiki tab is not created. There isn\u0026rsquo;t a way to manually attach a Wiki tab to the channel. Source: https://docs.microsoft.com/en-us/microsoftteams/known-issues#guest-access\n I did check this in my lab tenant using my work account as a guest.\n Luke created a team Luke added my work account as a guest to the team Luke configured the teams guest permission to allow channel creation I opened teams in my browser and switch to the lab tenant (friends don’t let friends switch tenants in the real teams app, even with fast tenant switching!) I opened the team and created a new channel Wiki tab was present and working   Teams Planner integration with Planner online\n Tasks buckets in Planner do not show up in Planner online experience. Source: https://docs.microsoft.com/en-us/microsoftteams/known-issues#tabs\n This is core feature of Planner and the issue was created 2+ year ago. It is just working as expected.  Unable to move, delete or rename files after editing\n After a file is edited in Teams it cannot be moved or renamed or deleted immediately Source: https://docs.microsoft.com/en-us/microsoftteams/known-issues#teams\n I tried this with a mix of accounts and apps (Teams app and Teams in the browser) today. I could not reproduce it. It is still a “common” issue SharePoint Online but I never experienced it or got clients reporting this issue regarding Teams.\n A team name with an \u0026amp; symbol in it breaks connector functionality\n When a team name is created with the \u0026amp; symbol, connectors within the Team/Group cannot be established. Source: https://docs.microsoft.com/en-us/microsoftteams/known-issues#teams\n I created two teams:\n Good \u0026amp; Bad Characters Only Good Characters  The connector option for both Teams showed up at about the same time. Maybe it is related to the special character but it took quite some time until the Exchange Online mailbox was provisioned. Longer than any test today for all the other use cases. But at the end I had no problems managing connectors for a Team with an “\u0026amp;” or without it in the name.\nI find the Microsoft article in general very valuable and I noticed a few other things I want to talk about in the future. So stay tuned.\n","date":"2019-12-22","permalink":"https://marcoscheel.de/post/2019/12/189806058922-microsoft-teams-known-issues-no-more/","tags":["Microsoft 365","Microsoft Teams"],"title":"Microsoft Teams - Known issues... no more!?"},{"content":"This post will introduce you to the Azure AD Access Review feature. With the introduction of modern collaboration through Microsoft 365 and Microsoft Teams being the main tool it is important to mange who is a member to the underlying Office 365 Group (Azure AD Group).\n\u0026lt;DE\u0026gt;Für eine erhöhte Reichweite wird der Post heute auf Englisch erscheinen. Es geht um die Einführung von Access Reviews (Azure AD) im Zusammenspiel mit Microsoft Teams. Das Verwalten der Mitgliedschaft eines Teams wird durch den Einsatz von diesem Feature unterstützt und stellt die Besitzer weiter in den Mittelpunkt. Sollte großes Interesse an einer komplett deutschen Version bestehen, dann lasst es mich bitte wissen.\u0026lt;/DE\u0026gt;\nMicrosoft has great resources to get started on a technical level. The feature enables a set of people to review another set of people. Azure AD is leveraging this capability (all under the bigger umbrella called Identity Governance) on two assets: Azure AD Groups and Azure AD Apps. Microsoft Teams as a hub for collaboration is build on top of Office 365 Groups and so we will have a closer look at the Access Review part for Azure AD Groups.\nEach Office 365 Group (each Team) is build from a set of owners and members. With the open nature of Office 365, members can be employees, contractors, or people outside of the organization. In our modern collaboration (Teams, SharePoint, \u0026hellip;) implementation we strongly recommend to leverage full self service group creation that is already built into the system. With this setup everyone is able to create and manage/own a group. Permanent user education is needed for everyone to understand the concept behind modern groups. Many organizations also have a strong set of internal rules that forces a so called information owner (could be equal to the owner of a group) to review who has access to their data. Most organization rely on the fact people are fulfilling their duties as demanded, but lets face it owners are just human beings that need to do their “real” job. With the introduction of Azure AD Access Review we can support these owner duties and make the process documented and easy to execute.\nAAD Access Review can do the following to support an up to date group membership:\n Setup an Access Review for an Azure AD Group Specify the duration (start date, recurrence, duration, \u0026hellip;) Specify who will do the review (owner, self, specific people, \u0026hellip;) Specify who will be reviewed (all members, guests, \u0026hellip;) Specify what will happen if the review is not executed (remove members, \u0026hellip;)  Before we start we need to talk about licensing. It is obvious that M365 E5 is the best SKU to start with ;) but if you are not that lucky, you need at least an Azure AD P2 license. It is not a “very” common license as it was only part of the EMS E5 SKU, but Microsoft started some time ago really attractive license bundles. Many orgs with strong security requirements will at some point hit a license SKU that will include AAD P2. For your trusty lab tenants start a EMS E5 trial to test these features today. To be precise only the accounts reviewing (executing the Access Review) need the license, at least this is my understanding and as always with licensing ask your usual licensing people to get the definitive answer.\nThe setup of an Access Review (if not automated through MS Graph Beta) is setup in the Azure Portal in the identity governance blade of AAD. To create our first Access Review we need to on-board to this feature. Please note we are looking at Access Review in the context of modern collaboration (groups created by Teams, SharePoint, Outlook, \u0026hellip;). Access Review can be used to review any AAD group that you use to grant access to a specific resource or keep a list of trusted users for an infrastructure piece of tech in Azure. The following information might not always be valid for your scenario!\nThis is the first half of the screen we need to fill-out for a new Access Review: Review name: This is a really important piece! The Review name will be the “only” visible clue for the reviewer once they get the email about the outstanding review. With self service setup and with the nature of how people name their groups we need to ensure people are understanding what they review. We try to automate the creation of the reviews so we put the review timing, the group name and the groups object id in the review name. The ID is helping during support if you send out 4000 Access Reviews and people ask why they got this email they can provide you with the ID and things get easier. For example: 2019-Q1 GRP New Order (af01a33c-df0b-4a97-a7de-c6954bd569ef)\nFrequency: Also very important! You have to understand that an Access Review is somehow static. You could do a recurring review, but some information will be out of sync. For example the group could be renamed, but the title will not be updated and people might get confused about misleading information in the email that is send out. If you choose to let the owner of a group do the review, the owners will be “copied” to the Access Review config and not updated for future reviews. Technically this could be fixed by Microsoft, but as of now we ran into problems in the context of modern collaboration. Users: \u0026ldquo;Members of a group” is our choice for collaboration. The other option is “Assigned to an application” and not our focus. For a group we have the option to do a guest only review or review everybody as a member of a group. Based on organizational needs and information like the confidentiality we can make a decision. As a starting point it could be a good option to go with guests only because guests are not very well controlled in most environments. An employee at least has a contract and the general trust level should be higher.\nGroup: Select a group the review should apply to. The latest changes to the Access Review feature allowed to select multiple groups at once. From a collaboration perspective I would avoid it, because at the end of the creation process each group will have its own Access Review instance and the settings are no longer shared. Once again from a collab point of view we need some kind of automation because it is not feasible to create these reviews by an manual task in a foreseeable future.\nReviewers: The natural choice for an Office 365 Group (Team) is to go with the “Group owners” option. Especially if we automate the process and don’t have an extra database to lookup who is the information owner. For static groups or highly confidential groups the option “Selected users” could make sense. An interesting option is also the last one called “Members (self)”. This option will \u0026ldquo;force” each member to take a decision if the user is any longer part of this project, team or group. We at Glück \u0026amp; Kanja are currently thinking about doing this for some of our internal clients teams. Most of our groups are public and accessible by most of the employee, but membership will document some kind of current involvement for the client represented by the group. This could also naturally reduce the number of teams that show up in your Microsoft Teams client app. As mentioned earlier at the moment it seems that the option “Group owners” will be resolved once the Access Review starts and the instance of the review is then fixed. So any owner change could be not reflected in future instances in recurring reviews. Hopefully this will be fixed by Microsoft.\nProgram: This is a logical grouping of access reviews. For example we could add all collaboration related reviews to one program vs administration reviews with a more static route. More advanced settings are collapsed, but should definitely be reviews.\nUpon completion settings: Allows to automatically apply the review results. I would suggest to try this settings, because it will not only document the review but take the required action on the membership. If group owners are not aware what these Access Review email are, then we talk about potential loss of access for members not reviewed, but at the end that is what we want. People need to take this part of identity governance for real and take care of their data. Any change by the system is document (Audit log of the group) and can be reverse manually. If the system is not executing the results of the review, someone must look up results regularly and then ensure to remove the users based on the outcome. If you go for Access Review, I strongly recommend on automatically applying the results (after you own internal tests).\nLets take a look on the created Access Review. Azure Portal: This is an overview for the admin (non recurring access review). Email: As you can see the prominent Review name is what is standing out to the user. The group name (also highlighted red) is buried within all other text. Click on “Start Review” from the email: The user now can take action based on recommendations (missing in my lab tenant due to inactivity of my lab users). Take Review: Accept 6 users.\nReview Summary: This is the summary if the owner has taken all actions. Azure Portal: Audit log information for the group.\nAfter the user completed the review the system didn’t make a change to the group. Based on the configuration if actions should be automatically applied the results apply at the end of the review process! Until this time the owners can change their mind. Once the review period is over the system will apply the needed changes.\nI really love this feature in the context of modern collaboration. The process of keeping a current list of involved members in a team is a big benefit for productivity and security. The “need to know” principal is supported by a technical implementation “free of cost” (a mentioned everyone should have AAD P2 through some SKU 😎).\nOur GK O365 Lifecycle tool was extended to allow the creation of Access Reviews through the Microsoft Graph based on the Group/Team classification. Once customers read or get a demo about this feature and own the license we immediately start a POC implementation. If our tool is already in place it is only a matter of some JSON configuration to be up and running.\n","date":"2019-08-02","permalink":"https://marcoscheel.de/post/2019/08/186728523052-manage-microsoft-teams-membership-with-azure-ad/","tags":["Microsoft 365","Azure AD","Microsoft Teams"],"title":"Manage Microsoft Teams membership with Azure AD Access Review"},{"content":"Für unser Glück \u0026amp; Kanja Lifecycle Tool setze ich im Schwerpunkt auf Microsoft Graph Calls. Für ein sauberes Setup habe ich mittlerweile ein Script. Es nutzt die PowerShell AZ und die Azure CLI. Besonders beim Erstellen einer Azure AD App (genauer Berechtigen und Granten) ist die Azure CLI noch ein Stück besser bzw. umfangreicher als die AZ PowerShell.\nDie Lifecycle App arbeitet mit AD Settings und Groups. Erweiterte Funktionen setzen auf Access Reviews Feature aus dem AAD P2 Lizenzset. Diese Graph Berechtigungen setze ich direkt per CLI Script:\naz ad app permission add --id $adapp.ApplicationId --api 00000003-0000-0000-c000-000000000000 --api-permissions 19dbc75e-c2e2-444c-a770-ec69d8559fc7=Role #msgraph Directory.ReadWrite.All az ad app permission add --id $adapp.ApplicationId --api 00000003-0000-0000-c000-000000000000 --api-permissions 62a82d76-70ea-41e2-9197-370581804d09=Role #msgraph Group.ReadWrite.All az ad app permission add --id $adapp.ApplicationId --api 00000003-0000-0000-c000-000000000000 --api-permissions ef5f7d5c-338f-44b0-86c3-351f46c8bb5f=Role #msgraph AccessReview.ReadWrite.All az ad app permission add --id $adapp.ApplicationId --api 00000003-0000-0000-c000-000000000000 --api-permissions 60a901ed-09f7-4aa5-a16e-7dd3d6f9de36=Role #msgraph ProgramControl.ReadWrite.All  Die Azure CLI kann dann auch gleich noch den Admin Grant erledigen (wenn man nicht in der Azure Cloud Shell läuft!):\naz ad app permission admin-consent --id $adapp.ApplicationId  Hier ein Beispiel, wie das Ergebnis dann im Azure AD Portal aussieht: Wer nun die Guid für seine Berechtigung sucht, kann ganz einfach mit diesem Befehlt (Azure Active Directory PowerShell 2.0) auf das ständig wachsende Set an App Permissions zugreifen:\n(Get-AzureADServicePrincipal -filter \u0026quot;DisplayName eq 'Microsoft Graph'\u0026quot;).AppRoles | Select Id, Value | Sort Value Id Value -- ----- d07a8cc0-3d51-4b77-b3b0-32704d1f69fa AccessReview.Read.All ef5f7d5c-338f-44b0-86c3-351f46c8bb5f AccessReview.ReadWrite.All 18228521-a591-40f1-b215-5fad4488c117 AccessReview.ReadWrite.Membership 134fd756-38ce-4afd-ba33-e9623dbe66c2 AdministrativeUnit.Read.All 5eb59dd3-1da2-4329-8733-9dabdc435916 AdministrativeUnit.ReadWrite.All 1bfefb4e-e0b5-418b-a88f-73c46d2cc8e9 Application.ReadWrite.All 18a4783c-866b-4cc7-a460-3d5e5662c884 Application.ReadWrite.OwnedBy b0afded3-3588-46d8-8b3d-9842eff778da AuditLog.Read.All 798ee544-9d2d-430c-a058-570e29e34338 Calendars.Read ef54d2bf-783f-4e0f-bca1-3210c0444d99 Calendars.ReadWrite a7a681dc-756e-4909-b988-f160edc6655f Calls.AccessMedia.All 284383ee-7f6e-4e40-a2a8-e85dcb029101 Calls.Initiate.All 4c277553-8a09-487b-8023-29ee378d8324 Calls.InitiateGroupCall.All f6b49018-60ab-4f81-83bd-22caeabfed2d Calls.JoinGroupCall.All fd7ccf6b-3d28-418b-9701-cd10f5cd2fd4 Calls.JoinGroupCallAsGuest.All 7b2449af-6ccd-4f4d-9f78-e550c193f0d1 ChannelMessage.Read.All 4d02b0cc-d90b-441f-8d82-4fb55c34d6bb ChannelMessage.UpdatePolicyViolation.All 6b7d71aa-70aa-4810-a8d9-5d9fb2830017 Chat.Read.All 294ce7c9-31ba-490a-ad7d-97a7d075e4ed Chat.ReadWrite.All 7e847308-e030-4183-9899-5235d7270f58 Chat.UpdatePolicyViolation.All 089fe4d0-434a-44c5-8827-41ba8a0b17f5 Contacts.Read 6918b873-d17a-4dc1-b314-35f528134491 Contacts.ReadWrite 1138cb37-bd11-4084-a2b7-9f71582aeddb Device.ReadWrite.All 7a6ee1e7-141e-4cec-ae74-d9db155731ff DeviceManagementApps.Read.All dc377aa6-52d8-4e23-b271-2a7ae04cedf3 DeviceManagementConfiguration.Read.All 2f51be20-0bb4-4fed-bf7b-db946066c75e DeviceManagementManagedDevices.Read.All 58ca0d9a-1575-47e1-a3cb-007ef2e4583b DeviceManagementRBAC.Read.All 06a5fe6d-c49d-46a7-b082-56b1b14103c7 DeviceManagementServiceConfig.Read.All 7ab1d382-f21e-4acd-a863-ba3e13f7da61 Directory.Read.All 19dbc75e-c2e2-444c-a770-ec69d8559fc7 Directory.ReadWrite.All 7e05723c-0bb0-42da-be95-ae9f08a6e53c Domain.ReadWrite.All 7c9db06a-ec2d-4e7b-a592-5a1e30992566 EduAdministration.Read.All 9bc431c3-b8bc-4a8d-a219-40f10f92eff6 EduAdministration.ReadWrite.All 4c37e1b6-35a1-43bf-926a-6f30f2cdf585 EduAssignments.Read.All 6e0a958b-b7fc-4348-b7c4-a6ab9fd3dd0e EduAssignments.ReadBasic.All 0d22204b-6cad-4dd0-8362-3e3f2ae699d9 EduAssignments.ReadWrite.All f431cc63-a2de-48c4-8054-a34bc093af84 EduAssignments.ReadWriteBasic.All e0ac9e1b-cb65-4fc5-87c5-1a8bc181f648 EduRoster.Read.All 0d412a8c-a06c-439f-b3ec-8abcf54d2f96 EduRoster.ReadBasic.All d1808e82-ce13-47af-ae0d-f9b254e6d58a EduRoster.ReadWrite.All 38c3d6ee-69ee-422f-b954-e17819665354 ExternalItem.ReadWrite.All 01d4889c-1287-42c6-ac1f-5d1e02578ef6 Files.Read.All 75359482-378d-4052-8f01-80520e7db3cd Files.ReadWrite.All 5b567255-7703-4780-807c-7be8301ae99b Group.Read.All 62a82d76-70ea-41e2-9197-370581804d09 Group.ReadWrite.All e321f0bb-e7f7-481e-bb28-e3b0b32d4bd0 IdentityProvider.Read.All 90db2b9a-d928-4d33-a4dd-8442ae3d41e4 IdentityProvider.ReadWrite.All 6e472fd1-ad78-48da-a0f0-97ab2c6b769e IdentityRiskEvent.Read.All db06fb33-1953-4b7b-a2ac-f1e2c854f7ae IdentityRiskEvent.ReadWrite.All dc5007c0-2d7d-4c42-879c-2dab87571379 IdentityRiskyUser.Read.All 656f6061-f9fe-4807-9708-6a2e0934df76 IdentityRiskyUser.ReadWrite.All 19da66cb-0fb0-4390-b071-ebc76a349482 InformationProtectionPolicy.Read.All 810c84a8-4a9e-49e6-bf7d-12d183f40d01 Mail.Read e2a3a72e-5f79-4c64-b1b1-878b674786c9 Mail.ReadWrite b633e1c5-b582-4048-a93e-9f11b44c7e96 Mail.Send 40f97065-369a-49f4-947c-6a255697ae91 MailboxSettings.Read 6931bccd-447a-43d1-b442-00a195474933 MailboxSettings.ReadWrite 658aa5d8-239f-45c4-aa12-864f4fc7e490 Member.Read.Hidden 3aeca27b-ee3a-4c2b-8ded-80376e2134a4 Notes.Read.All 0c458cef-11f3-48c2-a568-c66751c238c0 Notes.ReadWrite.All c1684f21-1984-47fa-9d61-2dc8c296bb70 OnlineMeetings.Read.All b8bb2037-6e08-44ac-a4ea-4674e010e2a4 OnlineMeetings.ReadWrite.All 0b57845e-aa49-4e6f-8109-ce654fffa618 OnPremisesPublishingProfiles.ReadWrite.All b528084d-ad10-4598-8b93-929746b4d7d6 People.Read.All 246dd0d5-5bd0-4def-940b-0421030a5b68 Policy.Read.All 79a677f7-b79d-40d0-a36a-3e6f8688dd7a Policy.ReadWrite.TrustFramework eedb7fdd-7539-4345-a38b-4839e4a84cbd ProgramControl.Read.All 60a901ed-09f7-4aa5-a16e-7dd3d6f9de36 ProgramControl.ReadWrite.All 230c1aed-a721-4c5d-9cb4-a90514e508ef Reports.Read.All 5e0edab9-c148-49d0-b423-ac253e121825 SecurityActions.Read.All f2bf083f-0179-402a-bedb-b2784de8a49b SecurityActions.ReadWrite.All bf394140-e372-4bf9-a898-299cfc7564e5 SecurityEvents.Read.All d903a879-88e0-4c09-b0c9-82f6a1333f84 SecurityEvents.ReadWrite.All a82116e5-55eb-4c41-a434-62fe8a61c773 Sites.FullControl.All 0c0bf378-bf22-4481-8f81-9e89a9b4960a Sites.Manage.All 332a536c-c7ef-4017-ab91-336970924f0d Sites.Read.All 9492366f-7969-46a4-8d15-ed1a20078fff Sites.ReadWrite.All 21792b6c-c986-4ffc-85de-df9da54b52fa ThreatIndicators.ReadWrite.OwnedBy fff194f1-7dce-4428-8301-1badb5518201 TrustFrameworkKeySet.Read.All 4a771c9a-1cf2-4609-b88e-3d3e02d539cd TrustFrameworkKeySet.ReadWrite.All 405a51b5-8d8d-430b-9842-8be4b0e9f324 User.Export.All 09850681-111b-4a89-9bed-3f2cae46d706 User.Invite.All df021288-bdef-4463-88db-98f22de89214 User.Read.All 741f803b-c850-494e-b5df-cde7c675a1ca User.ReadWrite.All ","date":"2019-07-08","permalink":"https://marcoscheel.de/post/2019/07/186138885112-app-permissions-f%C3%BCr-microsoft-graph-calls/","tags":["Microsoft 365","Azure AD","Microsoft Graph","Development"],"title":"App Permissions für Microsoft Graph Calls automatisiert einrichten"},{"content":"Der Microsoft Graph ist das “Schweizer Taschenmesser” für alle im Microsoft 365 Umfeld. Eine API für “alle” Dienste und noch besser immer das gleiche Authentifizierungsmodel. Im Hairless in the Cloud Podcast Nummer 18 habe ich meine Eindrücke zum Microsoft Graph schon geschildert. Der Graph Explorer auf der Website ist eine gute Methode den Graph kennenzulernen. Ich für meinen Teil bewege mich aber überwiegend ohne Benutzerinteraktion im Graph und somit nutze ich in meinen Anwendungen die Application Permissions. Die meisten APIs (vgl. Teams) kommen allerdings erstmal ohne App Permissions daher. Die Enttäuschung ist groß, wenn man über den Graph Explorer sein Research gemacht hat und dann feststellt, dass die Calls als App Permission scheitern.\nJeremy Thake aus dem Microsoft Graph Team hat vor einigen Monaten angefangen, die Samples (und mehr) aus dem Graph Explorer in einer Collection für Postman zu veröffentlichen. Diese Collection vereinfacht das Testen der eigenen Calls und gibt Anregung für neue Szenarien.\nIn der Vergangenheit habe ich mir aus meiner Azure Function das Token “geklaut” und dann im Postman als Bearer Token direkt hinterlegt: Es gibt aber eine viel elegantere Version. Die MS Graph Postman Collection arbeitet mit dem Environment und Variablen. Eine Methode, die eigentlich dem Code in der eigene App (bei mir eine Azure Function) entspricht, ist aber auch mit an Bord. Postman bietet eine native OAuth Integration an. Man wählt einfach OAuth 2.0 aus und kann dann folgende Informationen aus seiner eigenen App hinterlegen:   Grant Type: Client Credentials\n  Access Token URL: https://login.microsoftonline.com/malachor.onmicrosoft.com/oauth2/v2.0/token \n  Client ID: 50641771-73ac-42fa-9b6f-f25e49ec6871\n  Client Secret: dvMR0c_*_RlxvV*JQQZGDICH6N04ZT2/\n  Scope : https://graph.microsoft.com/.default Wenn man immer alle Scopes/Berechtigungen möchte, dann ist dieser Scope der einfachste\n  Hinweis: Ich habe meine App schon wieder gelöscht. Sie ist nicht länger nutzbar, also ist das Secret im Code auch kein Geheimnis mehr.\nÜber “Request Token” kann ich dann ein Token holen und für alle weiteren Requests verwenden. Zum Prüfen des Token (hat der Scope geklappt) kann man einfach auf jwt.io oder auf den Microsoft Service jwt.ms gehen.\nHinweis: Solche Token Decoder sind eine tolle Sache, aber bitte denkt dran, wenn ihr das mit produktiven Token macht, dann müsst ihr dem Service vertrauen, denn er hat in dem Moment eure Berechtigung! In meinem Fall könnten die beiden Websites das Token nehmen und gegen meinen Tenant einsetzen! Ich nutze hier mein LAB Tenant und ich glaube, dass ich weiß was ich tue :) Also alles gut! Mit dem Token kann man dann zum Beispiel in meinem Fall die Azure AD Access Reviews einsehen. Mein Debugging wurde extrem vereinfacht, da ich so einfach meine App Permissions testen kann.\n","date":"2019-07-01","permalink":"https://marcoscheel.de/post/2019/07/185978935612-microsoft-graph-postman-und-wie-bekomme-ich-ein/","tags":["Microsoft 365","Azure AD","Microsoft Graph","Tools"],"title":"Microsoft Graph, Postman und Wie bekomme ich ein App Only Token?"},{"content":"Ich bin ein Fan von diesem Typ Blog-Post. Tolle Beispiele gibt es zum Beispiel bei The Verge. Ich möchte euch heute Einblick in meine Tasche geben und zeigen, was ein 100% Cloud Berater Lead Cloud Architect für einen erfolgreichen Tag braucht.\nWir reden oft über den modernen Arbeitsplatz, aber gerade für mich ist der moderne Arbeitsplatz nirgendwo und überall. Was habe ich dabei? Es gibt immer mal wieder Blogs in denen ihr Autor beschreibt, was er mit sich trägt und welche Motivation dahintersteckt. In vielen Meetings kann man sehen, dass ich ein Microsoft Surface Book (GEN1) nutze, aber was noch alles in meiner Tasche steckt, zeige ich euch jetzt. Für die spannenden Dinge gibt es dann auch noch ein Satz zum Warum.\nDas Bild nochmal in XL auf Twitter.\nFangen wir mit dem Offensichtlichsten an: Vaude Albert M ist meine Tasche (1). Ich versuche „leicht“ unterwegs zu sein. Eine Tasche mit Rollen mag praktisch sein, aber für meine Anforderungen und mein Budget hat es die Tasche genau getroffen. Es sollten Laptops bis 13‘‘ reinpassen und für ausgedehnte Reisen kann Sie auf einen Trolley gesteckt werden. Die Fächeraufteilung finde ich ausreichend und es gibt 4 Netze im Inneren für all mein Kleinkram.\nMein Laptop (2) oder mein 2-1 ist ein Microsoft Book (GEN1). Seit ich bei Glück \u0026amp; Kanja arbeite, bin ich in der glücklichen Lage Stift-PCs zu verwenden. Eingestiegen mit einem Toshiba Portege m200 über eine Microsoft Surface Pro (das Original) direkt zum Surface Book. Einziger Ausrutscher war ein Dell Latitude E6400. Mein Laptop transportiere ich auch in der Tasche geschützt durch ein Belkin Neopren Schutzhülle (3). Die Hülle kommt auch zum Einsatz und schützt das Gerät vor eventuellem Regen, wenn ich mal bei einem Kunden ein Meeting in einem anderen Gebäude/Stadtteil habe.\nDie Akkuleistung des Surface Book ist OK, aber ohne Netzteil (4) traut sich keiner aus dem Haus. Das Netzteil ist super. Was kann an einem Netzteil super sein? Es hat ein USB Ladeport! Ich habe immer ein kurzes Micro-USB Kabel mit USB-C Adapter dran. Handy, Headset, PowerBank und ähnliches kann so schnell und unkompliziert geladen werden.\nAls mobile Maus kommt eine Microsoft Arc Touch Mouse Surface Edition (also mit Bluetooth) zum Einsatz. Die Maus war essentiell in der Zeit mit dem Surface Pro, da es nur ein USB Anschluss gab und der Trackpad seinen Namen nicht verdient hat. Mit dem Surface Book habe ich nun ein sehr gutes Trackpad und die Maus kommt immer seltener zum Einsatz, aber ganz ohne geht es einfach noch nicht. Durch den Knick-Mechanismus nimmt sie quasi kein Platz weg in der Tasche.\nFür die zahlreichen Telcos in der Woche ist ein richtiges Headset (6) unersetzlich. Glück \u0026amp; Kanja ist im Bereich UC nicht unbekannt und so haben ich viele Kollegen, die mir immer wieder mitteilen wie wichtig ein gutes Headset ist (und ein LAN Kabel). Mit dem Plantronics Voyager Focus UC habe ich ein Oberklasse Headset. Es hat sogar „Active Noise Cancelling“ und kann so auch im Zug zum Video gucken genutzt werden. Ich nutze dieses Headset mobil und am Arbeitsplatz in Offenbach. Das Teil hat ein super einfach zu erreichenden Mute Switch und so störe ich in Konferenzen zumindest nicht durch unnötige Geräusche. Durch ein Bug auf meinem Surface Book und der aktuellen Windows Insider Version nutze ich mein „altes“ mobiles Headset (7). Das Jabra Stealth UC hat mich immer mobil begleitet und nie meine Tasche verstopft. Privat nutze ich die Jaybird Freedom Sprint Bluetooth Kopfhörer (8). In der Regel sind sie nur mit meinem Smartphone gekoppelt und ich nutze sie überwiegend für Podcasts. Durch die Taste am Ohr kann ich den Podcast so jederzeit unterbrechen und fortsetzen. Die Kopfhörer kann man auch zum Joggen anziehen.\nMeine Tasche hat zwar viele Netze und Fächer, aber der Kleinkram ist immer irgendwie hin und her geflogen und die Kabelkopfhörer haben sie immer in allem verheddert. Ich habe auf einem US Blog in einem „Whats in my bag“ Post dieses tolle Gadget gefunden. Ein „Stuff“-Organizer (9) mit dem Namen Cocoon GRID-IT. Die Teile sind mittlerweile ganz erschwinglich und in unterschiedlichen Größen zu haben. Ich haben folgende Dinge organisiert:\n Kabelkopfhörer Kabelheadset (noch von meinem Lumia 920) MicroSD mit Adapter USB 2.0 Stick (klein) + USB 3.0 Stick (groß) Surface Pen Tip Kit USB-C to Headphone-Jack Adapter Mini-Display-Port zu VGA Mini-Display-Port zu HDMI, DVI, Display-Port Micro-USB zu USB Adapter  Eine PowerBank gehört heute einfach in jede Tasche. Ich habe mir im letzten Amazon Xmas Sale nochmal einen „kleinen“ Anker den PowerCore 10000 geholt. Ich hatte sonst einen Anker PowerCore mit 20100 mAH dabei, aber der war nur unnötig schwer. Unterwegs ein Headset oder Handy laden kann man mit dem kleinen Akku viel eleganter. Für diesen Einsatz habe ich auch kurze Kabel in der kleiner Tasche (Micro-USB und USB-C).\nMeine Kabelsammlung und ein Anker 24-Watt 2-Port Netzteil trage ich immer in der vordersten Tasche dabei. Für den Einsatz am Netzteil eignen sich lange Kabel. Mein Lieblingskabel (weil lang) ist das weiße Micro-USB Kabel, dass ich noch aus den Kindle Keyboard Zeiten übrighabe. Aktuellere Geräte brauchen ein USB-C Kabel und da habe ich noch eins von meinem Lumia 950 XL übrig. Mein Frau nutzt als „Computer“ ein iPad, also kann es nie schaden auch ein Lightning Kabel (natürlich von Anker) dabei zu haben.\nEs bleibt nur der Rest (12) übrig. Handcreme, Kugelschreiber, Bonbons gegen das Kratzen im Hals, Heuschnupfen-Tabellen, Batterien für den Surface Pen, Regenschirm und als Vater von zwei Jungs immer ein paar Feuchtigkeitstücher.\n","date":"2018-01-28","permalink":"https://marcoscheel.de/post/2018/01/170223335212-whats-in-your-bag-marco/","tags":["Work","Equipment"],"title":"What’s in your bag, Marco?"},{"content":"In meinen Projekten (Office 365 / 100% Cloud) ist Conditional Access (CA) nicht mehr wegzudenken. Für uns “SharePoint“ Leute hat CA eine spezielle Geschmacksrichtung. Eine der ersten Session-Based CA Policies arbeitet mit SharePoint und ermöglicht so zusätzliche Kontrollen beim Zugriff auf SharePoint. Microsoft hat das ganze in einem Blogpost beschrieben. Jetzt wurde angekündigt, dass es in die nächste Runde geht und die Konfiguration nicht mehr auf den ganzen Tenant zieht, sondern einzelne Site Collections adressiert. In diesem Zuge wollte ich mal wieder meine AAD + SPO Konfiguration gerade ziehen, aber siehe da\u0026hellip; es fehlt was! So sieht es im SharePoint Admin Center unter dem Punkt “Device Access” aus: Eigentlich sollte der Dialog so aussehen: Es fehlt der entscheidende Teil: \u0026ldquo;Control access from devices that aren\u0026rsquo;t compliant or joined to a domain\u0026rdquo; oder auf deutsch: _\u0026ldquo;Zugriff von Geräten steuern, die nicht konform oder einer Domäne beigetreten sind\u0026rdquo; _Jetzt stellt sich die Frage warum? Mein Tenant ist auf “First Release”, also sollte es doch funktionieren. Eine kurze Recherche hat mich zum überraschenden Ergebnis geführt: Quelle: Microsoft Support - Control access from unmanaged devices\nMein Tenant war tatsächlich für einen Test auf die Option “First release for selected users” eingestellt: Lösung Wie im Support Dokument beschrieben habe ich das Setting geändert auf die Option “First release for everyone”: Kurze Zeit später (es zieht nicht sofort), konnte ich über das SharePoint Admin Center die gewünschte \u0026ldquo;Device Access” Konfiguration abschließen.\nIch habe in der Vergangenheit immer nach den Unterschieden der beiden “First Release” Einstellungen gesucht. Dieser Fall ist der erste mir bekannte.\n","date":"2017-09-17","permalink":"https://marcoscheel.de/post/2017/09/165449521992-sharepoint-online-device-access-option-fehlt/","tags":["Office365","SharePoint","Azure AD"],"title":"SharePoint Online Device Access Option fehlt - Lösung: First Release for everyone"},{"content":"TL;DR Die Roadmap schließt somit eine Lücke, die überwiegend Hardcore SharePoint User (Publishing, echte Listen und Metadaten) betroffen hat und begeistern wird. Für den Rest wird es einfach einfacher und schöner.\nAlle die Office 365 noch nicht verwenden können die Zeit bis Ende 2016 nutzen, um eine solide Basis aufzubauen. Alle mit einer guten Office 365 Basis können die Zeit nutzen in ersten Gehversuchen mit Office 365 Groups eigene Erfahrungen zu sammeln. In nächsten Jahr (2017) können dann hoffentlich alle das volle Potential von SharePoint Online nutzen und über den Datei-Tellerrand schauen und den nächsten Level der modernen Teamräume ausnutzen.\nAm 4.5.2016 hat Microsoft Details zur weiteren Roadmap von SharePoint in einem Online-Event vorgestellt. In den letzten Monaten hat man den Eindruck bekommen, dass SharePoint sich immer mehr auf das Verwalten von Dateien konzentriert und dies auch mit einem sehr rudimentären Funktionsumfang. Woher kam die Vermutung? Meine persönliche Wahrnehmung als Berater im SharePoint Umfeld, Blogs aus der (MVP-)Community, die aktuelle (Stand Mai 2016) Implementierung der Office 365 Groups und nicht zuletzt der starte Fokus auf den OneDrive for Business Bereich (Next Gen Sync, \u0026hellip;).\nIch als SharePoint Berater bei der Glück \u0026amp; Kanja Consulting AG möchte ihnen gerne meine Wahrnehmung und Interpretation der Dinge schildern. Es ist keine 1:1 Zusammenfassung, denn die hat Microsoft selber hier geliefert:\n  https://blogs.office.com/2016/05/04/the-future-of-sharepoint/\n  https://blogs.office.com/2016/05/04/announcing-simple-and-powerful-file-sharing-and-collaboration-for-office-365/\n  https://blogs.office.com/2016/05/04/sharepoint-the-mobile-and-intelligent-intranet/\n  https://blogs.office.com/2016/05/04/the-sharepoint-framework-an-open-and-connected-platform/\n  https://blogs.office.com/2016/05/04/sharepoint-server-2016-your-foundation-for-the-future/\n  Sites heißt jetzt wieder SharePoint – Das Versteckspiel hat eine Ende\nMeine erste Reaktion auf Twitter: Persönlich bin ich mit dem Event sehr zufrieden. SharePoint versucht sich nicht nur auf Dateien zu beschränken, sondern möchte auch in Office 365 als echtes Intranet wahrgenommen werden. Abseits von Groups und OneDrive gibt es ja noch Listen. Ein Feature das nur \u0026ldquo;Wenige\u0026rdquo; verwenden, aber eigentlich liegt dort der Teil der SharePoint definiert. Was hat Microsoft nun getan, um die traditionellen Wert wiederzubeleben?\n  Moderne Teamsites sehen gut aus und funktionieren im Browser und in einer neuen SharePoint Mobile App\n  Moderne Teamsites bieten mehr als eine Dokumentenbibliothek\n  Moderne Teamsites fokussieren auf \u0026ldquo;Pages\u0026rdquo; und \u0026ldquo;Webparts\u0026rdquo; um Informationen aus der puren Existenz zum Benutzer zu führen\n  Moderne Teamsites bieten **Discovery **ala Delve und zeigen Activities auf einer Zeitlinie ähnlich wie Yammer  Ist der Focus auf das Intranet (News, Publishing, \u0026hellip;) und echte Listenfunktionalität die Lösung aller SharePoint (Online) Probleme?\nNein! Ich bin noch immer der Meinung, dass der Großteil meiner Kunden primär eine grundsolide Lösung für das Arbeiten mit Dateien benötigt. Die Roadmap hierfür (OneDrive for Business Client) scheint klar. Wird die Zeitlinie gehalten, dann sehe ich hier keine Probleme.\nFür was brauche ich jetzt aber Listen? Auch in internen Gesprächen wird den meisten Leuten nicht klar, warum man Listen benötigt. Selbst zusätzliche Metadaten an Dateien sind immer wieder ein Streitthema mit einem klaren Gewinner: Die Out-Of-The-Box Metadaten (Ersteller + Datum, Änderungshistorie, Version, Name) sind ausreichend. Benutzer wundern sich schon oft genug, warum SharePoint bei Dateien neben dem Dateinamen auch ein Titel Attribut hat. Zusätzliche Daten zu erfassen, ist für viele Benutzer neu und ungewohnt. Es mag valide Situationen (aber wirklich nur ein kleiner Prozentsatz) geben, wo extra Metadaten hilfreich sind. Das Handling der Metadaten stellt für Dateien aber noch immer ein Problem dar. Offline, Non-Office Dateien und verschiedene andere Situationen erlauben kein Zugriff auf diese extra Metadaten. Ein Bulkupload über OneDrive for Business oder den Browser erzeugt Dateien ohne Metadaten und oft ohne einen relevanten Handlungshinweis für den End-Benutzer. Kann ich auf Ordner verzichten, wenn ich Metadaten pflegen? Meine Meinung ist hier in 95% der Fälle ein \u0026ldquo;Nein\u0026rdquo;, da ich ohne Ordner zum Beispiel Offline ein Problem bekomme, wenn ich zum Beispiel 2000 Dateien in einer flachen Dateiliste bekomme. In solchen Situationen kann auch kein \u0026ldquo;selective Sync\u0026rdquo; helfen, wenn er mal implementiert wurde.\nDie Demos von Microsoft zeigen die Modern Teamsites unter Verwendung der \u0026ldquo;neuen\u0026rdquo; Seiten und Webparts zum Aufbereiten multipler Informationen und Quellen. Sieht toll aus. Ich fand schon immer die Startseite einer Teamsite dafür geeignet, neben einem Dokumentenbaum (wie aus dem Dateisystem) auch andere relevanten Informationen aufzubereiten. Die Realität zeigt aber, dass sich so was gerade für Demos perfekt eignet, aber im echten Leben nur selten angewendet wird.\nAlso doch alles Grütze? Nein, denn es gibt alleine durch die neue \u0026ldquo;Out-Of-The-Box\u0026rdquo; Funktionen so viele Vorteile, dass auch das normale Arbeiten unterstützt wird. Die neue SharePoint App funktioniert auch ohne zusätzliche Listen und Metadaten. Ein Delve ähnliches Discovery findet jetzt \u0026ldquo;überall\u0026rdquo; statt. Die \u0026ldquo;neue\u0026rdquo; Oberfläche kennen die User schon aus ihrem OneDrive for Business. Ergänzungen wie PowerApps und Flow können bei Bedarf aktiviert und genutzt werden. Das neue SharePoint Framework erweitert und ergänzt die bestehenden Add-In basierenden Entwicklungen.\nEs gibt noch einige nicht unwesentliche Unbekannte:\n  Wie integriert sich die Groups-Funktionalität in die neuen Teamsites und anderes herum\n  Modern Teamsites für On-Premise (SP2016) gibt es wahrscheinlich erst 2017 in einem Feature Pack\n  Wie überbrücken wir die Zeit bis Ende 2016 (da kommen das Gesamtpaket zusammen)\n  Die Roadmap schließt somit eine Lücke, die überwiegend Hardcore SharePoint User (Publishing, echte Listen und Metadaten) betroffen hat und begeistern wird. Für den Rest wird es einfach einfacher und schöner.\nPlanlos? Kein Grund nix zu tun. Nutzen Sie noch kein Office 365? Starten Sie einen Piloten und untersuchen Sie, was ihr Unternehmen der Cloud abgewinnen kann. Bevor es um SharePoint geht benötigen Sie solide Grundlagen/Planungen im Bereich Authentifizierung (Azure AD), eine eventuelle Verzeichnissynchronisation, Office Pro Plus (Client) Rolloutplanung, Mailmigration und mehr. Ist alles ready, dann ist es fast schon 2017 und mit all den wahrgewordenen SharePoint Funktionen kann auch hier solide ein Plan entwickelt werden.\nIhr Unternehmen hat schon die stabile Basis in Office 365 oder befindet sich in einem Proof-of-Concept? Diese Ausgangssituation ist ganz so einfach. Persönlich würde ich hier versuchen auf Basis der aktuellen Office 365 Groups Implementierung erste Gehversuche zu machen und so bis Ende 2016 die Grundlagen für eine erfolgreiche Modern SharePoint Implementierung vorzubereiten.\n Wir von der Glück \u0026amp; Kanja haben vor kurzem die GK WebcastFriday Serie gestartet. Im letzten Webcast haben meine Kollege Jan Geisbauer und ich das Thema SharePoint und im speziellen The Futute of SharePoint betrachtet. Schaut doch mal rein.\n","date":"2016-05-21","permalink":"https://marcoscheel.de/post/2016/05/144716486707-the-future-of-sharepoint-online/","tags":["Office365","SharePoint"],"title":"The Future of SharePoint (Online)"},{"content":"Marco Scheel nach seinem Umzug von Wordpress zu Tumblr\nDas alte Zeug gibt es hier: https://marcoscheel.wordpress.com/\n","date":"2016-05-02","permalink":"https://marcoscheel.de/post/2016/05/143746644752-zeit-f%C3%BCr-einen-neustart/","tags":["Meta"],"title":"Zeit für einen Neustart"},{"content":"Ich hatte heute auf unserem Bootcamp eine Anfrage von einem unserer Consultants bezüglich meines Projektes zum Wechseln von Benutzern auf dem IIS bekommen. Ich hatte das vor kurzem in einem Post gebloggt. Es ging um eine Konfiguration im Bereich Publishing TMG und genauer um Forms Authentication vs. Integrated Authentication. Wir haben kurz die Anwendung im Lab OWA erfolgreich getestet. Je nachdem wie der Kunde sich entscheidet gehen wir damit auch Live.\nBei der Konfiguration sind uns allerdings ein paar Dinge aufgefallen:\n * Die Konfiguration im OWA Ordner war nicht erfolgreich. Alle Aufrufe für den Handler wurden ignoriert und es wurde OWA angezeigt. * Die Lösung war einfach auf dem Web einen neuen Ordner im Root anzulegen und dort eine eigen Application für den Handler zu konfigurieren. Bei einem ersten Test ist allerdings aufgefallen, dass die Authentifizierung erst mal nicht wollte. Problem könnte hier Kerberos sein, da der normale App Pool ein anderen Account verwendet. WIr haben kurzerhand einfach den OWA App Pool verwendet und schon funktionierte die Lösung. * Nach der erfolgreichen Ummeldung landet man aber per Default im Root des aktuellen Folders, was für die OWA User jetzt keine Option ist, da hier nichts liegt :-) Mein Kollege hat das ganze per HTML Meta Refresh gelöst und ich hab das dann gleich integriert, da ich so ohne Code Änderung auskommen.  Bis heute habe ich tatsächlich schon 26 Downloads und immerhin 286 page visits :-)\nCiao Marco\n","date":"2012-06-13","permalink":"https://marcoscheel.de/2012/06/13/minor-sign-in-as-a-different-user-fur-owa-und-html-redirection/","tags":["xArchived"],"title":"Minor update \"Sign In As A Different User\" für OWA und HTML Redirection"},{"content":"Wir sind aktuell auf Bootcamp und ich habe Claims und alles drum herum zum Thema. Natürlich ist auch die Migration aus \u0026ldquo;alten\u0026rdquo; Systemen ein Punkt auf der Agenda. Ich brauche also ein altes System\u0026hellip; Also habe ich schnell eine ASP.NET 4.5 WebForms Site angelegt und konnte auch schnell die Logins aus dem SQL Membership Provider zum Fliegen bekommen. Allerdings konnte ich keine Seiten vor Zugriff über eine spezielle Role schützen. Hier meine Web.Config:\n\u0026lt;system.web\u0026gt;\n  \u0026lt;/system.web\u0026gt;\n Bei einem Test hattet die Konfiguration aber kein Erfolg. Mein Admin User durfte die Admin.aspx einfach nicht sehen. Eine kurze Recherche hat gezeigt, dass ich den Role Provider der zwar vorkonfiguriert war noch enablen mußte.\nVorher:\n   Nachher:\n\u0026lt;roleManager defaultProvider=\u0026ldquo;DefaultRoleProvider\u0026rdquo; enabled=\u0026ldquo;true\u0026rdquo;\u0026gt;\n   Anschließen konnte ich mit dem Admin User die entsprechende Seite aufrufen :-)\nCiao Marco\n","date":"2012-06-11","permalink":"https://marcoscheel.de/2012/06/11/asp-net-4-5-sqlmembership-und-keine-roles-2/","tags":["xArchived"],"title":"ASP.NET 4.5, SQLMembership und (keine) Roles"},{"content":"Ich hatte vor einiger Zeit bei einem Kunden das Problem, dass die Diagnostic Logs von Microsoft SharePoint 2010 nicht automatisch komprimieren ließen. SharePoint kümmert sich ja selber um die Verwaltung des Speicherplatzes. Eines der neuen Features ist, dass auf die Files in dem LOGS Ordner automatisch die NTFS Kompression aktiviert wird.\nDer Kunde betreibt mehrere Farmen (Production, Staging, Testing und Dev), um den ganzen Lifecyle des Deployments korrekt abzudecken. Bis auf die Produktion laufen alle Farmen als Guest in geshared virtuellen Umgebungen. Die Produktion hat eigene Virtualisierungshosts und wurde extra für SharePoint neu aufgebaut. Ich konfiguriere (abweichend vom Standard) das Verzeichnis der LOGS nicht in den 14er Hive sondern auf \u0026ldquo;D:SP2010-DiagLogs\u0026rdquo;. Auf einigen der Maschinen im Staging ist mir dann aufgefallen, dass der Ordner für das SharePoint Diagnostics Logging keine NTFS Kompression verwendet. Im ersten Moment dachte ich, dass hier hat einer der Kollegen was \u0026ldquo;konfiguriert\u0026rdquo; und ich wollte es manuell gerade ziehen. Ich stellte allerdings fest, dass die Option für die Kompression des Dateisystems nicht existiert.\nEine kurze Recherche brachte zum Thema SharePoint Diagnostic Logs wenig, um nicht zu sagen gar nichts. Die Suche wurde nun erweitert auf OS-Ebene, da die Funktion der \u0026ldquo;File Compression\u0026rdquo; nicht durch SharePoint bereitgestellt wird. Es stellte sich dann langsam heraus, dass die Funktion nur für Platten bereitsteht, die mit einer Cluster-Größe von 4kb formatiert wurden. Hier ein Auszug von Microsoft Support:\nThe minimum default cluster size for NTFS under Windows NT 4.0 and later is 4 kilobytes (KB) because NTFS file compression is not possible on drives with a larger cluster size. [http://support.microsoft.com/kb/140365/en-us](http://support.microsoft.com/kb/140365/en-us) Und es war tatsächlich so, dass in einigen der D: Drives der Guest im Development eine \u0026ldquo;falsche\u0026rdquo; Cluster-Size hatten.\nCiao Marco\n","date":"2012-05-29","permalink":"https://marcoscheel.de/2012/05/29/sharepoint-2010-diagnostics-log-files-werden-nicht-komprimiert/","tags":["xArchived"],"title":"SharePoint 2010 Diagnostics Log Files werden nicht komprimiert"},{"content":"Meine Kollege Karten Kleinschmidt und ich haben beim Kunden ein interessante kleine Aufgabe bekommen, in der es darum ging, den verwendeten Benutzer (Domainen Benutzer) an einer bestehenden Webseite zu wechseln.\nIn einem Unternehmensnetzwerk mit integrierter Anmeldung an der Domain muss der User im Normalfall keine weiteren Anmeldeinformationen eingeben, um auf Websites zuzugreifen. Anwendungen wie SharePoint, CRM und viele Eigenentwicklungen nutzen diese Funktion natürlich für Authentifizierung und Autorisierung aus. Es kommt aber immer mal wieder vor, dass der aktuelle Benutzer nicht der \u0026ldquo;richtige\u0026rdquo; Benutzer für die aktuelle Aufgabe ist. In vielen Unternehmen haben Benutzer neben dem eigenen Account einen administrativen Benutzer (gkms und gkadm_ms). Ist man am PC mit dem normalen Account angemeldet und möchte nun den administrativen Account verwenden, dann muss man den ganzen Brower mit anderen Credentials starten, sich am PC ummelden oder per Terminal Services auf einem anderen Rechner anmelden. Anwendungen wie SharePoint bieten solche Funktionen Out-Of-The-Box bereits an. Für Anwendungen, welche das nicht können, ist es für den normalen Mitarbeiter im Unternehmen kein leichtes Unterfangen, diese Aufgabe zu lösen. \nKarsten (und damit irgendwann auch ich) hat nun die Aufgabe, für eine Website im Lync Umfeld so eine Funktion anzubieten. Eine kurze Recherche (Bing | Google) hat gezeigt, dass Roel van Lisdonk in seinem Blogpost ASP .NET – C# – How to “Sign in as Different User” like in Microsoft SharePoint with Windows Authentication genau das beschreibt. Seine Lösung ist allerdings Page-basierend und ich wollte was allgemeineres. Ich habe dann einfach seine Code als Basis genommen, ein HTTPModule erstellt und das Ergebnis dann auf Codeplex veröffentlicht. Da nicht so viel Eigenleistung drin steckt, war es schnell klar, dass es nur \u0026ldquo;fair\u0026rdquo; ist es weiter zu teilen. Hier ist also das erste Glück \u0026amp; Kanja Codeplex Projekt.\nhttp://signinas.codeplex.com - Glück \u0026amp; Kanja Consulting AG - Sign In As A Different User\n\nDas Projekt steckt sicher noch in den Kinderschuhen und kann deutlich erweitert werden, aber dafür gibt es ja Codeplex :-)\nIch hatte übrigens viel \u0026ldquo;Spaß\u0026rdquo; mit Codeplex, aber das ist ne andere Geschichte und die Schuld von TFS ;-)\n","date":"2012-05-27","permalink":"https://marcoscheel.de/2012/05/27/sign-in-as-a-different-user-benutzer-wechseln-fur-iis-basierte-sites/","tags":["xArchived"],"title":"Sign In As A Different User - Benutzer wechseln für IIS basierte Sites"},{"content":" Coole Sache  Posted from WordPress for Windows Phone","date":"2010-12-19","permalink":"https://marcoscheel.de/2010/12/20/windows-phone-7-test/","tags":["xArchived"],"title":"Windows Phone 7 test"},{"content":"In einer der SharePoint 2007 Farmen, die ich betreue, nutzen wir die Blobcache Funktion, um Video’s, Flash und Bilder zu cachen. Wir haben zwei Web Front End Server (WFE) und die Funktion leistet eigentlich gute Dienste. Es kommt allerdings immer mal wieder vor, dass Bilder (das Verhältnis ist viel höher zum restlichen Content im Cache) nicht mehr angezeigt werden und es ein ASP.NET Error gibt. Genau es gibt folgendes Fehler:\nServer Error in '/' Application. -------------------------------------------------------------------------------- Cannot complete this action. Please try again. Description: An unhandled exception occurred during the execution of the current web request. Please review the stack trace for more information about the error and where it originated in the code. Exception Details: System.Runtime.InteropServices.COMException: Cannot complete this action. Please try again. Source Error: An unhandled exception was generated during the execution of the current web request. Information regarding the origin and location of the exception can be identified using the exception stack trace below. Stack Trace: [COMException (0x80004005): Cannot complete this action. Please try again.] Microsoft.SharePoint.Library.SPRequestInternalClass.GetAclForScope(String bstrWebUrl, Guid guidScopeId, Object\u0026amp; pvarAcl, UInt64\u0026amp; lAnonymousMask) +0 Microsoft.SharePoint.Library.SPRequest.GetAclForScope(String bstrWebUrl, Guid guidScopeId, Object\u0026amp; pvarAcl, UInt64\u0026amp; lAnonymousMask) +238 [SPException: Cannot complete this action. Please try again.] Microsoft.SharePoint.Library.SPRequest.GetAclForScope(String bstrWebUrl, Guid guidScopeId, Object\u0026amp; pvarAcl, UInt64\u0026amp; lAnonymousMask) +349 Microsoft.SharePoint.SPReusableAcl..ctor(SPRequest request, String webUrl, Guid scopeId) +149 Microsoft.SharePoint.SPSite.GetReusableAclForScope(Guid scopeId) +133 Microsoft.SharePoint.Publishing.\u0026lt;\u0026gt;c__DisplayClass3.\u0026lt;EnsureAuthenticatedRights\u0026gt;b__0() +1296 Microsoft.SharePoint.SPSecurity.CodeToRunElevatedWrapper(Object state) +73 Microsoft.SharePoint.\u0026lt;\u0026gt;c__DisplayClass4.\u0026lt;RunWithElevatedPrivileges\u0026gt;b__2() +592 Microsoft.SharePoint.Utilities.SecurityContext.RunAsProcess(CodeToRunElevated secureCode) +319 Microsoft.SharePoint.SPSecurity.RunWithElevatedPrivileges(WaitCallback secureCode, Object param) +571 Microsoft.SharePoint.SPSecurity.RunWithElevatedPrivileges(CodeToRunElevated secureCode) +135 Microsoft.SharePoint.Publishing.BlobCache.EnsureAuthenticatedRights(Guid siteID, Guid scopeID) +1020 Microsoft.SharePoint.Publishing.BlobCache.RewriteUrl(Object sender, EventArgs e) +2949 System.Web.SyncEventExecutionStep.System.Web.HttpApplication.IExecutionStep.Execute() +80 System.Web.HttpApplication.ExecuteStep(IExecutionStep step, Boolean\u0026amp; completedSynchronously) +171 -------------------------------------------------------------------------------- Version Information: Microsoft .NET Framework Version:2.0.50727.3603; ASP.NET Version:2.0.50727.3082  \nDie Lösung in diesem Fall ist das Resetten des Caches für diese SiteColletion. Über die SiteSettings kommt man zu den “Object Cache Settings” und hier kann man die drei Checkboxen zum Zurücksetzen des Caches setzen:\n\nIch hoffe es hilft dem einen oder anderen, der noch in Sharepoint 2007 feststeckt ;-)\nCiao Marco\n","date":"2010-10-03","permalink":"https://marcoscheel.de/2010/10/03/sharepoint-blob-cache-corrupt/","tags":["xArchived"],"title":"SharePoint Blob Cache Corrupt"},{"content":"Eigentlich nicht mein Dinge auf irgendwelche Blogposts hinzuweisen, aber denke hier ist mal eine Ausnahme angesagt. Es geht um eine Lücke in ASP.NET, die seit einigen Tagen die Runde macht. Hier das Security Bulitin von MS: http://www.microsoft.com/technet/security/bulletin/ms10-sep.mspx\nScottGu beschreibt sehr schön, was zu tun ist und welche Auswirkungen es hat, wenn man das ASP.NET Security Update anwendet. http://weblogs.asp.net/scottgu/archive/2010/09/28/asp-net-security-update-now-available.aspx\nBetroffen ist auch SharePoint (2007/2010) und somit sollten alle Admins loslegen.\n\nUnsere Sharepoint 2010 Installation hat leider eine Reboot benötigt. Auch wenn ScottGu sagt, dass es eigentlich keinen braucht… Schade.\nCiao Marco\n","date":"2010-09-29","permalink":"https://marcoscheel.de/2010/09/29/call-2-action-asp-net-security-update-now-available/","tags":["xArchived"],"title":"Call 2 Action: ASP.NET Security Update Now Available"},{"content":"Jetzt fehlt nur noch das passende Telefon. Windows Phone 7 ich komme.\n\nCiao Marco\n","date":"2010-09-24","permalink":"https://marcoscheel.de/2010/09/24/zune-software-4-2-ist-da/","tags":["xArchived"],"title":"Zune Software 4.2 ist da"},{"content":"Heute bin ich wieder ein Schritt weiter. Die fehlenden Bilder aus meinem GraffitiCMS wurden dank S3 und dem Search \u0026amp; Replace Plugin gerade gezogen.\nOffen sind jetzt noch ein paar URL’s, die ins Leere laufen, aber mal sehen, ob mir da noch was einfällt :-)\nCiao Marco\n","date":"2010-09-21","permalink":"https://marcoscheel.de/2010/09/21/umzug-phase-zwei/","tags":["xArchived"],"title":"Umzug - Phase Zwei"},{"content":"Lange ist es her und dann so was langweiliges. Der Blog läuft jetzt auf Wordpress. Mein vServer wird in den nächsten Tagen geplättet. Ich hoffe auf einen reibungslosen Umzug. Dank der eingesetzten Tools sollte keiner was merken. Das Theme update ich irgendwann nochmal, allerdings halte ich nicht viel davon Arbeit in die Oberfläche zu stecken. Der wahre Blick auf ein Blog geht sowieso durch Google Reader. Danke Feedburner muss NIEMAND sein RSS Reader anfassen.\nDrückt mir die Daumen, dann gibt es hier bald auch wieder echten Content.\nCiao Marco\n","date":"2010-09-19","permalink":"https://marcoscheel.de/2010/09/19/website-ist-umgezogen/","tags":["xArchived"],"title":"Website ist umgezogen"},{"content":"Die letzten SharePoint 2007 Installationen werden immer häufiger auf Windows Server 2008 R2 (W2K8R2) ausgeführt. Der Unterschied (für SharePoint relevant) zum “alten” Windows Server 2008 (W2K8) liegt zum Beispiel in einigen IIS7 Änderungen. Eine dieser Änderungen hat mich in einer der letzten Installationen “eingeholt”. Wie ich SharePoint installiere, habe ich bereits dokumentiert. Bei der Auswahl der Internet Information Server 7 (IIS7) Konfiguration sind nun die ersten Out-Of-Band Komponenten direkt anwählbar. Hier habe ich, ohne mir weiter Gedanken zu machen, auch WebDav ausgewählt, da nun die downloadbare Variante von W2K8 direkt integriert ist.\nNach einer Woche hatte ich dann die Kundenanfrage auf dem Tisch: WebDav geht nicht unter Vista und höher. Beim Klick auf “Open in Windows Explorer” dauert es einen kleinen Moment und es passiert… garnichts. Unter XP bekommt man in der Regel zwei Authentifizierungsanfragen bevor der Share sich dann doch öffnet. Der Server an sich ist aktuell nur unter HTTP und nicht unter HTTPS unterwegs. Wichtig bei XP Clients, da es hier noch die FrontPage Extensions genutzt werden können, weshalb auch der Zugriff unter XP klappte.\nErstes Troubleshooting habe ich mit folgenden Guide von MS gemacht: Whitepaper - Understanding and Troubleshooting the SharePoint Explorer View\nLeider ohne Erfolg. Im HTTP Log des IIS fangen wir dann folgende Meldung: “405 Method not allowed”. Kurz mit Bing gegoogled und kein Treffer gefunden (googlen mit Google brachte ebenfalls kein Ergebnis). Irgendwann hatte ich zusammen mit dem Kunden die Idee, dass wir mal die IIS Features checken und einfach mal die WebDav Funktion wieder abwählten. Es brachte den nötigen Erfolg. Bleibt also anzumerken:\nDie Option WebDav unter Windows Server 2008 (auch R2) kann SharePoint ins Stolpern bringen, da SharePoint eine eigene Implementierung hat, was oft übersehen wird.\nCiao Marco\n","date":"2010-01-17","permalink":"https://marcoscheel.de/2010/01/17/windows-server-2008-r2-sharepoint-2007-und-webdav-probleme/","tags":["xArchived"],"title":"Windows Server 2008 R2, Sharepoint 2007 und WebDav Probleme"},{"content":"Twitter ist schon cool… Ich dachte ich checke mal, ob sich in der Beta Welt von Office was tut und siehe da… alles voll. In der MSDN war zuerst nur über den RSS Feed was zu sehen… und seit 5 Minuten kann ich auch die gewohnte Struktur der MSDN nutzen.\nViel Spaß beim Downloaden :-) Der Server ist schon da (rund 1,5 MB/s) und der Rest ist gescheduled.\nWenn ich mir am Wochenede nicht irgendwie ne Nervenentzündung unterm Schulterblatt zugezogen hätte… würde ich heute vielleicht noch etwas länger durch halten… aber so muss ich gleich Schluss machen :-S\nViele Spaß.\nCiao Marco\n","date":"2009-11-16","permalink":"https://marcoscheel.de/2009/11/16/office-und-sharepoint-2010-beta-fr-msdn-subscriber-verfgbar/","tags":["xArchived"],"title":"Office und SharePoint 2010 Beta für MSDN Subscriber verfügbar"},{"content":"Wieder ein Kollege mehr, der sich dem \u0026ldquo;harten” Wettbewerb des Blogging stellt. Seit ein paar Tagen gibt’s Content unter http://www.itpros.de\n\nNach Jan (Exchange, SharePoint, IT Stuff) und Karsten (OCS, Exchange, IT Stuff) ist nun auch Jochen dabei sein Wissen für andere bereit zu stellen. Vielleicht schaffen wir es auch noch Oliver zu motivieren und sonst passt der Plural im Domainnamen nicht ganz :-)\nCiao Marco\n","date":"2009-08-13","permalink":"https://marcoscheel.de/2009/08/13/itpros-de-schon-abonniert/","tags":["xArchived"],"title":"ITPros.de schon abonniert?"},{"content":"Ich bin seit einiger Zeit ein echter Last.fm Fan. Über den Service habe ich schon ein Paar echt interessante Künstler gefunden. Larissa Ness ist eine von diesen Entdeckungen. Am Anfang gab es das Album nur über iTunes zu kaufen oder bei Amazon als CD für 26 Euro!!! Heute habe ich mal wieder bei Amazon MP3 vorbei gesehen und siehe da… Larissa Ness für 8,99 Euro. Die Entscheidung war einfach und schon habe ich den Amazon MP3 Downloader angeworfen:\n\nIch bin echt Happy, dass ich über Last.fm darüber gestolpert bin und Amazon mir den Zugang so solcher Musik ermöglicht :-) Natürlich DRM Free. Gerade Synct mein Windows Mobile Phone Device damit ich die Muke überall dabei habe.\nCiao Marco\nP.S. Durch den Last.fm RSS Feed könnt ihr auch meinem Blog sehen, dass ich ab jetzt nix anderes mehr höre ;-)\n","date":"2009-08-04","permalink":"https://marcoscheel.de/2009/08/04/last-fm-larissa-ness-und-amazon-mp3/","tags":["xArchived"],"title":"Last.fm, Larissa Ness und Amazon MP3"},{"content":"In meiner letzten SharePoint Installation wurde ich mit einem Active Directory mit Windows NT 4.0 Wurzeln begrüßt. Die Domäne wurde von “kunde.de” auf “neu.kunde.de” upgegraded. Der NETBIOS Name der Domain ist somit “kunde.de” und der Full Qualified Domain Name (FQDN) ist “neu.kunde.de”. In der Vergangenheit hatte ich keine gute Erfahrung mit FQDN’s im Bezug auf den SQL Server gemacht. Mein Versuch den Server über den vollen Namen anzusprechend, ist gescheitert (Pre SP1 Erfahrung).\nIm aktuellen Fall haben wir versucht, den FQDN für die Angabe der Domain Accounts in der Form “neu.kunde.deaccountname” zu verwenden. Bis zur Provisionierung des Shared Service Provider hat das auch super funktioniert. Das Erstellen des SSP endete mit folgender Seite: \nIn der Liste der SSP’s gab es mehr Details: \nIm Eventlog: A runtime exception was detected. Details follow. Message: Windows NT user or group \u0026lsquo;ms.localsvcMossProd\u0026rsquo; not found. Check the name again.\nA runtime exception was detected. Details follow. Message: Windows NT user or group 'ms.localsvcMossProd' not found. Check the name again. Techinal Details: System.Data.SqlClient.SqlException: Windows NT user or group 'ms.localsvcMossProd' not found. Check the name again. at System.Data.SqlClient.SqlConnection.OnError(SqlException exception, Boolean breakConnection) at System.Data.SqlClient.SqlInternalConnection.OnError(SqlException exception, Boolean breakConnection) at System.Data.SqlClient.TdsParser.ThrowExceptionAndWarning(TdsParserStateObject stateObj) at System.Data.SqlClient.TdsParser.Run(RunBehavior runBehavior, SqlCommand cmdHandler, SqlDataReader dataStream, BulkCopySimpleResultSet bulkCopyHandler, TdsParserStateObject stateObj) at System.Data.SqlClient.SqlCommand.FinishExecuteReader(SqlDataReader ds, RunBehavior runBehavior, String resetOptionsString) at System.Data.SqlClient.SqlCommand.RunExecuteReaderTds(CommandBehavior cmdBehavior, RunBehavior runBehavior, Boolean returnStream, Boolean async) at System.Data.SqlClient.SqlCommand.RunExecuteReader(CommandBehavior cmdBehavior, RunBehavior runBehavior, Boolean returnStream, String method, DbAsyncResult result) at System.Data.SqlClient.SqlCommand.InternalExecuteNonQuery(DbAsyncResult result, String methodName, Boolean sendToPipe) at System.Data.SqlClient.SqlCommand.ExecuteNonQuery() at Microsoft.Office.Server.Data.SqlSession.ExecuteNonQuery(SqlCommand command) at Microsoft.Office.Server.Data.SqlServerManager.GrantLogin(String user) at Microsoft.Office.Server.Administration.SharedResourceProvider.SynchronizeConfigurationDatabaseAccess(SharedComponentSecurity security) at Microsoft.Office.Server.Administration.SharedResourceProvider.SynchronizeAccessControl(SharedComponentSecurity sharedApplicationSecurity) at Microsoft.Office.Server.Administration.SharedResourceProvider.Microsoft.Office.Server.Administration.ISharedComponent.Install() at Microsoft.Office.Server.Administration.SharedResourceProvider.Provision()  Die SharePoint Logs:\n06/24/2009 17:32:42.34 OWSTIMER.EXE (0x0788) 0x0DC4 Office Server Office Server General 6pqn High Granting user 'ms.localsvcMossProd' login access to server 'moss-prod-db'. 06/24/2009 17:32:43.04 OWSTIMER.EXE (0x0788) 0x0DC4 484 880i High System.Data.SqlClient.SqlException: Windows NT user or group 'ms.localsvcMossProd' not found. Check the name again. at System.Data.SqlClient.SqlConnection.OnError(SqlException exception, Boolean breakConnection) at System.Data.SqlClient.SqlInternalConnection.OnError(SqlException exception, Boolean breakConnection) at System.Data.SqlClient.TdsParser.ThrowExceptionAndWarning(TdsParserStateObject stateObj) at System.Data.SqlClient.TdsParser.Run(RunBehavior runBehavior, SqlCommand cmdHandler, SqlDataReader dataStream, BulkCopySimpleResultSet bulkCopyHandler, TdsParserStateObject stateObj) at System.Data.SqlClient.SqlCommand.FinishExecuteReader(SqlDataReader ds, RunBehavior runBehavior, String resetOptionsString) at System.Data.SqlClient.SqlCommand.RunExecuteReaderTds(C... 06/24/2009 17:32:43.04* OWSTIMER.EXE (0x0788) 0x0DC4 484 880i High ...ommandBehavior cmdBehavior, RunBehavior runBehavior, Boolean returnStream, Boolean async) at System.Data.SqlClient.SqlCommand.RunExecuteReader(CommandBehavior cmdBehavior, RunBehavior runBehavior, Boolean returnStream, String method, DbAsyncResult result) at System.Data.SqlClient.SqlCommand.InternalExecuteNonQuery(DbAsyncResult result, String methodName, Boolean sendToPipe) at System.Data.SqlClient.SqlCommand.ExecuteNonQuery() at Microsoft.Office.Server.Data.SqlSession.ExecuteNonQuery(SqlCommand command) 06/24/2009 17:32:43.04 OWSTIMER.EXE (0x0788) 0x0DC4 484 880j High SqlError: 'Windows NT user or group 'ms.localsvcMossProd' not found. Check the name again.' Source: '.Net SqlClient Data Provider' Number: 15401 State: 1 Class: 11 Procedure: 'sp_grantlogin' LineNumber: 49 Server: 'W2K8-PROD-SQL' 06/24/2009 17:32:43.04 OWSTIMER.EXE (0x0788) 0x0DC4 484 880k High at Microsoft.Office.Server.Data.SqlServerManager.GrantLogin(String user) at Microsoft.Office.Server.Administration.SharedResourceProvider.SynchronizeConfigurationDatabaseAccess(SharedComponentSecurity security) at Microsoft.Office.Server.Administration.SharedResourceProvider.SynchronizeAccessControl(SharedComponentSecurity sharedApplicationSecurity) at Microsoft.Office.Server.Administration.SharedResourceProvider.Microsoft.Office.Server.Administration.ISharedComponent.Install() at Microsoft.Office.Server.Administration.SharedResourceProvider.Provision() at Microsoft.Office.Server.Administration.SharedResourceProviderJob.Execute(Guid targetInstanceId) at Microsoft.SharePoint.Administration.SPTimerJobInvoke.Invoke(TimerJobExecuteData\u0026amp; data, Int32\u0026amp; result) 06/24/2009 17:32:43.05 OWSTIMER.EXE (0x0788) 0x0DC4 484 880l High ConnectionString: 'Data Source=moss-prod-db;Initial Catalog=master;Integrated Security=True;Enlist=False;Pooling=False' ConnectionState: Open ConnectionTimeout: 15 06/24/2009 17:32:43.06 OWSTIMER.EXE (0x0788) 0x0DC4 484 880m High SqlCommand: 'sp_grantlogin' CommandType: StoredProcedure CommandTimeout: 0 Parameter: '@loginame' Type: NVarChar Size: 128 Direction: Input Value: 'ms.localsvcMossProd' 06/24/2009 17:32:43.12 OWSTIMER.EXE (0x0788) 0x0DC4 Office Server Office Server General 900n Critical A runtime exception was detected. Details follow. Message: Windows NT user or group 'ms.localsvcMossProd' not found. Check the name again. Techinal Details: System.Data.SqlClient.SqlException: Windows NT user or group 'ms.localsvcMossProd' not found. Check the name again. at System.Data.SqlClient.SqlConnection.OnError(SqlException exception, Boolean breakConnection) at System.Data.SqlClient.SqlInternalConnection.OnError(SqlException exception, Boolean breakConnection) at System.Data.SqlClient.TdsParser.ThrowExceptionAndWarning(TdsParserStateObject stateObj) at System.Data.SqlClient.TdsParser.Run(RunBehavior runBehavior, SqlCommand cmdHandler, SqlDataReader dataStream, BulkCopySimpleResultSet bulkCopyHandler, TdsParserStateObject stateObj) at System.Data.SqlClient.Sq... 06/24/2009 17:32:43.12* OWSTIMER.EXE (0x0788) 0x0DC4 Office Server Office Server General 900n Critical ...lCommand.FinishExecuteReader(SqlDataReader ds, RunBehavior runBehavior, String resetOptionsString) at System.Data.SqlClient.SqlCommand.RunExecuteReaderTds(CommandBehavior cmdBehavior, RunBehavior runBehavior, Boolean re 06/24/2009 17:32:43.13 OWSTIMER.EXE (0x0788) 0x0DC4 Office Server Office Server Shared Services 7fxr Exception (Watson Reporting Cancelled) System.Data.SqlClient.SqlException: Windows NT user or group 'ms.localsvcMossProd' not found. Check the name again. at System.Data.SqlClient.SqlConnection.OnError(SqlException exception, Boolean breakConnection) at System.Data.SqlClient.SqlInternalConnection.OnError(SqlException exception, Boolean breakConnection) at System.Data.SqlClient.TdsParser.ThrowExceptionAndWarning(TdsParserStateObject stateObj) at System.Data.SqlClient.TdsParser.Run(RunBehavior runBehavior, SqlCommand cmdHandler, SqlDataReader dataStream, BulkCopySimpleResultSet bulkCopyHandler, TdsParserStateObject stateObj) at System.Data.SqlClient.SqlCommand.FinishExecuteReader(SqlDataReader ds, RunBehavior runBehavior, String resetOptionsString) at System.Data.SqlClient.Sql... 06/24/2009 17:32:43.13* OWSTIMER.EXE (0x0788) 0x0DC4 Office Server Office Server Shared Services 7fxr Exception ...Command.RunExecuteReaderTds(CommandBehavior cmdBehavior, RunBehavior runBehavior, Boolean returnStream, Boolean async) at System.Data.SqlClient.SqlCommand.RunExecuteReader(CommandBehavior cmdBehavior, RunBehavior runBehavior, Boolean returnStream, String method, DbAsyncResult result) at System.Data.SqlClient.SqlCommand.InternalExecuteNonQuery(DbAsyncResult result, String methodName, Boolean sendToPipe) at System.Data.SqlClient.SqlCommand.ExecuteNonQuery() at Microsoft.Office.Server.Data.SqlSession.ExecuteNonQuery(SqlCommand command) at Microsoft.Office.Server.Data.SqlServerManager.GrantLogin(String user) at Microsoft.Office.Server.Administration.SharedResourceProvider.SynchronizeConfigurationDatabaseAccess(SharedComponentSecurity security) at Microsoft.Office.Se... 06/24/2009 17:32:43.13* OWSTIMER.EXE (0x0788) 0x0DC4 Office Server Office Server Shared Services 7fxr Exception ...rver.Administration.SharedResourceProvider.SynchronizeAccessControl(SharedComponentSecurity sharedApplicationSecurity) at Microsoft.Office.Server.Administration.SharedResourceProvider.Microsoft.Office.Server.Administration.ISharedComponent.Install() at Microsoft.Office.Server.Administration.SharedResourceProvider.Provision()  Aus irgendeinem Grund konnte der Account nicht korrekt auf den SQL Server zugreifen. Eine Analyse der IIS Application Pool Accouts hat gezeigt, dass dort einige Accounts in der Form “neu.kunde.deaccountname” (FQDN) und andere in der Form “neu.kundeaccountname” (NETBIOS) gespeichert wurden:\n\nIn der Datenbank werden die User wie folgt gelistet (NETBIOS):\n\nLösung:\nDas Problem ist also der FQDN. Das Provisioning des SSP wird jede Minute erneut versucht und hinterläßt die entsprechenden Spuren im Eventlog und im SharePoint Log. Jetzt ist die Hilfe von STSADM gefragt, damit die Accounts wieder gerade gezogen werden können. Einfach alle Accounts nach Anleitung des folgenden KB auf die Form “kunde.deaccountanme” (NETBIOS) ändern:\nKB 934838: How to change service accounts and service account passwords in SharePoint Server 2007 and in Windows SharePoint Services 3.0\nNach den Änderungen wurde der SSP provisioniert und ich konnte weiter machen :-)\nErst heute mit der Lösung “in der Hand” bin ich mit den richtigen Schlagwörtern über Bing.com zu folgendem Blog Post gekommen:\nThaddparker: Error in using FQDN Domain Names and MOSS 2007\nEin kurzer Check hat gezeigt, dass die Verwendung in Form von “accountname@neu.kunde.de” ebenfalls beim Provisionieren des SSP nicht funktioniert:\nApplication Server Administration job failed for service instance Microsoft.Office.Server.Search.Administration.SearchAdminSharedWebServiceInstance (15226029-e4cd-4ad6-aefc-965019284510). Reason: The specified account name is invalid. Parameter name: account Techinal Support Details: System.ArgumentException: The specified account name is invalid. Parameter name: account ---\u0026gt; System.Security.Principal.IdentityNotMappedException: Some or all identity references could not be translated. at System.Security.Principal.NTAccount.Translate(IdentityReferenceCollection sourceAccounts, Type targetType, Boolean forceSuccess) at System.Security.Principal.NTAccount.Translate(Type targetType) at Microsoft.Office.Server.Utilities.WindowsSecurity.ValidateAccount(NTAccount account, Boolean throwIfInvalid) --- End of inner exception stack trace --- at Microsoft.Office.Server.Utilities.WindowsSecurity.ValidateAccount(NTAccount account, Boolean throwIfInvalid) at Microsoft.Office.Server.Administration.SharedAccessRule.Validate() at Microsoft.Office.Server.Administration.SharedComponentSecurity.SetAccessRule(SharedAccessRule accessRule) at Microsoft.Office.Server.Administration.SharedResourceProvider.GetApplicationSecurity() at Microsoft.Office.Server.Administration.SharedWebServiceInstance.ProvisionSharedResourceProviderWebConfigSettings(SharedResourceProvider srp) at Microsoft.Office.Server.Administration.SharedWebServiceInstance.Synchronize() at Microsoft.Office.Server.Administration.ApplicationServerJob.ProvisionLocalSharedServiceInstances(Boolean isAdministrationServiceJob)  Leason learned? Es steckt halt immer noch zu viel Tahoe in SharePoint und NETBIOS rulez :-(\nCiao Marco\n","date":"2009-06-25","permalink":"https://marcoscheel.de/2009/06/25/sharepoint-fqdn-serviceaccounts-und-ssp-provisioning-failed/","tags":["xArchived"],"title":"SharePoint, FQDN, ServiceAccounts und SSP Provisioning failed"},{"content":"Ein weitere Kollege von Glück \u0026amp; Kanja versucht sich in der Blogosphäre ;-) Nach dem erfolgreichen Exchange-Pro-Expert-Guru und “Junior” C# Coder Jan Geisbauer ist nun auch Karsten Kleinschmidt mit von der Partie :-) Er betätigt sich zumindest in seinem Blog aktuell mit OCS (Office Communication Server). Sein aktueller Blogeintrag über das freie e-Book zum Thema OCS Programmierung war für mich ein Volltreffer :-)\n\nFür eine OCS Applikation (Prototyp für Communicator Automation API und UCC API) habe ich doch glatt “entdeckt”, dass ich die Objekte aus dem Communicator Automation API per “System.Runtime.InteropServices.Marshal.ReleaseComObject(object)” frei geben muss :-)\nDanke Karsten und weiter so.\nCiao Marco\n","date":"2009-06-24","permalink":"https://marcoscheel.de/2009/06/24/ocs-e-book-und-ein-weiterer-kollege-bloggt/","tags":["xArchived"],"title":"OCS, e-Book und ein weiterer Kollege bloggt"},{"content":"Nachdem in letzter Zeit viel Maintenance an bestehenden Systemen betrieben wurde, gab es auch mal wieder eine Initialinstallation bei einem Kunden. Es wurde sich für ein Single Box Deployment entschieden. Folgende Details (sehr übliche Konfiguration und von mir empfohlene Konfiguration):\n1. Windows Server 2008 Standard – X64 – SP2 – EN 2. Microsoft SQL Server 2008 Standard – X64 – SP1 – EN 3. Microsoft SharePoint Server 2007 Standard – X64 – SP2 – EN – Language Pack German  Ich hab mich an meine übliche Installationsreihenfolge gehalten und das Setup lief Weitestgehends reibungslos. Als einer der letzten Schritte kommt das Erstellen der Haupt-Portal-Applikation (in der Regel das Intranet Portal) an die Reihe. Ich installiere wann immer möglich ein englischen Basis SharePoint, damit man die Central Administration in Englisch bekommt. Für die Enduser muss es dann aber ein deutsches Portal sein, damit die Fachbereiche damit arbeiten können. Und was soll ich sagen… beim Erstellen der SiteCollection fehlt der DropDown für die Sprachauswahl :-S Für die Installation hatte ich einen alten Blog Post als Spickzettel verwendet: Wie installiere ICH SharePoint in einer Farm?\nIch hatte wie folgt installiert:\n1. MOSS x64 mit SP1 (Splistream Trial Download von MS) 2. Language Pack DE x64 (gibt es nicht als SP1, also Download aus dem MSDN Sub.) 3. WSS SP2 + WSS SP2 Language Pack (das Setup des Language Pack bracht ab und sagt es wäre nicht passend für das System) 4. MOSS SP2 + MOSS SP2 Language Pack 5. April CU WSS 6. April CU MOSS 7. PSCONFIG Wizard und erstellen einer neuen Farm  Bis auf den Punkt 3 Language WSS SP2 hat alles funktioniert. Ich habe ein paar alte Dokus zu reinen SP1 Installationen gecheckt und hatte das bisher genau so gemacht. Nach dem Language Pack habe ich WSS SP1 Language Pack und MOSS SP1 Language Pack installiert und das hatte beides funktioniert :-S Was war also los? Ich habe meine Testumgebung angeworfen und eine Neue SharePoint Farm erstellt. Genau dasselbe Verhalten. Dann mal an die Reparatur:\nVersuch 1:\n1. Download WSS SP1 Language Pack x64 DE 2. Installation bricht ab und sagt, es gäbe keine Software zu aktualisieren  Versuch 2:\n1. Download WSS Language Pack x64 DE 2. Installation WSS Language Pack** = OK** 3. Installation WSS Language Pack SP2** = OK** 4. Installation April CU WSS** = OK** 5. PSCONFIG** = OK**  Anlegen einer neuen Website im Central Admin und siehe da… ein Language DropDown :-)\nMir ist nicht klar, warum das Verhalten von SP1 Fresh Install zu SP2 Fresh Install sich unterscheidet, oder ob sich meine Install Sources geändert haben, aber Microsoft macht es uns nicht wirklich einfach, mit all den Versionen. Bei einer Single Box Installation kann man fast auf das Splipstreamen verzichten und gleich mit der nackten Version beginnen, was allerdings unter Windows Server 2008 zu einem Warnhinweis führt (Kompatibilitätshinweis, der ignoriert werden kann solange man ein SP1 oder höher nach installiert).\nIch habe nach den Problemen meine Installationsanleitung aktualisiert: Wie installiere ICH SharePoint in einer Farm?\nCiao Marco\n","date":"2009-06-24","permalink":"https://marcoscheel.de/2009/06/24/fresh-sharepoint-install-mit-sp2-und-language-pack-de/","tags":["xArchived"],"title":"Fresh SharePoint Install mit SP2 und Language Pack DE"},{"content":"Manchmal sind es Kleinigkeiten, die mich erfreuen :-) Beim Eintragen eines A-Record über ein W2K8 Server ist mir eben aufgefallen, dass man hier endlich mit Copy \u0026amp; Paste arbeiten kann. Gerade im DNS kann ein Tippfehler echt tödlich sein :-) In den Vorgängerversionen waren die einzelnen Quads abgebildet und der Versuch, etwas in das geteilte Eingabefeld zu kopieren, scheitert. Manchmal in weniger mehr.\nCiao Marco\n","date":"2009-06-24","permalink":"https://marcoscheel.de/2009/06/24/windows-server-2008-dns-snap-in-copy-paste/","tags":["xArchived"],"title":"Windows Server 2008 DNS Snap-In – Copy \u0026 Paste"},{"content":" Beim Arbeiten mit den ADO.NET Data Services (aka Astoria) kommt man irgendwann an den Punkt, dass die Aufgaben komplizierter werden oder die Last auf die Server steigt. In solchen Situationen ist man mit potentiellen Timeouts in allen Ebenen konfrontiert. In diesem Post zeige ich welche Timeouts es gibt, aber speziell will ich zeigen wie ich das Timeout des Entity Frameworks (SQL) in Kombination mit der MS REST Schicht ADO.NET Data Services anpassen kann.\nEs bleibt anzumerken, dass in der aktuellen Version V1 ein Bug existiert. Die Serverkomponente wirft bei einem Timeout eine NullReferenceException anstatt die eigentliche Fehlermeldung, was leider nicht wirklich weiter hilft.\nWelche Timeouts gibt es also und was sind die Defaults:\n1. **HTTP Client Timeout – Browser**  Leider habe ich hier kaum Informationen gefunden… nichts was ich hier schreiben möchte. 2. HTTP Client Timeout - ADO.NET Data Services Client Library Wo: Code Default: 100.000 Millisekunden (100 Sekunden) vererbt von HttpWebRequest.Timeout Mehr: MSDN - System.Data.Services.Client.DataServiceContext.Timeout Mehr: Microsoft KB 962933: Client Timeout Bug Mehr: ADO.NET Data Services Team Blog: Client Timeout Bug 3. HTTP Server ExecutionTimeout Wo: web.config Default: 110 Sekunden (wenn Debug=False, dann unendlich) Mehr: httpRuntime Element (ASP.NET Settings Schema) 4. **ADO.NET Data Services Server Library **Hier gibt es kein extra Timeout. Die unterliegende Connection (zum Beispiel SQL) bestimmt die Dauer. 5. Entity Framework Connection/Command Timeout (SQL) Wo: Code Default: 30 Sekunden CommandTimeout Default: 15 Sekunden SqlConnection\nWenn wir uns nun um ein Timeout kümmern müssen wir also immer checken, ob wir damit gegen ein anderes rennen. Angenommen wir wollen unseren Sql Queries 5 Minuten Zeit geben (nur für das Beispiel!), dann müssen wir folgendes tun:\n1. Entity Framework Timeout = 5*60 = 300 Sekunden 2. HttpServer Timeout = 5*60 + 10 (Puffer) = 310 Sekunden 3. ADO.NET Data Services Client Timeout = 5*60*1000 + 20*1000 (Puffer) = 320.000 Millisekunden (320 Sekunden)  Wichtig: Wir brauchen überall ein wenig Puffer, sonst kann es sein, dass uns die Gesamtgeschwindigkeit der Maschine (da kann ja noch mehr Code dazwischen laufen) dazwischenkommt.\nJetzt zum eigentlichen Inhalt des Posts ;-) Wie kann ich überhaupt das Timeout des Entity Frameworks erhöhen. Die ADO.NET Data Service Website nutzt das Entity Framework und stellt so die Tabellen und Views zur Verfügung. Der Default Code in den SVC Dateien erlaubt es nicht die Connection zu beeinflussen:\n using System; using System.Web; using System.Collections.Generic; using System.ServiceModel.Web; using System.Linq; using System.Data.Services; using NorthwindModel; namespace SimpleDataService { public class Northwind : DataService\u0026lt;NorthwindEntities\u0026gt; { public static void InitializeService(IDataServiceConfiguration config) { config.SetEntitySetAccessRule(\u0026quot;*\u0026quot;, EntitySetRights.All); } } }  Die Klasse DataService bietet nun eine Methode CreateDataSource mit der ich von Hand die EntityConnection erzeugen kann und somit auch den Command Timeout setzen kann:\nprotected override NorthwindEntities CreateDataSource() { var dbcon = new NorthwindEntities(); dbcon.CommandTimeout = Core.Config.Application.TimeoutSql; //Core is my helper library return dbcon; }  Ich hoffe ich kann dem einen oder anderen weiter helfen :-) Mir weiter geholfen bei der Lösung hat folgender Blogpost:\nTimeout in ADO.Net Entity Framework and ADO.Net Data Services\nCiao Marco\n","date":"2009-06-13","permalink":"https://marcoscheel.de/2009/06/13/ado-net-data-services-entity-framework-und-sqlhttp-timeouts/","tags":["xArchived"],"title":"ADO.NET Data Services, Entity Framework und SQL/HTTP Timeouts"},{"content":" Microsoft bietet unter folgender URL immer die letzte verfügbare PatchVersion für SharePoint 2007 an: Cumulative updates are available from the Microsoft Office team to fix reported problems\nCiao Marco\n","date":"2009-06-06","permalink":"https://marcoscheel.de/2009/06/06/aktuellsten-sharepoint-patchstand-finden/","tags":["xArchived"],"title":"Aktuellsten SharePoint Patchstand finden"},{"content":"Wer mit SharePoint arbeitet und darin die üblichen verdächtigen Dateitypen der Office Familie verwendet, kennt vielleicht die Securityhinweise, wenn zum Beispiel Excel aus einer SharePoint Lib geladen wird. SharePoint gilt in solchen Situationen eigentlich als Vertrauenswürdig ;-) Also sollten wir es auch so behandeln. Hier ein paar Info’s wie man das machen kann.\nGenereller Artikel: http://technet.microsoft.com/en-us/library/cc178946.aspx#trustedsettings\nDetail Informationen: MS Excel Team Blog - Trust Center Part 4: Trusted Locations\n\nWie komme ich zu dem Dialog? http://office.microsoft.com/en-us/help/HA100319991033.aspx\nAlternativ kann man die Einstellungen auch per Group Policy setzen und im Unternehmen verteilen.\nCiao Marco\n","date":"2009-06-06","permalink":"https://marcoscheel.de/2009/06/06/trusted-location-in-office-2007/","tags":["xArchived"],"title":"Trusted Location in Office 2007"},{"content":" In einem meiner letzten Projekte bin ich in die “glückliche” Lage gekommen, mit einer VAX über FTP zu kommunizieren. Gegen unser IIS7 FTP hatten alle Tests super funktioniert. Ich verbinde mich gegen den Server und mache dann einen Upload mit der Angabe der vollen URI:\nConnect: ftp.servername.com\nUpload File: /upd/knr/abc/request.txt\nMein erster Code nutzte die native C# FTP Api und antwortet einfach nur mit einem Fehler ohne Details. Ich habe dann auf eine Komponente von Aspose zurückgegriffen (wir nutzen Aspose.NET Total, was ich nur empfehlen kann). Ich bekam folgende Fehlermeldung:\nAspose.Network.Ftp.FtpException: Failed to upload file :500 %CLI-W-IVQUAL, unrecognized qualifier - check validity, spelling, and placement at Aspose.Network.Ftp.FtpClient.Upload(Stream localStream, String remoteFilePath) at Aspose.Network.Ftp.FtpClient.Upload(String localFilePath, String remoteFilePath) at Ibu.Core.Utility.Ftp.UploadFile(String localFilename, Boolean append) in C:UsersscheelCodeProjectXCoreUtilityFtp.cs:line 403 at ConsoleApplication1.Program.Main(String[] args)\nEine kurze Recherche brachte tatsächlich ein Ergebnis: http://www.physiology.wisc.edu/comp/docs/mg/ftp_users_guide.html\n   If the remote file specification includes ``/'', it will be necessary to enclose the entire remote file specification in double quotes, because the FTP command interpreter interprets ``/'' as the beginning of a qualifier. Notice the difference in behavior on some systems if the remote file specification is and is not quoted.    Meine Lösung: Ich habe den führenden Slash einfach weg gelassen und schon hat es funktioniert.\nCiao Marco\n","date":"2009-06-06","permalink":"https://marcoscheel.de/2009/06/06/c-ftp-gegen-openvms-ftp-server/","tags":["xArchived"],"title":"C# FTP gegen OpenVMS FTP Server"},{"content":" SharePoint und WSS können Dokumente von einer Bibliothek in eine andere, über ein DropDown Menü am Item selber, kopieren. Jetzt hatten wir bei einem Kunden ein echt komisches Problem. Auf einigen Rechnern gab es ein Problem beim Kopieren. Es ist auch aufgefallen, das bei einem Test auf einem Windows Server 2003, die Oberfläche für den Dialog ganz anders aussieht, also auf einer Standard Workstation. Mit Office 2007 auf der Maschine nutzt der Client ein ActiveX Control. Ohne ActiveX nutzt SharePoint eine ASPX Page und die kann zwar über SiteCollections hinweg kopieren, aber nicht über Application, also von http://portal.ms.local auf http://mysite.ms.local\nDas ganze wird hier beschrieben:\nhttp://office.microsoft.com/en-us/help/HA101208501033.aspx\n   By default, you can use the Send To command to copy files to libraries within a site collection. If you copy files from a client computer that has a program that is compatible with Microsoft Windows SharePoint Services 3.0, such as Microsoft Office Word 2007, and a browser that supports Microsoft ActiveX Controls, such as Microsoft Internet Explorer, you can use the Send To command to copy files not only between libraries within a site collection, but also between different Web applications. If you copy files from the browser of a client computer that does not support ActiveX Controls, you will be able to copy files only to libraries within site collections that share the same domain name (top-level site name) as the source library.    Ciao Marco\n","date":"2009-06-06","permalink":"https://marcoscheel.de/2009/06/06/sharepoint-send-to-could-not-find-the-destination-item-or-folder-on-this-server/","tags":["xArchived"],"title":"SharePoint – Send To – Could not find the destination item or folder on this server."},{"content":"Seit Vista und Windows Server 2008 kann man den Task Scheduler sehr vielfältig einsetzen. Eine sehr praktische Funktion ist das Auslösen von Aktionen (Email, Run Program, etc), wenn ein spezielles Event eintritt. Entweder man erstellt den Task direkt im Task Scheduler oder man kann auf das entsprechende Event klicken und ein Task anhängen.\n\nJetzt stellt sich die Frage, wie man das ganze aber testen kann? Warten bis das Event ausgelöst wird? Die Situation erzwingen? Je nach Komplexität der Anwendung oder der Situation, die den Event erzeugt, kann das nicht ganz befriedigend sein. Den Task einfach über Run zu starten, ist eine Option, allerdings kann man dann nicht auf Eigenschaften des Events zugreifen, welches normalerweise das Ereignis auslöst. Microsoft liefert hier ein mir bisher unbekanntes Tools mit:\nEventcreate.exe - http://technet.microsoft.com/en-us/library/bb490899.aspx\nSo kann man recht einfach sein Event erzeugen und Tasks, die zum Beispiel Daten aus dem Event lesen, voll testen. Feine Sache.\nCiao Marco\n","date":"2009-06-06","permalink":"https://marcoscheel.de/2009/06/06/tolles-trio-eventlog-task-scheduler-und-eventcreate-exe-attach-task-to-this-event/","tags":["xArchived"],"title":"Tolles Trio – Eventlog, Task Scheduler und Eventcreate.exe – Attach task to this event"},{"content":"Ich habe immer mal wieder in der Vergangenheit Probleme beim Ansehen von CHM Dateien, die ich aus dem Internet oder von einem Fileshare kopiert habe. Das Inhaltsverzeichnis wird angezeigt, aber egal was angeklickt wird, es kommt einer Fehlerdarstellung. Microsoft hat dazu eine KB Artikel: http://support.microsoft.com/kb/902225/\nWenn man das Unblocken jetzt nicht immer von Hand machen will, da es auch bei anderen Dateien potentiell störend sein kann, habe ich ein Tool von SysInternal (Microsoft) im Einsatz: Streams - http://technet.microsoft.com/en-us/sysinternals/bb897440.aspx\nMit diesem Tool kann man sehr einfach den NTFS Stream löschen, der diesen “Schutz” auslöst.\nstreams.exe -d -s *.chm  Fertig :-)\nCiao Marco\n","date":"2009-06-06","permalink":"https://marcoscheel.de/2009/06/06/unblock-aller-dateien-in-einem-verzeichnis/","tags":["xArchived"],"title":"Unblock aller Dateien in einem Verzeichnis"},{"content":"Dirk Primbs hat seine Top 10 Tools auf 14 erweitert, nachdem er meinen Post gelesen hatte. Live Mesh wurde besonders hervorgehoben. Ich nutzte für einige Dev-Aufgaben wie angedeutet ja ein Windows Server 2008. Die Basis Installation wurde überarbeitet und ich starte direkt mit SP2 die Neuinstallation. Jetzt wollte ich die tollen Tools mit dem “tollen” Tool Live Mesh auf meine Maschine holen… aber es wollte sich nicht installieren lassen :-S\n http://msdn.microsoft.com/en-us/library/aa368304.aspx\nMein Rechnung sieht also so aus: 14 Tools minus 1 Tool macht nicht 13 sondern 0, weil das eine Tool alle transportiert :-S\nIch glaube, ich hatte in der alten Installation UAC abgeschaltet und eventuell deswegen den Fehler nicht bekommen. Im Moment will ich doch mal versuchen mit UAC zu leben und dafür muss man dann kurzfristig folgenden RegKey setzen:\n\nNach der Installation habe ich den Key wieder gelöscht. Ich hoffe ich erlebe nicht noch mehr solcher Überraschungen.\nNach der Installation habe ich mich mal auf den Weg gemacht, den Grund zu finden… aber mehr als das habe ich nicht gefunden: http://social.msdn.microsoft.com/Forums/en-US/liveframework/thread/d71d367a-7604-4b26-9459-1a8f3dcdf7fc\nCiao Marco\n","date":"2009-05-13","permalink":"https://marcoscheel.de/2009/05/13/14-10-oder-windows-server-2008-sp2-als-development-workstation/","tags":["xArchived"],"title":"14-1=0 oder Windows Server 2008 SP2 als Development Workstation"},{"content":" Dirk Primbs stellt die Frage nach den Top Development Tools. Dirk Primbs: 10 Tools, die jeder .NET Entwickler kennen sollte\nIch stimme zu und hier meine Erweiterungen:\n* [LinqPad](http://www.linqpad.net/): Dirk nennt den [Snipped Compiler](http://www.sliver.com/dotnet/snippetcompiler/)… ich selber stehe auf [LinqPad](http://www.linqpad.net/), der mehr als nur Linq kann. Für 19$ gibt es dann auch [Autocompletion](http://www.linqpad.net/Purchase.aspx). * [Live Mesh](http://www.mesh.com): Ich habe eine Hauptentwicklermaschine und zusätzlich noch eine Windows Server 2008 VM. Meine Tools (um die es hier und in Dirks Post geht) und meine Reference Libraries (SharePoint DLL’s, Log4Net, …) halte ich so auf allen Rechnern aktuell und das ohne großen Aufwand. * [XML Notepad 2007](http://www.microsoft.com/downloads/details.aspx?FamilyID=72D6AA49-787D-4118-BA5F-4F30FE913628\u0026amp;displaylang=en): Da heute “Alles” als XML daher kommt, bringt das Tool einen schnellen Einstieg auch in komplexere Strukturen. * [TortoiseSVN](http://tortoisesvn.net/): Wir nutzen SubVersion und damit ist das Tool absolute Pflicht.  Wie sieht es bei euch aus?\nCiao Marco\n","date":"2009-05-12","permalink":"https://marcoscheel.de/2009/05/12/re-10-tools-die-jeder-net-entwickler-kennen-sollte/","tags":["xArchived"],"title":"RE: 10 Tools, die jeder .NET Entwickler kennen sollte"},{"content":" Ich bin nun schon seit einer Weile auf der Beta von Win7 unterwegs gewesen und habe gleich nach Verfugbarkeit des Release Candidate an diesem Wochenende auch neu installiert. In meinem letzten Beitrag hat mit dann Tobias angesprochen. Wie meine Erfahrungen mit dem Laptop und dem aktuellen Stand von Windows 7 sind. Hier nun mein Antwort fur \u0026ldquo;alle\u0026rdquo;:\nMein Firmenlaptop ist ein Lenovo X61 Tablet. Hier kurz die Hardware-Specs:\n Intel Core 2 Duo L7500 processor (1.60GHz, 4MB L2 cache, 800MHz front-side bus) 4 GB RAM 200GB 7.200 rpm Hitatchi HDD Graphics Intel Graphics Media Accelerator X3100 Intel Wireless WiFi Link 4965AGN (n-Disabled) Network Connection Bluetooth wireless technology Sierra Wireless MC8775 PCI Express MiniCard (nachgerustet)  Vor den Zeiten von Windows 7 hat mich Vista Enterprise 64 bit (English) durch den Alltag begleitet. Ich war mit der Performance zufrieden. Ich habe fur ein 3 Screenlosung ein zusatzlichen DisplayLink Adapter angeschlossen. So betreibe ich 2 Screens mit 1600x1050 und den LCD des Laptop mit 1400x1050. Die Geschwindigkeit ist \u0026ldquo;dank\u0026rdquo; der integrierten Grafikkarte ausreichend, aber nicht wirklich berauschend. Unter Windows 7 gibt es ebenfalls Beta Treiber, die nach zwei Nachbesserungen auch unter der Beta mit 3 Bildschirmen liefen. Allerdings ist DisplayLink wie angedeutet angewiesen auf die Leistungsfahigkeit der Graka und somit auch der Treiberqualitat. Der DisplayLink Adapter funktioniert unter Win7, allerdings war es fur mich zu langsam, dass ich lieber auf einen meiner Screens verzichte und auf die finalen Bits und Treiber von Intel und DisplayLink warte. Auch wenn das wichtige Punkte sind, durfte das nicht jeden X61 Besitzer interessieren ;-) Also los zum eigentlichen Thema:\nDie Installation habe ich von einem USB DVD Brenner gemacht, da mir das Erstellen eines USB Sticks, dann doch \u0026ldquo;zu\u0026rdquo; aufwendig war im Vergleich zum Brennen einer DVD. Die Installation geht ohne Probleme durch. Schon wahrend der Installation kann ich mein WLAN erreichen und die Aktivierung durchfuhren. Nach der Installation werden nur zwei unbekannte Gerate angezeigt.\nEin Gerat ist der UPEK Fingerprint Scanner. UPEK stellt auf seiner Website in Windows 7 kompatiblen Treiber bereit. Die Geschwindigkeit und die Integration in die neuen Win7 Api\u0026rsquo;s sind genial. ich habe nur den Treiber und nicht die ganze Suite installiert. Download hier.\nDas zweite Gerat ist meine WWAN Karte (UTMS 3G). Die Installation war schon in der Beta nicht ohne, da sie alles andere als offiziell ist. Ich hab es mit einem Treiber geschafft, die Karte zum Laufen zu bekommen. Ich habe auf die Lenovo Software zu Gunsten des Sierra Wireless Watcher 3G verzichtet und bin ganz happy. Die Installation ist hier naher beschrieben: Helmers Blog - Update : Windows 7 (64-bit) and Sierra Wireless HSDPA\nVon Lenovo habe ich noch den FestplattenSchockChecker installiert und die Pakete fur die Tablet Buttons (also System Interface, Power Management und Hotkey + Tablet Buttons). Ohne irgendwelche Settings komme ich durch die Installation mit einem Hinweis auf ein Regkeyfehler, aber das System lauft trotzdem.\nMein Fazit: Die Beta machte einen etwas stabileren Eindruck. Ich habe ab und an Probleme mit dem Sleep Modus. Ich schiebe, dass aber im Moment auf die Installation meiner WWAN Karte. Ich kann es nur empfehlen.\nMein Fazit fur Developer: Funktioniert und mittlerweile ist auch das Azure Webrole Problem gefixet.\nCiao Marco\n","date":"2009-05-11","permalink":"https://marcoscheel.de/2009/05/11/lenovo-x61-tablet-pc-mit-windows-7-betarc/","tags":["xArchived"],"title":"Lenovo X61 Tablet PC mit Windows 7 (Beta/RC)"},{"content":"Ein Bilick in Windows Update lohnt sich doch immer wieder:-) Ich bin mal gespannt was es bringt. Den FingerPrint Reader habe ich ja noch mit einem Tweak von UPEK selbst zum Laufen bekommen. Mit dem Video Driver erhoffe ich mir etwas mehr Speed, damit ich mal wieder meinen DisplayLink Adapter ausprobieren kann :-) Und wieder auf 3 Screens komme, die ich von der Performance unter Win7 auch nutzen kann.\n\nCiao Marco\n","date":"2009-05-03","permalink":"https://marcoscheel.de/2009/05/03/lenovo-x61-tablet-und-windows-7-rc-video-und-fingerprint-aktualisiert/","tags":["xArchived"],"title":"Lenovo X61 Tablet und Windows 7 RC – Video und FingerPrint aktualisiert :-)"},{"content":"SharePoint Joel hat ne ganze Menge toller Info’s da in den KB Artikel die X64 Language Packs von WSS und MOSS falsch verlinkt sind, kann man einfach von seinem Post aus starten:\nWSS \u0026amp; MOSS SP2 is Available - 5 No Brainer Reasons to Install\noder hier:\n* Stefan Gossner: [MOSS 2007 and WSS 3.0 Service Pack 2 has been released](http://blogs.technet.com/stefan_gossner/archive/2009/04/28/moss-2007-and-wss-3-0-service-pack-2-has-been-released.aspx) * Michael Greth: [SharePoint 2007 SP2 und WSS SP2](http://weblogs.mysharepoint.de/mgreth/archive/2009/04/28/sharepoint-2007-sp2-und-wss-sp2.aspx)  Irgendwann auch auf dem Team Blog der Produktes ;-)\nUnd was nun? Als SharePoint Developer und Architekt gilt es erst mal alle Kombinationen herunter zu laden, da unsere Kunden alle möglichen Kombinationen installiert haben:\n1. WSS only 2. WSS in English + Deutschen Language Pack 3. SharePoint Englisch mit Deutschem Language Pack 4. SharePoint nur in Deutsch (soll es auch geben)  Und ganz am Ende kommt noch die Plattform, also 64 oder 32 Bit was noch erlaubt ist ;-)\nNach dem Download sieht das dann so aus:\n\nNe ganze Menge. Was brauche ich aber jetzt für die Installation und was muss ich beachten?\n1. Was ist meine Plattform? 1. X64 2. X86 2. Welches Produkt habe ich installiert? 1. WSS 2. MOSS 3. Habe ich ein Language Pack installiert? 1. Deutsch 2. Englisch 3. … 4. Habe ich einen SharePoint Server oder mehrere? Auf wie vielen Servern wurden die Binaries installiert? Wenn man von SQL2K8 Reporting Installationen im Integrated Mode absieht, muss man den dedizierten SQL Server nicht berücksichtigen, weil dort nichts von SharePoint installiert wird. 1. One Server Farm 2. Multi Server Farm (zum Beispiel 2 WFE, 1 Application Server, 1 DB)  Alles Infos eingeholt? Dann gehts los:\n1. Für ein sinnvolles Backup sorgen 1. Datenbanken 2. Lokale Dateien in den IIS Verzeichnissen falls mal keine WSP für ein WebPart oder ähnliches eingespielt wurde 2. Auf dem Server mit der Central Administration (sollte es mehrere geben, einfach den ersten aussuchen) wird angefangen: 1. Je nach Plattform (X64/X86) Dateien verwenden 2. Zwischen den Patches niemals den PSCONFIG Dialog durchlaufen lassen 3. WSS SP2 in der Originalsprache (in der Regel hat die Central Admin Site die Sprache der Erstinstallation) installieren 4. WSS Language Pack installiert? Dann die zugehörigen Language Packs installieren 5. MOSS Installation? Dann das MOSS SP2 in der Originalsprache installieren 6. MOSS Language Pack installiert? Dann die zugehörigen Language Packs installieren 7. Multi Server Farm? Dann die selbe Reihenfolge auf allen anderen System durchführen 8. Alle Server haben die Binaries? Dann den PSCONFIG Wizard auf dem Central Admin Server starten und am Hinweis stehen lassen und erst auf allen anderen Server den PSCONFIG Wizard ausführen (nach einander, nicht gleichzeitig) 9. Alle Server durch? Dann auch den Central Admin Server Dialog durchlaufen lassen. 3. Checken, ob alles da ist :-) 4. Durchatmen  In Installationen mit mehreren Servern und großen Content-Datenbanken (\u0026gt; 5 GB) hat man einen deutlichen Vorteil, wenn man die Datenbanken vor der Installation per STSADM detached und nach der Installation wieder zu adden. Info’s finden sich hier:\nDeploy software updates for Office SharePoint Server 2007\nGerade in komplexeren Umgebungen ist die Installation nicht immer ganz einfach. Zu den Anfangszeite ist aber schon viel getan und dokumentiert worden.\nJetzt heißt es “Daumendrücken” bei Step 8 in der GUI oder Step 3 in der Console ;-)\nCiao Marco\n","date":"2009-04-28","permalink":"https://marcoscheel.de/2009/04/28/office-2007-und-office-server-2007-sharepoint-sp2-ist-da-und-wie-und-was-ich-installieren-muss/","tags":["xArchived"],"title":"Office 2007 und Office Server 2007 (SharePoint) SP2 ist da und wie und was ich installieren muss…"},{"content":" Dauert wohl nicht mehr lange:\nhttp://support.microsoft.com/default.aspx?scid=kb;EN-US;953334\nDie Downloads gehen im Moment des Posts noch nicht… aber das dauert sicher nicht mehr lange.\nQuelle:\nhttp://blogs.msdn.com/spfargo/archive/2009/04/28/microsoft-office-sharepoint-server-service-pack-2-is-now-available.aspx\nCiao Marco\n","date":"2009-04-28","permalink":"https://marcoscheel.de/2009/04/28/microsoft-office-server-2007-sp2-is-coming/","tags":["xArchived"],"title":"Microsoft Office Server 2007 SP2 is coming"},{"content":"Ich hatte die Gelegenheit, meinem Kollegen Jan bei einem Projekt zu helfen. Es geht darum, aus einer C# Applikation PowerShell Scripte zu erzeugen. Diese Scripte werden dann später auf einem Server ausgeführt. Klingt erst mal nicht wirklich spannend. Spannend wurde es dann, als der Code für die Erzeugung abgeschlossen wurde und das erste mal die Scripte ausgeführt wurden. Die PowerShell meldete eine kryptische Exception:\n\n   Unexpected token in expression or statement.    Beim Editieren des PowerShells mit dem neuen Windows 7 PowerShell Editor kommt beim Speichern folgende Meldung:\n\nUnd schon hatten wir den fehlenden Hinweis. .NET Applikationen arbeiten per Default mit UTF-8. Die PowerShell kommt aber wohl mit einigen Zeichen nur klar, wenn die Datei mit dem Unicode Encoding geschrieben wurde. Der Code wurde also kurz erweitert:\n\u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 1:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;rem\u0026quot;\u0026gt;//Alt\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 2:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;rem\u0026quot;\u0026gt;//File.WriteAllText(saveTo, fileContent);\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 3:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;rem\u0026quot;\u0026gt;//Neu\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 4:\u0026lt;/span\u0026gt; File.WriteAllText(saveTo, fileContent, System.Text.Encoding.Unicode);  Es kann manchmal so einfach sein ;-) Ein kurzer Test per NotePad2 zeigt das Ergebnis:\nVorher:\n\nNachher:\n\nSollte irgendjemand den genauen Grund kennen, dann wäre ich für einen echten Grund, warum die PowerShell hier streikt dankbar. Kurze Recherchen haben leider nichts gefunden :-S\nCiao Marco\n","date":"2009-04-20","permalink":"https://marcoscheel.de/2009/04/20/powershell-mit-c-generieren-und-scheitern-unicode-anstatt-utf-8/","tags":["xArchived"],"title":"PowerShell mit C# generieren und scheitern? Unicode anstatt UTF-8!"},{"content":"\nGenau… was passiert mit den ADO.NET Data Services (also dem REST Layer aus dem Hause Microsoft) in dem von Dariusz beschriebenen Szenario:\nDariusz Parys: Improving Entity Framework Performance\nWir nutzen die ADO.NET Data Services aktuell in einem Projekt und ich werde morgen nochmal den Code checken, ob wir auch von so einer Lösung profitieren können. Einen vielversprechenden Ansatz zeigt der Post von Kollegen Shawn Wildermuth auf:\nShawn Wildermuth: ADO.NET Data Services for Multiple Databases?\nDas Überschreiben des GetDataSource könnt hier ein Lösung bringen, allerdings muss man schauen, ob sich der Ansatz überhaupt auf eine Web-Anwendung anwenden läßt.\nCiao Marco\n","date":"2009-04-07","permalink":"https://marcoscheel.de/2009/04/08/improving-entity-framework-performance-und-was-ist-mit-dem-rest/","tags":["xArchived"],"title":"Improving Entity Framework Performance – und was ist mit dem REST?"},{"content":"Ich habe mich diese Woche wieder mit den Core Funktionen der ADO.NET Data Services beschäftigt. Wir nutzen aktuell das Microsoft Ex-Astoria Projekt, um eine SQL 2008 Datenbank (läuft mit SQL Mirroring) über das Internet an eine Applikation zu binden. An sich eine tolle Sache, aber immer wieder gibt es “neue” Situationen auf die man reagieren muss. In der klassischen Programmierung (C#, SQL über TCP 1433, …) kommt es recht selten vor, dass die Verbindung zur Datenschicht (SqlNativeClient, OLEDB, ODBC, …) unterbrochen wird. In unserem Szenario steckt viel “Cloud” in der Lösung und somit müssen wir mit dem “nicht immer” funktionierenden Internet rechnen. Aktuelle Implementierungen von Microsoft im Cloud-Umfeld, haben für solche Probleme im Code meist die Möglichkeit anzugeben, wie oft der Request wiederholt werden soll. Die ADO.NET Data Services haben so was noch nicht. Also was machen? Wir haben uns für den Weg der Extension Method entschieden. Und so sieht das dann aus (erste Tests waren sehr vielversprechend):\nusing System; using System.Collections.Generic; using System.Linq; using System.Text; using System.Data.Services.Client; using System.Diagnostics; namespace Core.ExtensionMethods { public static class DataServiceContextExtension { public static DataServiceResponse SaveChangesWithRetry(this DataServiceContext ctx, int retryCount) { for (int i = 0; i \u0026lt; retryCount; i++) { try { return ctx.SaveChanges(); } catch (System.Data.Services.Client.DataServiceRequestException ex) { #region Deeper analysis, disabled //foreach (ChangeOperationResponse cor in ex.Response) //{ // if (cor.Descriptor is EntityDescriptor) // { // EntityDescriptor ed = (EntityDescriptor)cor.Descriptor; // Trace.WriteLine(ed.State); // } // else if (cor.Descriptor is LinkDescriptor) // { // LinkDescriptor ld = (LinkDescriptor)cor.Descriptor; // Trace.WriteLine(ld.State); // } //} #endregion if (i + 1 \u0026gt;= retryCount) { throw; } } catch (Exception ex) { if (i + 1 \u0026gt;= retryCount) { throw; } } } throw new ApplicationException(\u0026quot;Loop must be wrong in retry SaveChanges\u0026quot;); } public static DataServiceResponse SaveChangesWithRetry(this DataServiceContext ctx, SaveChangesOptions options, int retryCount) { for (int i = 0; i \u0026lt; retryCount; i++) { try { return ctx.SaveChanges(options); } catch (System.Data.Services.Client.DataServiceRequestException ex) { #region Deeper analysis, disabled //foreach (ChangeOperationResponse cor in ex.Response) //{ // if (cor.Descriptor is EntityDescriptor) // { // EntityDescriptor ed = (EntityDescriptor)cor.Descriptor; // Trace.WriteLine(ed.State); // } // else if (cor.Descriptor is LinkDescriptor) // { // LinkDescriptor ld = (LinkDescriptor)cor.Descriptor; // Trace.WriteLine(ld.State); // } //} #endregion if (i + 1 \u0026gt;= retryCount) { throw; } } catch (Exception ex) { if (i + 1 \u0026gt;= retryCount) { throw; } } } throw new ApplicationException(\u0026quot;Loop must be wrong in retry SaveChanges Batch\u0026quot;); } } }  Wir haben das ganze nun in einer Library verpackt. Wenn es dann daran geht, den Code in einem anderen Projekt zu nutzen, das den oben gezeigten Code als Assemly einbindet, dann muss man, wie in der MSDN beschrieben, den Namespace als Using angeben. In unserem Fall also:\nusing Core.ExtensionMethods;  How to: Implement and Call a Custom Extension Method (C# Programming Guide)\n  This topic shows how to implement your own extension methods for any type in the .NET Framework Class Library, or any other .NET type that you want to extend. Client code can use your extension methods by adding a reference to the DLL that contains them, and adding a using directive that specifies the namespace in which the extension methods are defined.\n  Der aufrufende Code sieht also in etwas so aus:\nvar ctxMaster = Core.Data.Context.GetMaster(); //Wrapper around setting url and credentials var query = from ts in ctxMaster.TStuff where ts.StuffId == someGuidkey \u0026amp;\u0026amp; ts.IsDeleted == false select ts; var firstStuff = query.First(); firstStuff.IsDeleted = true; ctxMaster.UpdateObject(firstStuff); //ctxMaster.SaveChanges(); //Old: save and pray ctxMaster.SaveChangesWithRetry(3); //New: with retry!  Der Post bringt hoffentlich zwei Dinge: 1.) Wie einfach es ist ein Retry auf einem “MS-REST” Context funktioniert und 2.) Was man tun muss, um die Extension Method auch in einem anderen Projekt oder Namespace nutzen zu können.\nCiao Marco\n","date":"2009-03-28","permalink":"https://marcoscheel.de/2009/03/28/ado-net-data-services-c-extension-method-to-the-rescue-savechangeswithretry/","tags":["xArchived"],"title":"ADO.NET Data Services – C# Extension Method to the rescue – SaveChangesWithRetry"},{"content":"Wer regelmäßig Google als Suchdienst nutzt (Was nutzt man sonst?) wird an speziellen Tagen feststellen, dass sich das Google Logo ändert, wenn spezielle Ereignisse die “Welt” bewegen. Google spricht dann von einem Doodle. Microsoft hat sich das wohl abgeguckt und hat für die aktuelle MSDN Site das Layout geändert/gedoodlet. Grund ist die aktuelle MIX09. Sehr nette Idee:\n\nSelber mal anschauen: http://msdn.microsoft.com/en-us/default.aspx\nMehr Informationen zum Thema Google Doodle: http://www.google.com/support/websearch/bin/answer.py?hl=en\u0026amp;answer=17509 http://googleblog.blogspot.com/2004/06/oodles-of-doodles.html http://www.google.com/holidaylogos.html\nCiao Marco\n","date":"2009-03-18","permalink":"https://marcoscheel.de/2009/03/18/microsoft-is-doodling/","tags":["xArchived"],"title":"Microsoft is doodling?"},{"content":"Heute habe auch ich Post im Briefkasten. Das .NET Logo (seit der PDC 08) stand gleich neben meinem Name. Der Inhalt war etwas ratselhaft:\nNaturlich hab ich gleich die Microsoft Tag Software fur mein Windows Mobile installiert (was bestimmt Ziel der Aktion war):\nGetTag.mobi\nMicrosoft - Tag\nMein Kollege Jan hat es auch schon erwischt:\nJan Geisbauer - Stille Post\nUnd weitere .NET Devs:\n Der Albert.com sein Blog - Langeweile und dass Microsoft-Tag .NET virales Marketing - mehr Enthullungen Michael Schwarz - Microsoft Tag und Virales Marketing  Es gibt aber auch negatives Reaktionen:\nKay Giza - Bin sauer: Virales Marketing, es nervt!\nMich personlich stort es nicht… aber ich bekomme so etwas nicht so oft :-) Bin mal gespannt, ob da noch was raus kommt.\nCiao Marco\nUPDATE 2009-03-10: Nach dem Hinweis von Kay (danke fur den Kommentar) habe ich den Titel geandert (alt: Post von Microsoft mit Tag und 01).\n","date":"2009-03-09","permalink":"https://marcoscheel.de/2009/03/09/snailmail-aus-internet-mit-microsoft-tag-und-01/","tags":["xArchived"],"title":"SnailMail aus Internet mit Microsoft Tag und 01"},{"content":" Seit letztem Jahr arbeiten wir an einem Projekt, welches die ADO.NET Data Services (aka Astoria) einsetzt, um auf die Daten einer Microsoft SQL 2008 Datenbank zuzugreifen. Der Einsatz der Technologie hat gezeigt, das die Theorie zu REST + SQL Server DB ganz toll klingt, man sich die Features aber teils teuer erkauft. Ich möchte irgendwann man ein eigene Post zum Thema schreiben. Heute möchte ich einfach mal zeigen, über was man sich plötzlich alles freuen kann (aus dem Blog von des Astoria Teams):\n   Row Count: One scenario we heard a ton of feedback on after shipping V1 of ADO.NET Data Services in the .NET Framework 3.5SP1 is the ability for the a client of a data service to determine the total number of entities in a set without having to retrieve them all. To address this need, we have extended the data services addressing scheme to allow a client to obtain this type of information without having to download all the entities in a set.       Enhanced BLOB Support: This feature enhances the BLOB support provided in V1 to enable data services to stream arbitrarily large BLOBs, store binary content separate from its metadata, easily defer the loading of BLOB content when its metadata is requested, etc. For CTP1 we'll include server side support for this feature only. Client library support will likely come in a future CTP.    Mehr Informationen findet man hier: Project Astoria Team Blog - Announcing ADO.NET Data Services v1.5 CTP1 JohnPapa.net - ADO.NET Data Services v1.5 is on its way\nDie Entscheidung in unserem Projekt zu Gunsten von ADO.NET Data Services bleibt noch immer eine richtige, allerdings hätte man als \u0026ldquo;Dev mit Microsofterfahrung” wissen müssen, das eine V1 nie reibungslos läuft :-)\nCiao Marco\nP.S. Was mich am meisten stört, ist die Problematik richtig nach dem Produkt zu suchen, da es bisher wohl keine “offizielle” Abkürzung gibt.\n","date":"2009-03-03","permalink":"https://marcoscheel.de/2009/03/03/ado-net-data-services-v-1-5-sind-im-ctp1-status/","tags":["xArchived"],"title":"ADO.NET Data Services V 1.5 sind im CTP1 Status"},{"content":" Ich hatte bereits auf das aktuelle Update hingewiesen. Am Donnerstag kam dann auch auf dem “offiziellen” Blog die Nachricht zum Februar Update:\nIs it time to apply Feburary Cumulative Update?\nWas soll man also davon halten? Ihr habt bereits den Dec CU drauf? Dann könnt ihr eigentlichen auch den aktuellen Patchstand für Februar einspielen. Wichtig! Im Blogpost wird auch die Reihenfolge der Patches genannt und an die sollte man sich in der Regel auch halten:\n   For customers who already have Dec CU installed, they can apply these update packages in the following sequence: 961750, 967703, 961749, 961754. But we also suggest to wait for the uber package.    Ich werde die Patches auf jeden Fall am nächsten internen PatchDay (Mi) einspielen, da wir “natürlich” schon auf dem Dec CU laufen :-)\nCiao Marco\n","date":"2009-02-28","permalink":"https://marcoscheel.de/2009/02/28/sharepoint-feb-cu-2009-bitte-warten/","tags":["xArchived"],"title":"SharePoint Feb CU 2009 – Bitte warten?!"},{"content":" Heute habe ich eine Mail mit der Info des aktuellen CU für einen der offenen SharePoint PSS Cases bei einem Kunden bekommen. Es wurde auf das Release des aktuellsten Updates für SharePoint hingewiesen. Es gibt noch kein “Rundum-Sorglos” Paket, aber für den dringenden Fall kann man wie in dem Blogpost genannt vorgehen und “traditionell” installieren.\nJoerg Sinemu - February Cumulative Update for WSS V3 and MOSS 2007\nWir hatten vor einigen Monaten ein Case zum Thema BLOB Caching eingestellt. Es ging darum für ein Intranet Videos (Flash – FLV) zu cachen, damit die Datenbank entlastet wird. An sich funktionierte das auch super, allerdings wurde auf allen System des Kunden ab einer Größe von 40-50 MB die Datei nicht ausgeliefert und man bekam eine Anmeldemaske (HTTP Error 401). Es konnte sich aber nicht authentifiziert werden. Nach einigen Repros auch bei MS dann die Info, dass es eine API des IIS Schuld daran war. Kurzerhand hat man die gerufene API gewechselt und nun soll es wieder gehen… bin mal gespannt :-)\nHier der von mir genannte Entry im KB Artikel:\n   If blob caching is enabled for a Web site, you cannot download files whose size is more than 32 megabytes (MB) from that Web site.    Ciao Marco\n","date":"2009-02-25","permalink":"https://marcoscheel.de/2009/02/25/sharepoint-feb-2009-cumulative-update-blob-caching-fixed/","tags":["xArchived"],"title":"SharePoint Feb 2009 Cumulative Update – BLOB Caching fixed"},{"content":"Es ist so weit… die Welt hat nur darauf gewartet :-) Ich hatte direkt am Donnerstag Abend den MSDN Download angeworfen und auf Freitag als ersten Windows 7 Tag gehofft… leider kam es anders. Der Download ist mehrfach abgebrochen. Der Download Speed ist trotz VDSL nicht über 25 KB/S gekommen. Am Montag morgen war es dann so weit… allerdings hatte ich in den letzten 30 Minuten die x86 und die x64 Version über den Public Download bei Akami fertig… was schon eher VDSL Speed entspricht :-)\nWer nun die erste Runde gedreht hat (ich kenne das meiste schon aus dem PDC Alpha Build) und in den zweiten Level will, der kann mal hier nach ein paar echt tollen Tips schauen.\nhttp://blogs.msdn.com/tims/archive/2009/01/12/the-bumper-list-of-windows-7-secrets.aspx\nCiao Marco\n","date":"2009-01-14","permalink":"https://marcoscheel.de/2009/01/14/windows-7-fieber-second-level/","tags":["xArchived"],"title":"Windows 7 Fieber – Second Level"},{"content":"\nAlle Jahre wieder beginnt der Poker um die Mobilfunkverträge und diesmal gab es neben dem Geld auch noch ein Hardwaregrund für die Überlegungen. Apple unterstützt seit dem iPhone 3G “Exchange Active Sync” und ist somit einfach ein weitere mobiler Outlook Client an einer gut funktionierenden Schnittstelle. Ich nutzt schon seit Anbeginn der Zeit Outlook als PIM. Seit meinem ersten mobilen Windows Begleiter (dem Ur-HTC schlecht hin) bin ich von der reibungslosen Synchronisation begeistert. Ich will nicht sagen, das der Erfinden von Active Sync uns einen Gefallen getan hat :-) Aber besser als mein Siemens S45 oder Sony Ericsson T610 hat das ganze auf jeden Fall funktioniert.\nEin iPhone kommt also selbst in einer Microsoft Famlie zur Diskussion. Also auch bei meinem Arbeitgeber. Neben den Features der Multisim und der Nutzung von 3 Sim-Karten zu “selben Zeit”, hat das iPhone den Wechselgrund bekräftigt. Der Karton zeigt… wir haben gewechselt und nicht wenige sind der Microsoft Plattform untreu geworden… Wer den Satz richtigt deutet… richtig, ich bin ein HTC und Windows Mobile Anhänger geblieben. Ein Tag iPhone hat mir gezeigt, das es ein tolles Gerät ist, aber ich softwaretechnisch nicht Apple-Kompatibel bin. Hier einige Punkte, die mich überzeugt haben bei Microsoft zu bleiben:\n1. Ich hatte keinen Apple Account und wollte keine Kreditkarte bei der kostenlosen Anmeldung hinterlegen, also konnte ich den sicher tollen AppStore nicht nutzen 2. Ich nutze den Windows Media Player für meine Musik und hatte kein iTunes auf dem Rechner, also musste ich zum Freischalten erst mal einen Rechner finden und ich weiß jetzt, dass mir iTunes nichts Neues bietet, was ich beim WMP nicht haben und noch wichtiger es auch nutze. 3. Das Gerät hat keine Hardware-Tastatur und ich nutze diese mehrmals täglich. Mein erstes WinMo-Gerät hatte keine Tastatur und ich weiß, das ich ein will. Ein Jahr Motorola MPx220 hat mir auch gezeigt, dass [Clam-Shell](http://en.wikipedia.org/wiki/Clamshell) Handys auch dazu führen, das ich nix mehr tippe. Ich habe mich nicht an T9 gewöhnt und ich würde mich nicht an die iPhone Tastatur gewöhnen. Auch wenn ich damit alleine auf dieser Welt bin :-) 4. Bilder syncen wohl nur dann auf Basis von Metadaten, wenn man iPhoto nutzt, also irgendwo ein echten Mac mit Mac OSX hat. Im Windows Media Player sage ich einfach, dass alle Bilder mit einem Stern oder höher auf die microSD Karte meines HTC TYTN kommen. Fertig. 5. Die Software auf dem Gerät hat mich nicht überzeugt. Sie ist gut und funktioniert, aber das tut mein WinMo Gerät ebenfalls. (Ich rede von meinem :-) keine Ahnung was ihr mit eurem Gerät macht) 6. Das Gerät lässt sich nicht per MiniUSB laden und syncen. Ich habe überall MiniUSB. Auto, Zuhause, Büro und unterwegs.  Fazit: Hätte ich bereits ein iPod, dann würde ich viele meiner Probleme nicht haben und ich würde heute darüber schreiben, wie toll mein neues iPhone ist. Um Fair zu bleiben, will ich eben noch die Pro’s auflisten:\n1. Der Browser ist gut, sehr gut, sehr intelligent. Opera auf WinMo sieht gut aus, aber die kleinen Details zeigen, wie deutlich der mobile Safari gewinnt. Besonders der intelligente Zoom schiebt Apple weit nach vorne. 2. Das Gerät reagiert, wenn man es berührt. Nicht “so” umwerfend, wie von einige beschrieben und ich es deshalb erwartet hatte, aber in der Regel schlägt Apple MS hier um Längen. Wie immer hat Apple den Vorteil, Hardware und Software zu stellen + den Preis vorzugeben. Nur all zu oft kommen WinMo Geräte mit schlechtem Design für Speicher und CPU daher… bezahlt wird auf lange Sicht in Imageverlust und Marktanteilen. 3. Der AppStore ist auch wenn von mir ungetestet eine geniale Erfindung für Anwender und Software-Bastler. 4. Mein Ford S-Max Navi hat das Adressbuch per Bluetooth zwar nicht gefüllt (keine Sprachsteuerung) aber ich hatte alle Nummern in der Anzeige und konnte per “Knopf” wählen. Mein HTC zeigt vielleicht 200 Kontakte an… der Rest fehlt.  Mein mobiles Browser beschränkt sich aber auf mein Google Reader in der Mobile-Variante und das schafft selbst der Pocket IE noch ohne Probleme. Ich installiere “keine” Applikationen auf meinem Device. Es reicht mein Today Kalender Plugin, SideShow, Windows Live (für Hotmail Push Mail) und bisher ein Picture Viewer. Mein VPA lebt im Moment sogar ohne HTC Taskmanager und alles ist super. Und telefonieren kann ich auch mit dem Gerät. Ich habe gelernt, mit den wenigen Macken in der Plattform zu leben und bleibe deshalb beim Gerät mit der Tastatur.\n\nIch bin gespannt, ob die Apple Plattform den einen oder anderen Kollegen zurück zum Windowslager treibt. Ich würde Momentan nicht mal auf einen wetten (da ich mich ja schon jetzt dagegen entschieden habe ;-). Wir werden es sehen. Jan?\nCiao Marco\nP.S. Das “Karton-voll-mit-iPhones” Bild ist über Live Mesh Mobile auf mein PC in den Blog gekommen. Sehr genial… aber auch das kann Apple :-)\n","date":"2008-12-12","permalink":"https://marcoscheel.de/2008/12/12/von-microsoft-zu-apple-von-vpa-zu-iphone/","tags":["xArchived"],"title":"Von Microsoft zu Apple von VPA zu iPhone"},{"content":"\nWer bisher noch nichts vom Google Reader gehört hat oder nicht warm geworden ist, der kann es ja jetzt nochmal mit dem “besten” RSS Reader versuchen :-) Mir gefällts.\nGoogle Reader\nGoogle Reader Blog: Square is the new round\nCiao Marco\n","date":"2008-12-09","permalink":"https://marcoscheel.de/2008/12/09/google-reader-updated/","tags":["xArchived"],"title":"Google Reader updated"},{"content":"Immer mal wieder passiert es meinen Kollegen und mir das Technet oder MSDN Artikel unklar oder falsch sind. Bisher war es immer besonders ärgerlich, wenn man eine Lösung hatte und maximal auf seinem Blog darüber berichten konnte. Die aktuellen Versionen der MSDN und Technet können “Community Content” anzeigen. Ich habe eben ein Fehler in der Performance Point Kerberos Konfig-Anleitung gefunden und kommentiert… das nenne ich mal Web 2.0:\nPPS: Configure Monitoring Server for Kerberos delegation\n\nVielleicht hilft mein Kommentar, dem einen oder anderen weiter und es sorgt für ein paar schlaflose Nächte weniger. Gerade im Kerberos Umfeld ist so ein Fehler, ohne das richtige Know-How kaum zu finden.\nCiao Marco\n","date":"2008-12-09","permalink":"https://marcoscheel.de/2008/12/09/web-2-0-gelebt-msdntechnet-verbessern/","tags":["xArchived"],"title":"Web 2.0 gelebt – MSDN/Technet verbessern"},{"content":"**Keynote **Die Keynote heute geht an die Research Abteilung von Microsoft. Es wurde kurz über die Geschichte von MSR gesprochen und welche “Größe” die Einrichtung hat. Es arbeiten eine ganze Menge hoch dekorierter Leute da. Die Statistiken (naja was sagt das schon aus) haben gezeigt, dass die smartesten Leute bei MS arbeiten :-) Für mich interessant war der Beitrag über ein Projekt, in dem Microsoft versucht Kindern die Logik des Programmieren bei zu bringen. Es handelt sich quasi um eine programmierbare Welt die über einen Xbox 360 Controller gesteuert wird. Es war wirklich spannend zu sehen, wie man mit viel Spaß logisches Denken lernt und “komplexe Zusammenhänge” erkennt. Das zweite Highlight war die Vorstellung der nächsten Generation der Surfacegeräte (SecondLight). Neben der eigentlichen Oberfläche, sitzt noch ein zweiter Projektor unter dem Tisch, welcher aber nicht auf der Oberfläche erscheint sondern auf einer einfachen “Plastikscheibe” die man einfach über den Tisch hält. Einfach mal in die Keynote reinschauen.\nBilder:\n        \nIIS 7.0 and Beyond: The Microsoft Web Platform Roadmap In der Session gab es mal wieder ein “liebloses” Demo einer ASP.NET WebSite. Es wurden ein paar Standard IIS7 Feature gezeigt (ein Image-WaterMark-Handler). Die Tools die vorgestellt wurde haben es allerdings in sich. BitTrotteling (wenn man so was machen muss) ist einfach zu bedienen und kann geld und Bandbreite sparen. Es wurd gezeigt, wie man ein Video wirklich streamt und dann noch ein “nicht spultbaren” Video-Ad davor hängt. Super easy. Interessant war auch das Microsoft Web Deployment Tool. Im IIS 7 kann ich zwar mit Copy \u0026amp; Paste eine WebSite in Config erzeugen, aber ein echter Transfern ist schon komplizierter. Alle Webdeveloper sollten sich tatsächlich dieses Tools mal anschauen: http://www.iis.net/downloads/default.aspx?tabid=34\u0026amp;i=1603\u0026amp;g=6\nBilder:\n \n**Microsoft SQL Server 2008: Powering MSDN **Wieder mal ein Lunch-Session und diesmal auch noch mit geändertem Raum. Ich war also zu spät und nicht der einzige der den Raum nicht gefunden hatte. Es wurde gezeigt, wie das Team der MSDN es bewerkstelligt die Inhalte aus verschiedenen Fremdsystemen zu importieren und weltweit zu replizieren. Bei der Menge an Daten ist es wichtig die Änderungen nur dann zu machen, wenn sie wirklich nötig sind, sonst repliziert sehr viel Content umsonst um die ganze Welt. Es wurde primär der Einsatz von MERGE und FULL JOIN benutzt. Zwei Statements die mit SQL 2008 eingeführt wurden. Jeder der Daten importiert und das effizient tun will, der sollte sich das mal anschauen. MERGE ist zum Beispiel dann sehr schön einzusetzen, wenn man bei einem bestehenden Datensatz ein Update macht und nur dann wenn er fehlt ein Insert. Einfach mal die Session ansehen und dann die Hilfe lesen.\nBilder:\n    [](http://marcosche el.de/files/media/image/WindowsLiveWriter/PDCTag3KeynoteundSession_611A/IMG_3793.jpg) \nImproving .NET Application Performance and Scalability Der Titel war etwas unglücklich gewählt. Es ging nicht darum wie man im allgemeinen die Performance steigert, sondern was man mit Hilfe der Team Edition für Tester (wenige Dinge auch mit der Developer Edition) tun kann, um Performance Probleme aufzudecken. Beispiele kam direkt von Microsoft und wie man den Team Foundation Server selber mit diesen Tools überwachte. Interessant war es allemal. Gelernt hae ich, dass ich irgendwann doch die Team Suite brauche, oder eben die Test Suite ordern muss.\nBilder:\n\n**\u0026ldquo;Dublin\u0026rdquo;: Hosting and Managing Workflows and Services in Windows Application Server **Eine tolle Session über die Zukunft und die Optimierung im Workflow- oder besser Workflow-Hosting-Bereich informiert. Ein negatives Highlight war der Auftritt eines ISV (mit einem durchaus interessanten Produkt). Laptop läuft auf XP und also die Demo nicht läuft steht er für Ewigkeiten da und tut… nichts.\nBilder:\n     \n.NET Services: Access Control Service Drilldown Letzte Session des Tages war nochmal ein echter Höhepunkt. Sehr unterhaltsam und es wurde nochmal deutlich wie wichtig MS Interoperabilität an dieser Stelle ist. Der Access Control Service hat das Potential zur Killerapplikation im Bereich Authentifizierung. Das Arbeiten in den Applikationen mit Claims und den gezeigten Techniken, wird das Arbeiten über Unternehmensgrenzen hinaus den Weg ebnen.\nBilder:\n \nCiao Marco\n","date":"2008-11-08","permalink":"https://marcoscheel.de/2008/11/08/pdc-tag-3-keynote-und-session/","tags":["xArchived"],"title":"PDC – Tag 3 – Keynote und Session"},{"content":"In vielen unserer Projekte ist ein SQL Server Teil der Lösung. In einigen Projekten muss über die Daten in den Tabellen gesucht werden. Für kleinere Projekte mit kleinen Datenbeständen kommt man meist mit dem T-SQL Bordmittel LIKE aus. Ein Projekt hat jedoch so “viele” Daten und zu speziellen Zeiten so viel Traffic, das eine Suche mit Hilfe von LIKE nicht mehr vertretbar ist. LIKE an sich ist nichts schlimmes, wenn es nicht am Anfang ebenfalls ein Wildcard enthält. Die Anforderungen sind aber meist “irgendwo soll das Suchwort vorkommen”. In solchen Situationen bietet es sich an auf den Volltext-Dienst des SQL Server zurückzugreifen. Folgendes Bild deutet an, wo man unter Last Probleme bekommt:\n\nDer Fulltext-Service des SQL Server bringt also die nötige Reserve und noch mehr. Beim Lesen meiner Blogs bin ich nun auf die Seite von Simon Sabin gestoßen. Er hat eine Serie von Posts zum Thema SQL2K8 und was neu ist im Bereich Volltext. Nach dem Lesen bleibt der Schluß: Ein Feature mehr warum man Upgraden sollte. Hier der Link zur Serie und mein persönliches Highlight:\n1. Serie: [SQL Server 2008 – iFTS Introduction](http://sqlblogcasts.com/blogs/simons/archive/2008/02/19/SQL-Server-2008---iFTS-Introduction.aspx) 2. Mein Highlight – Was genau macht CONTAINS aus meiner Anfrage: [SQL Server 2008 – iFTS Transparency – dm_fts_parser](http://sqlblogcasts.com/blogs/simons/archive/2008/02/20/SQL-Server-2008---iFTS-Transparency---dm_5F00_fts_5F00_parser.aspx)  Ciao Marco\n","date":"2008-11-08","permalink":"https://marcoscheel.de/2008/11/08/microsoft-sql-server-2008-interated-fulltext-service-ist-cool/","tags":["xArchived"],"title":"Microsoft SQL Server 2008 – Interated Fulltext Service ist cool"},{"content":"Nach einem interessanten Tag ging es mit dem Bus erst zurück ins Hotel und dann zu den Universal Studios. Der Freizeitpark war ausschließlich für PDC Teilnehmer (und Anhang) geöffnet. Bei dem Datum hat es sich angeboten ein Halloween Special zu veranstalten. Hab ich schon mal gesagt, dass ich Horror hasse? An jeder Ecke ist aus dem Dunkeln eine Gestalt hervorgesprungen. Zombies schlurfen langsam über den Weg nur um dann hinter einem kehrt zu machen und mich zu erschrecken. Den größten Schrecken haben mir aber diese Irren mit den Kettensägen eingejagt. Die Jungs haben den Motor aufgedreht, es hat gestunken und dann sind sie mit der Klinge auf dem Boden funkensprühend in die nächste Menschraube gestürmt. Mein Herz ist mehr als einmal stehen geblieben, ganz besonders, wenn man sich gerade angeregt unterhalten hat.\n   \nBeim Essen hab ich dann ein paar SharePoint-Leidensgenossen kennen gelernt. Es war eine echt cooler Abend. Ich hab ein paar Achterbahnen mit gemacht und ganz am Ende noch die letzte Waterworld-Show mitgenommen. Super Event.\n  \nCiao Marco\n","date":"2008-11-02","permalink":"https://marcoscheel.de/2008/11/02/pdc-tag-2-universal-studios-party/","tags":["xArchived"],"title":"PDC – Tag 2 – Universal Studios Party"},{"content":"**Keynote – Don und Chris **Nach der Hauptkeynote ging es ohne PowerPoint weiter. Don Box und Chris Anderson haben in der Vergangenheit immer für viel Unterhaltung gesorgt. Don Box ist quasi eine Legende auf der Bühne. Einer redet, einer codet. Einfacher und überzeugendes Konzept. Es wurde gezeigt, wie ein Beispiel von der letzten PDC (2005 über WCF) umgebaut wurde, um in der Cloud zu leben und gegen .NET Services zu authentifizieren.\nBilder:      \nSQL Services: Tips and Tricks for High-Throughput Data-Driven Applications Die Session war schrecklich. Es war eine Lunch Session und durch die beiden Keynotes war es schon knapp, überhaupt was zu essen. Der Speaker hat an dem Tag nichts auf die Reihe bekommen. Keine einzige Demo hat funktioniert. Solche Tage soll es ja geben, aber er hat das ganze nicht wirklich professionell gehandhabt.\nProject \u0026ldquo;Velocity\u0026rdquo;: A First Look Ein neues und interessantes Feature wird die Windows Server 2008 erweitern. Der “Distributed Cache” wird über kurz oder eine normale Serverrolle ergeben. Die Software stellt einen verteilten und hochverfügbaren Cache zur Verfügung. Die Demos haben gezeigt wie einfach das alles aussieht und auch funktioniert. Der aktuelle Stand kann zum Beispiel den ASP.NET SessionState halten. Von der Performance kommt “Velocity” an den ASP.NET StateServerice heran, mit dem Unterschied, dass “Velocity” hochverfügbar ist und super skaliert. Ich kann es nur jedem Entwickler und Architekten ans Herz legen. Schaut euch mal das Produkt und die Session an. Durch solche Techniken können plötzlich auch .NET Applikationen auf eine Cache-Infrastruktur ähnlich wie ASP.NET Cache zurückgreifen. Die Lösung wird kostenlos bleiben und Teil des “OS” werden.\nBilder:\n    \n**\u0026ldquo;Oslo\u0026rdquo;: The Language **Die Sesion habe ich hauptsächlich wegen Don Box gemacht. “Oslo” hat mich auch interessiert und die Session war die einzigste zum Thema an der ich Zeit hatte. Die Umsetzung im Bereich DSL ist gut gemacht. Für mich und meinen aktuellen Aufgaben ist es aus Entwicklersicht erstmal nicht so wichtig. Als User einer Entwicklungsumgebung kommt man aber sicher früher oder später damit in Berührung. Bleibt das Fazit: Ich mag Don Box :-)\nBilder:\n       ![IMG_3722](http://marcoscheel.de/files/media/ima ge/WindowsLiveWriter/PDCTag2Session_46A6/IMG_3722_thumb.jpg) \n**ASP.NET and JQuery **Der Tag 2 endet in dem Bereich mit dem ich “damals” angefangen habe. ASP.NET steht nicht oft auf meiner Agenda, aber wenn es um die Kombination ASP.NET und JavaScript geht, dann muss man doch mal auf dem Laufenden bleiben. jQuery wird jetzt offiziell von Microsoft supported. Richtig: Wir können sogar ein PSS Call zum Thema aufmachen und MS kümmert sich. jQuery bleibt allerdings so organisiert wir bisher. Microsoft hat sich einfach entschieden, die eigene AJAX Plattform um ein gutes Tool für einen bisher nicht besetzten Bereich zu ergänzen. Was ich aber nie verstehen werde… sind MS und die Speaker nicht in der Lage ein technisch UND OPTISCH überzeugendes Bild abzuliefern? Warum sieht das immer so WEB 0.01 aus? Ist es wirklich so schwer mit der Technologie auch gut aussehenden Sites zu bauen?\nBilder:\n  \n","date":"2008-11-02","permalink":"https://marcoscheel.de/2008/11/02/pdc-tag-2-session/","tags":["xArchived"],"title":"PDC – Tag 2 – Session"},{"content":"Ich habe schon eine Weile einen Account für die Microsoft Photosynth Software. Mit Hilfe des Tools kann man quasi ein 3D Panorama-Photo erstellen. Nachdem ich mein ein Kaffee geholt hatte, dachte ich mir es wäre ein toller Moment mal ein paar Bilder zu machen und sie gleich in die Photosynth Software zu werfen. Ich habe einfach ein paar (35) Bilder gemacht, ohne irgendwelche Einstellungen an der Kamera vorzunehmen (Canon IXUS 70). Die Software fragt eigentlich nur nach wenigen Metadaten und nach den Bildlern. Per Drag \u0026amp; Drop aus dem Live Photo Ablum eingefügt arbeitet die Software ein wenig und nach wenigen Minuten ist es fertig. Das Ergebnis gibts hier zu sehen:\nhttp://photosynth.net/view.aspx?cid=6677b6de-3bf4-41b9-9d0a-4ef51906aef4\n \n\nCiao Marco\n","date":"2008-11-01","permalink":"https://marcoscheel.de/2008/11/01/photosynth-aus-los-angeles/","tags":["xArchived"],"title":"Photosynth aus Los Angeles"},{"content":" Die Mesh Beta ist nun auch für User mit nicht englischen ReginalSettings geöffnet:\nhttp://www.mesh.com/\nBin mal gespannt, ob meine Erwartungen an den SyncDienst in Mesh erfüllt werden.\nCiao Marco\nUpdate: Hier gibt es detailierte Informationen LiveSide\n","date":"2008-10-30","permalink":"https://marcoscheel.de/2008/10/30/windows-live-mesh-nun-auch-in-deutschland/","tags":["xArchived"],"title":"Windows Live Mesh nun auch in Deutschland"},{"content":"Eben ist die erste Keynote fertig. Windows 7 ist sicher toll, aber die Features die gerade im Touch gehyped werden sind bis auf MultiTouch schon heute auf dem Tablet existent. Das Ah und Oh des Publikum konnte ich nicht nachvollziehen.\n Interessant fand ich die Funktionen von Office Online. Web Editoren für die großen Microsoft Applikationen. Excel, Word, PowerPoint, etc. Die Demourl auf dem alles lief war: http://liteware/_layouts/… also gehe ich mal davon aus, dass auch SharePoint die Funktionen bekommen wird. Die Zeit wird es zeigen. Es nichts was ich zwingen benötige, aber es kann ab und an nützlich sein.\nRichtig interessant wurde es dann mit dem Live Framework. Zugriff auf die Mesh infrastruktur kann richtig viel´Spaß machen. Die Demo’s waren hübsch gemacht. Interessant wird es nöchste Woche… dann wird Live Mesh wohl für alle geöffnet. So hab ich es zumindest interpretiert. Durch Mesh kann ich mit all meinen Clients (PC, Laptop, Windows Mobile, Mac, ..) kommunizieren und zum Beispiel Files sharen.\nDon Box ist jetzt dran und mal wieder ohne PowerPoint :-)\nCiao Marco\n","date":"2008-10-28","permalink":"https://marcoscheel.de/2008/10/28/pdc2008-tag-2-keynote-i/","tags":["xArchived"],"title":"PDC2008 – Tag 2 – Keynote I"},{"content":"Keynote\nDie Keynote hat wirklich Spaß gemacht. Es hatte nicht den Unterhaltungswert wie die Apple-Shows mit Steve Jobs, aber wer hatte schon mit einem “Developers, Developers, …” 2.0 gerechnet :-) Ich dachte der Schwerpunkt würde auf “Windows 7” liegen, aber ich hatte mich geirrt. Ray Ozzie hat das Cloud Operation System “Windows Azure” vorgestellt. Für meinen Interessensschwerpunkt auch viel interessanter als ein neues Desktop OS. Azure bringt endlich Farbe ins Spiel. Bisher ging es im Bereich Cloud Computing nur bei Anbietern wie Amazon (EC2, SimpleDB, …) oder Google (Google Apps, GData, …) voran. Mit dem Service von Microsoft ändert sich das. Es wurde mal wieder alles in einem Topf geworden und mit fehlenden Parts aus dem bestehenden Portfolio ergänzt und mit einigen neuen Tools aufgefüllt. BizTalk.NET hat mit BizTalk an sich nur wenig zu tun, sondern steuert Azure den Enterprise Messaging Bus bei. Teil davon ist auch die Workflow Engine Version 4.0 (WF4). Daten werden in den SQL Services gespeichert. Zukünftig kommen im Datenbereich noch Reporting Services, ETL (also etwas SSIS) und Analysis Services hinzu. Schon heute soll alles durch die beiden OnlineDienste SharePoint und CRM durch Real Life Apps ergänzt werden. “Ganz” neu in der Riege ist die Access Schicht, die Primär aus dem Tool “Geneva” (ehemals “Zermat”) besteht. Geneva hat mich wirklich begeistert und weitere Sessions sind zwingend. Wir selber sind in der Lage ADFS (Active Directory Federation Services) beim Kunden zu implementieren, damit ich meine Credentials außerhalb des Unternehmens in 3rd Party Apps nutzen kann. Die Realität zeigt aber das weder die Administration noch das Application Development so weit ist das wirklich einzusetzen. Mit etwas Zeit und Reife kann Geneva das ändern. Mein Support hat die Idee und ich werde versuchen, einen Case daraus zu generieren. Am Ende hatte ich ein gutes Bild wo die Reise hin gehen soll. Noch ist alles in einem CTP und Microsoft hat alle aufgefordert, aktiv an der weiteren Gestaltung mit zu arbeiten.\nBilder:         \nBB11 – Identity Roadmap for Software + Services Kim Cameron ist einer der Identity Gurus. Er hat schon verschiedene Events zum Thema Security und auch Card Space gemacht. Die Session hat uns die Microsoft Umgebung vorgestellt, die benötigt wird, um in den Cloud Services, aber nicht nur da benötigt wird, um mit Identitäten und Claims zu arbeiten. Die Claims sind das “wichtigste” beim Arbeiten mit den Cloud Diensten. Ein CRM oder SharePoint in der Cloud zu betreiben und dafür die Unternehmenslogindaten nutzen zu können, ist ein kritischer Faktor für den Erfolg. Je mehr Services hinzukommen, desto schimmern, wenn ich für jeden meine extra Zugangsdaten benötige. Die Idee hinter Geneva ist nicht neu. Sie wird nur von Microsoft durch einen Layer dem Developer zugänglich gemacht. Der Geneva Server ist ein STS (Security Token Service). Am Client erweitert Geneva die CardSpace Umgebung. Die Applikations wird durch Geneva mit dem Server und dem Client verbunden. Durch den Server kann ich zum Beispiel mein eigenes AD als STS publishen. Autorisierte “Clients” (Applikationen als auch weitere STS-Server) können dann damit interagieren. Wer nicht seinen eigene STS aufsetzen will, für den bietet Microsoft den Microsoft Federation Gateway an. In dem MS Gateway stehen gleichzeitig alle LiveID’s als Source zur Verfügung. Eingesetzt werden aktuelle “Standards” wie SAML 2.0, WS-Trust und WS-Federation. Die Authentifizierung funktioniert sehr ähnlich dem Keberos Protokoll, allerdings Internetfähig und mit MetaDaten angereichert.\nBilder: \nTL40 - \u0026ldquo;Dublin\u0026rdquo; and .NET Services: Extending On-Premises Applications to the Cloud\nMeine Lunch Session reduzierte meine Pause zum Essen auf 30 Minuten… und ich musste einmal quer über das Gelände. Der Raum war brechend voll. Die Lunch Sessions gehen nur 45 Minuten und der Presenter hatte schon angedeutet, dass es ein anstrengendes Unterfangen wird das Thema da hinein zu packen. So war es dann auch ein “Rushen” durch Slides und Codefragmente. Was geblieben ist? .NET Services waren mal BizTalk.NET. Dublin ist e in Teil der .NET Services und soll eine Evolution des Application Servers aus Basis von IIS/WAS sein. Die Demo zeige die Integration in eine SharePoint Site die durch einen WorkFlow mit dem Enterprise Service Bus (ESB) kommuniziert. Der ESB ist das Herz von Dublin. Technisch bildet es ein Publisher/Subscriber Model ab, welches über Messages interagiert. Vielleicht wäre die Session besser gewesen, wenn man nicht versucht hätte das Big Picture in 45 Minuten durchzuprügeln.\nBilder:     \nBB42 - Identity: \u0026ldquo;Geneva\u0026rdquo; Server and Framework Overview\nZur Session von Kim Cameron sind schon einige Infos aus dieser Session eingeflossen. Ich habe meine ASP.NET 4.0 Ausblick Session sausen lassen, um hier zu sein. Die Präsentation durch Stuart Kwan war es wert. Es hat Spaß gemacht, wie er das Thema vermittelt. Für viele Applikationen, die ich bisher geschrieben habe, gibt es eine Einsatzmöglichkeit für das gezeigte. Das Arbeiten mit Claims sollte einen deutlich höheren Stellenwert für Entwickler haben. Es werden viele Probleme eine Ebene nach oben geschoben und Provider kümmern sich darum, dass die Daten da ankommen, wo sie hin sollen. Es wurde gezeigt wie eine klassische ASP.NET Web Application, die Windows Auth nutzt, per Config, wenigen Zeilen Code und mit Hilfe des Geneva Server (STS) auf Claims umgebaut wurde. Nach der Umstellung ist die Anwendung nicht mehr darauf angewiesen auf einem AD Member Server zu leben. Das Demo hätten wir so in die Cloud schieben können und Geneva und der Server hätten sich um die Authentifizierung gekümmert. Die erzeugten Security Tokens können, wenn die Konfiguration es erlaubt delegiert werden. Der Prozess dahinter funktioniert wie in Kerberos Es ist ebenfalls der Einsatz von Constraint Delegation möglich, welche expliziert die Ressourcen definiert. Geneva ist ebenfalls im CTP Stadium. Microsoft bittet auch hier um Mithilfe und Feedback. Es läuft wohl ein TAP und der RTM für einige Komponenten ist für H2 2009 terminiert.\nMeine Frage nach Hochverfügbarkeit wurde mit dem Verweis auf Work in Progress beantwortet, aber es wurde auf klassische Load Balancing Techniken wie NLB verwiesen.\nBilder:   \nBB01 - A Lap Around the Azure Services Platform\nNette Session mit ein paar Info’s und Tiefen zum Thema. Die Demo hat gezeigt in welchen Szenarien das ganze einsetzbar ist und warum es eine gute Idee ist es so einzusetzen. Faszinierend ist noch immer das der beliebteste Host für WCF etc eine Console Application ist :-)\nBilder:       \nBB15 SQL Server: Database to Data Platform - Road from Server to Devices to the Cloud\nDie Session hat gezeigt, das MS nicht versucht den SQL Server damit abzuschaffen und nur noch eine Cloud DB anzubieten. Es soll das aktuelle Angebot ergänzen. Es wurde aufgezeigt wie die Evolution vom RDBMS zur Database Plattform vollzogen wird. Es ist noch nicht klar, wann es eine “offline” Lösung zum Entwickeln für die Datenbank geben wird. Interessant war zu hören, dass die RDL Files die durch den Report Builder 2.0 erstellt werden, durch Visual Studio 2008 editier bar sind.\nBilder:       \nSSDS Get Together at Westin Bonaventure\nDas Get Together hat zum Glück in meinem Hotel satt gefunden. Die MS SSDS Anwesenden stammen zum großen Teil aus dem aufgelösten WinFS Team ;-) Von dort wurden einige der Techniken übernommen. Ich hatte die Chance mit einem Architekten über eines unserer Projekte zu sprechen. Es wurde deutlich das SSDS deutliche Vorteil zum Beispiel zu ADO.NET Data Services hat. Neben dem kleineren Protokoll und den Features Blobs einzeln ohne den gesamten Datensatz zu holen. Heute Vormittag wurde an alle SSDS Connect User eine Update Mail versandt. Es wurde neue Features vorgestellt, die auch morgen auf der Keynot bekannt gegeben werden. Die Entwicklungen schreiten mit riesen Schritten voran und die Geschwindigkeit soll wohl auch bei behalten werden. Es war toll, das Team zu treffen und mich mit ihnen auszutauschen.\nZusammenfassung\nEin ziemlich cooler Tag. Die Konferenz ist Perfekt organisiert. Vom Shuttelbus, über die Registrierung bis zum Essen. Es ist einfach alles stimmig. Ich hoffe es geht so weiter.\n","date":"2008-10-28","permalink":"https://marcoscheel.de/2008/10/28/pdc-2008-day-1/","tags":["xArchived"],"title":"PDC 2008 – Day 1"},{"content":" Ich sitze in der HALL A und warte auf die Keynote. Ich habe Internet und mein Status den ich über Twitter abgegeben hat ist eben über einen der reisen Screens geflogen. Das nenne ich Interaktion :-)\nNach meinem Flug gestern war ich echt platt. Ich bin nicht 100% gesund und hab deshalb die Zeit genutzt um mich auszuruhe und die Party at Palermo einfach sausen lassen :-S Dafür hat der JetLag mir geholfen heute morgen pünktlich um 5:30 aufzustehen. Meine Halsschmerzen bringen mich nicht um, aber ich hoffe das es bald noch etwas besser wird. Genießen werde ich es so oder so. Ich bringe mich jetzt nochmal auf die Leinwand und dann ist der Laptop aus. Ich denke morgen nehme ich den N800 mit damit kann ich länger online sein :-)\nCiao Marco\n","date":"2008-10-27","permalink":"https://marcoscheel.de/2008/10/27/pdc-2008-starten-in-wenigen-minutes/","tags":["xArchived"],"title":"PDC 2008 starten in wenigen Minutes"},{"content":"Ich bin seit einiger Zeit glücklicher Besitzer eines IBM Lenovo X61 Tablet PC. Meine Einsätze beim Kunden und meine Zeit ohne echten Internetzugang haben sich verstärkt. Im ICE oder aber im Starbucks im Bahnhof oder einfach nur in Standort der Kunden ganz ohne “echtes” Internet (also mit mehr als nur Port 80 und 443) zu sein, hat irgendwann genervt und einfach nur Zeit gekostet. Mein Windows Mobile Handy ist in der Lage meinem Tablet ins Internet zu helfen, allerdings funktioniert das nicht immer und die Variante kosten Handy und Laptop extrem viel Saft… also wieder weniger Zeit dinge zu erledigen. Die Lösung ist UMTS. Es gibt nun zwei Geschmacksrichtungen:\n1. PCMCIA oder USB: Extern angeschlossen und somit anfällig für: Habs vergessen, Habs abgebrochen, … allerdings günstig. 2. Eingbautes PCI Express Modul neben der WLAN Karte  Ich habe mich für die teure Variante entschieden und es bis heute nicht bereut. Jederzeit an jedem Ort online zugehen, solange ich in der “Zivilisation” bin, bringt mir extreme Vorteile. Der Weg dahin war allerdings recht steinig. Folgendes ist zu tun:\n1. Checken, ob das eigene Modell mit dem Modul kompatibel ist. Eine Mail an Lenovo hat hier weiter geholfen und bei mir grünes Licht gegeben. 2. Sicherstellen das eine Antenne vorhanden ist die das UMTS Modul nutzen kann! Ich hatte Glück im Unglück. Mein Gerät hat ein WLAN Modul mit ABGN (allerdings ist der N-Draft disabled und unbrauchbargemacht). Die normalen WLAN Module ohne N haben nur zwei Antennen. Für N-Draft werden drei benötigt… und siehe da, obwohl abgeschaltet hatte ich die dritte Antenne. Das freigemachte Kabel kommt bei mir also als UMTS antenne zum Einsatz :-) 3. Modul beim Dealer der persönlichn Vorliebe bestellen 4. Eine SIM-Karte mit Datentarif am besten “Flat”, wenn so wie bei mir TS, Citrix und Outlook der Primäreinsatz sind. 5. Einen Schraubenzieher, Schrauben! und folgende Lenovo Dokumentation zum Zerlegen des Gerätes: [ThinkPad® X60 Tablet and X61 TabletHardware Maintenance Manual](http://www-307.ibm.com/pc/support/site.wss/document.do?sitestyle=lenovo\u0026amp;lndocid=MIGR-66749)  Ich habe das Gerät geöffnet und nebenbei das nicht angeschlossene Bluetoothkabel (hat der Support bei der letzten Reperatur nicht geschafft anzuschließen) fixiert. Ich habe einfach das Modul mit passenden Schrauben, die nicht mit geliefert werden, festgezogen. Die Antenne vom N-Draft Anschluss an die UMTS Option angeschlossen und fertig. Der Empfang ist ausreichend, wenn er sicher hier und da besser sein könnte. Die Lösung funktioniert super auch unter Vista X64. Zur Steuerung nutze ich den Lenovo Access Manager.\nHier die Info’s zum Modul:\n1. [http://www5.pc.ibm.com/de/products.nsf/$wwwPartNumLookup/_43R1821?open\u0026amp;OpenDocument\u0026amp;epi=web_expressepi](http://www5.pc.ibm.com/de/products.nsf/$wwwPartNumLookup/_43R1821?open\u0026amp;OpenDocument\u0026amp;epi=web_expressepi) 2. [http://lenovo.pcse.de/Pages/Product/Details.aspx?SubSetID=2268\u0026amp;ProductID=263844\u0026amp;ProductCategoryID=FA](http://lenovo.pcse.de/Pages/Product/Details.aspx?SubSetID=2268\u0026amp;ProductID=263844\u0026amp;ProductCategoryID=FA) 3. Bestellnummer: 43R1821  Jetzt noch ein paar Bilder:\n   \nCiao Marco\n","date":"2008-10-27","permalink":"https://marcoscheel.de/2008/10/27/lenovo-x61-tablet-mit-umts-modul-43r18219-nachrsten/","tags":["xArchived"],"title":"Lenovo X61 Tablet mit UMTS Modul (43R18219 nachrüsten"},{"content":"Nach der letzten Installation IU und CU August 2008 hatte ich einen kurzen Schocker, da die Suche nicht mehr wollte. Wir setzen die “Facetted Search” in Version 2.0 ein, um die Suche aufzuwerten. Ich wurde allerdings mit folgenden Fehlern in den Webparts beglückt:\nThe type initializer for \u0026lsquo;Microsoft.Office.Server.EvaluatorModeProvisioning.OfficeServerProductToProvision\u0026rsquo; threw an exception.\nEine kurze Google-Recherche hat mich auf die simple Lösung durch IISRESET gebracht :-) Wow. War das einfach. Bei der Installation und der anschließenden Deinstallation der Facetted Search 2.5 kam der Fehler allerdings wieder und die Lösung brachte diesmal nicht IISRESET, sondern der Trust Level in der Web.Config der betroffenen WebApplication.\nCiao Marco\n","date":"2008-10-27","permalink":"https://marcoscheel.de/2008/10/27/sharepoint-suche-bringt-officeserverproducttoprovision-fehler-nach-infrastructure-update/","tags":["xArchived"],"title":"SharePoint suche bringt “OfficeServerProductToProvision”-Fehler nach Infrastructure Update"},{"content":" Ich lade gerade alles was ich an elektronischem Equipment besitze. Die 11 Stunden Flug wollen genutzt werden. Morgen um 9:30 geht es mit der Lufthansa von FRA nach LAX.\nWenn alles klappt und ich fit genug bin, dann hoffe ich noch folgendes Event am Sonntag mit zu nehmen:  Ich kannte das irgendwie noch von den Berichten der letzten PDC, allerdings kann ich mir noch nicht vorstellen, warum das so IN sein soll. Ich lasse mich überraschen.\nAls ich mich ans Buchen des Hotel gemacht habe, war eigentlich schon alles Gute weg. Zumindest das, was so ganz in der Nähe war. Mit dem Kenntnisstand heute, hätte ich es allerdings nicht besser treffen können. Das “The Westin Bonaventure Hotel and Suites” ist die Location für einige Events nach dem offiziellen Ende. Am Montag treffen sich hier die Gruppe “Anonymen SSDS Nutzer” ;-) Ich bin schon sehr gespannt.\nJetzt aber erst mal die nötige Anzahl an Unterhosen raussuchen :-)\nIch werde versuchen, hier ab und an mal ein Eintrag zu hinterlassen. Für alle die “mehr” wollen, ist hier mein Account bei Twitter: http://twitter.com/marcoscheel. Sollte ich Netz, Lust und Luft haben gibt es kurze Updates.\nCiao Marco\n","date":"2008-10-25","permalink":"https://marcoscheel.de/2008/10/25/pdc-2008-es-geht-los-bald/","tags":["xArchived"],"title":"PDC 2008 es geht los… bald"},{"content":" Ein Traum wird wahr. Seit dem ich mich mit Microsoft Technologie beschäftige und auch Zugriff auf Betasoftware habe, war einer meiner Träume, einmal auf die Primary Professional Developer Conference zu gehen. Letztes Jahr ist es ausgefallen… oder verschoben worden ;-) Aber diese Jahr ist es so weit. Ich fliege nach Los Angeles, CA.\nWir machen gerade ein Projekt in dem es um Cloud Computing geht. Ich hoffe auf viele Infos aus der Basis und einen regen Austausch mit Leidensgenossen ;-) Zumindest auf der Microsoft Seite stehen wir was die Integration in die Cloud angeht noch ganz am Anfang. Das Framework 3.5 SP1 hat einen Anfang gemacht, aber das Ziel ist noch fern.\nIch wollte eigentlich bei Xing eine Gruppe beantragen, aber die wurde abgelehnt. Es gibt keine Gruppen für “Einmal-Events”. Ich hätte halt die Jahreszahl aus dem Gruppennamen nehemn sollen :-)\nEgal: Wer fährt von euch hin? Wann geht’s los? Was sind eure Vorbereitungen?\nIch habe einen gültigen Reisepass… und meine Sessions “geflaggt”. Was ist noch zu tun? Wie bereitet ihr euch vor?\nIch freue mich riesig, dass mein Arbeitgeber mir das ganze ermöglicht hat. DANKE!\nBis dann in LA\nBye Marco\n","date":"2008-10-23","permalink":"https://marcoscheel.de/2008/10/23/microsoft-pdc-2008-endlich/","tags":["xArchived"],"title":"Microsoft PDC 2008 – Endlich"},{"content":"Lektion gelernt! Ich habe bei einem Kunden gestern der “Infrastructure Update” (IU) und die “Cumulative Updates August 2008” (CU) installiert. Auf der Startseite des Unternehmensintranet werden Nachrichten aus allen Bereichen abgebildet. Zur optischen Auffrischung und zur Kategorisierung wird ein Bild für das zugehörige Unternehmen (Unternehmens- oder Markenlogo) abgebildet. Der verwendete ContentType hat ein Feld “Unternehmen” vom Typ “Lookup”. uIm Content Query WebPart (CQWP) lese ich dann die ID des Lookup aus und nutze diese, um auf ein Bild als Source zu verweisen. Für solche Aktionen muss ja wie bei Heather Solomon beschrieben der WebPart mit den entsprechenden CommonViewFields “bestückt” werde.\nNach der Installation der IU und CU habe ich die Startseite geprüft und festgestellt, das die Bilder nicht mehr funktionieren. Hier mein Eintrag aus dem Webpart:\n\u0026lt;span style=\u0026quot;color:#606060;\u0026quot;\u0026gt; 1:\u0026lt;/span\u0026gt; \u0026lt;property name=\u0026lt;span style=\u0026quot;color:#006080;\u0026quot;\u0026gt;\u0026quot;CommonViewFields\u0026quot;\u0026lt;/span\u0026gt; type=\u0026lt;span style=\u0026quot;color:#006080;\u0026quot;\u0026gt;\u0026quot;string\u0026quot;\u0026lt;/span\u0026gt;\u0026gt;Unternehmen,Lookup;\u0026lt;/property\u0026gt;  In der ItemStyle.xsl greife ich dann für die Source über folgenden Variablendefinition zu:\n\u0026lt;span style=\u0026quot;color:#606060;\u0026quot;\u0026gt; 1:\u0026lt;/span\u0026gt; \u0026lt;xsl:variable name=\u0026lt;span style=\u0026quot;color:#006080;\u0026quot;\u0026gt;\u0026quot;UnternehmenId\u0026quot;\u0026lt;/span\u0026gt;\u0026gt; \u0026lt;span style=\u0026quot;color:#606060;\u0026quot;\u0026gt; 2:\u0026lt;/span\u0026gt; \u0026lt;xsl:\u0026lt;span style=\u0026quot;color:#0000ff;\u0026quot;\u0026gt;value\u0026lt;/span\u0026gt;-of select=\u0026lt;span style=\u0026quot;color:#006080;\u0026quot;\u0026gt;\u0026quot;@Unternehmen\u0026quot;\u0026lt;/span\u0026gt; /\u0026gt; \u0026lt;span style=\u0026quot;color:#606060;\u0026quot;\u0026gt; 3:\u0026lt;/span\u0026gt; \u0026lt;/xsl:variable\u0026gt;  Ergebnis vor der Installation war die ID des Lookupvalues. Ich musste etwas googlen und habe dann folgendes Versucht. Ich habe in den CommonViewField einfach den Typ weggelassen:\n\u0026lt;span style=\u0026quot;color:#606060;\u0026quot;\u0026gt; 1:\u0026lt;/span\u0026gt; \u0026lt;property name=\u0026lt;span style=\u0026quot;color:#006080;\u0026quot;\u0026gt;\u0026quot;CommonViewFields\u0026quot;\u0026lt;/span\u0026gt; type=\u0026lt;span style=\u0026quot;color:#006080;\u0026quot;\u0026gt;\u0026quot;string\u0026quot;\u0026lt;/span\u0026gt;\u0026gt;Unternehmen;\u0026lt;/property\u0026gt;  Als Ergebnis habe ich dann ID und Inhalt des Feldes bekommen (Format bei SharePoint ist: ID;#TITELCOLUMNVALUE). Ich habe mein XSL also umgebaut und siehe da es funktioniert. Hier der Code:\n\u0026lt;span style=\u0026quot;color:#606060;\u0026quot;\u0026gt; 1:\u0026lt;/span\u0026gt; \u0026lt;xsl:variable name=\u0026lt;span style=\u0026quot;color:#006080;\u0026quot;\u0026gt;\u0026quot;UnternehmenId\u0026quot;\u0026lt;/span\u0026gt;\u0026gt; \u0026lt;span style=\u0026quot;color:#606060;\u0026quot;\u0026gt; 2:\u0026lt;/span\u0026gt; \u0026lt;xsl:\u0026lt;span style=\u0026quot;color:#0000ff;\u0026quot;\u0026gt;value\u0026lt;/span\u0026gt;-of select=\u0026lt;span style=\u0026quot;color:#006080;\u0026quot;\u0026gt;\u0026quot;substring-before(@Unternehmen, ';')\u0026quot;\u0026lt;/span\u0026gt; /\u0026gt; \u0026lt;span style=\u0026quot;color:#606060;\u0026quot;\u0026gt; 3:\u0026lt;/span\u0026gt; \u0026lt;/xsl:variable\u0026gt;  Wenn man drüber nachdenkt ist das total logisch :-) Mit Typ in den CommonViewFields löst er mir den Inhalt gleich darstellungsfertig auf. Ohne den Typ bekomme ich das was wirklich gespeichert wurde.\nCiao Marco\n","date":"2008-10-23","permalink":"https://marcoscheel.de/2008/10/23/itemstyle-xsl-commonviewfields-und-die-lookup-column/","tags":["xArchived"],"title":"ItemStyle.xsl, CommonViewFields und die Lookup Column"},{"content":"Heute ist der Tag der Windows Firewall und NETSH. Nachdem ich unserem Cloud SQL Flügel gemacht hatte, musste ich mich kurz dem FTP des IIS7 annehmen. Wichtig: Der FTP Server, der unter Features hinzugefügt werden kann ist der IIS6 FTP und kann auch auf dem Windows Server 2008 nicht überzeugen. Erst der Download von IIS.NET bringt den “echten” FTP Server auf die neuste MS Serverplattform. Es geht noch immer um den selben und nun geht es darum sicher FTP Daten auszutauschen. FTP im Standard macht da wenig Spaß. Die heruntergeladene Komponente des IIS kann aber mit SSL (TLS) umgehen. Mit Hilfe diese Funktion kann ich mein Login und den Transfer vor Mithörern schützen. Eine Anleitung zum nackten Einrichten gibt es hier: http://learn.iis.net/page.aspx/263/installing-and-troubleshooting-ftp7/\nJetzt hat man ein FTP auf der Kiste und nun kann man mit gesunden Menschenverstand oder dieser Anleitung die SSL Konfig vornehmen: http://learn.iis.net/page.aspx/304/using-ftp-over-ssl/\nMit der Firewall hat das leider alles immer noch nicht viel zu tun. Ein Connect auf offenem Port 21 bringt nach der Erfolgreichen Anmeldung folgenden Fehler in FilZilla: “Error: Failed to retrieve directory listing”. Im IIS Manager kann man zwar für Passiv-FTP eine Portrange angeben, allerdings ist die Windows Firewall nicht in der Lage, eine Portrange entgegen zu nehmen. Eine Rule wie “21, 5000-6000” schlägt fehl… und all einzeln eingeben? Nö. Wie geht es also weiter? Keine Ahnung, warum MS da so wenig Wert drauf legt, non LAN Szenarien einfach mal zu dokumentieren. Aber irgendwo wird dann auch auf den Blog von Jaro Dunajsky verlinkt: http://blogs.iis.net/jaroslad/archive/2007/09/29/windows-firewall-setup-for-microsoft-ftp-publishing-service-for-iis-7-0.aspx\nFür ein Server, der im Internet steht und FTP SSL sprechen soll, muss also folgendes getan werden:\n1. Download und Installation des FTP7 2. Konfiguration des FTP zum Beispiel mit einem Self-Signed-Cert dank IIS7 Cert-Manager 3. Binding auf einen Port (zum Beispiel 21)   4. Zertifikat zuweisen  5. NETSH Konfiguration netsh advfirewall firewall add rule name=\u0026ldquo;FTP for IIS7\u0026rdquo; service=ftpsvc action=allow protocol=TCP dir=in netsh advfirewall set global Statefulftp disable 6. Für den Zugriff aus .NET heraus ist wichtig: 1. Es muss der Passiv-Mode verwendet werden 2. Es muss kein spezieller Port verwendet werden, er muss nur mit dem des Servers übereinstimmen 3. Die Zertifikate müssen bekannt sein oder ignoriert werden\nJetzt funktioniert’s und ist sogar mit den .NET Bordmitteln (.NET 2.0 oder höher) zu erreichen.\nCiao Marco\n","date":"2008-09-22","permalink":"https://marcoscheel.de/2008/09/22/windows-server-2008-firewall-iis7-und-ftp-mit-ssl/","tags":["xArchived"],"title":"Windows Server 2008 Firewall, IIS7 und FTP mit SSL"},{"content":"Wir entwickeln gerade an einer Anwendung mit Cloud Komponenten. Wichtig… wir sind HIP… oder heißt das heute anders? In der Cloud hat den Vorteil, fast alles über HTTP(S) machen zu können. Das Arbeiten in der Cloud hat aber auch Nachteile… zum Beispiel, das es fast nur HTTP(S) gibt :-) Mir geht es hier nicht um Probleme anderer Leute, alternative Ansätze stehe nicht zur Diskussion… ich erzähle, das hier, weil ich damit gerade ein Problem hatte. Also zu den Details.\nWir haben ein Server im Internet (sucht euch ein Hoster aus und stellt euch da ein Server vor). Dieser Server wird auch zu Entwicklungszwecken genutzt, da wir die Probleme des Web 2.0 von Anfang an benötigen (DNS, Cross-Site-Access, SSL Validation, etc). Der Server hat als Betriebssystem Windows Server 2008. Ein IIS7 bietet die Cloud-Dienste an und ein SQL Server 2008 steht hinter unserer Cloud für das Persistieren der Daten bereit. Mit Hilfe von Astoria (aka ADO.NET Data Services) gibt es Zugriff auf die Daten. Für das Erstellen der Zugriffsschicht (EDMX) benötigt das Visual Studio direkten Zugriff auf die SQL Datenbank. Hier liegt nun das Problem: Für Browsing ins Internet nutzten wir eine schnelle DSL Leitung mit wechselnden IP Adressen. Ich kann dern Windows Server also nicht einfach eine IP Eintragen und fertig. Für den Quickstart (und für den Quasi-Securitytest) wurde der gepatchte Server einfach auf 1433 (SQL Standard TCP Port) geöffnet. Schon nach einem Tag zeige das EventLog massiv viele Einträge über fehlerhafte SA Loginversuche. Slammer lässt also noch 2008 grüßen :-)\nWelche Optionen gibt es in solchen Fällen?\n* Die Clients benötigen eine Statische IP Adresse * Für die Servernetze ist das ok, aber fürs Browsen muss das nicht sein * Den Server ins Clientnetz holen * Einfach aber nicht realitätsnah * Eine VPN Lösung einsetzen * Administrativer Aufwand ist zu hoch und am Client muss man immer schauen das die VPN oben ist * Die dynamische IP Adresse am Server bekannt machen * Bingo… aber wie? Einfach weiter lesen.  Wir mache ich also eine dynamisch DSL IP, die täglich einmal wechselt, auf dem Server bekannt? Für die ersten zwei Tage habe ich unsere IP bei http://www.wasistmeineip.de nachgeschlagen und in der Firewall des SQL Server eingetragen. Funktioniert… aber ist auf dauer langweilig. Jetzt kommen die Tools aus dem Titel zum Einsatz:\n1. [Windows Server 2008 Advanced Firewall](http://www.microsoft.com/windowsserver2008/en/us/security-policy.aspx)  Eine Inbound-Rule anlegen, die 1433 erlaubt. 2. NETSH Eine CommandLine, die in der Lage ist, Rules der Advanced Firewall zu modifizieren. 3. PowerShell Die Script-Umgebung für den Zugriff auf .NET Objekte wir zum Beispiel: System.Net.DNS 4. DynDNS Ein Dienst, der auf vielen Routern genutzt werden kann, um sein “privates” DSL Netz mit einer dynamischen IP im Netz auffindbar zu machen. 5. Windows Task Scheduler 2.0 Kurze Zeit nach dem IP Wechsel (der ja in der Regel immer zur selben Zeit ist) wird das PowerShell los getreten, um die Firewall zu modifizieren.\nUnd so sieht das ganze aus:\n\u0026lt;span style=\u0026quot;color:#606060;\u0026quot;\u0026gt; 1:\u0026lt;/span\u0026gt; ipconfig /flushdns \u0026lt;span style=\u0026quot;color:#606060;\u0026quot;\u0026gt; 2:\u0026lt;/span\u0026gt; $ip = [System.Net.Dns]::GetHostAddresses(\u0026lt;span style=\u0026quot;color:#006080;\u0026quot;\u0026gt;\u0026quot;meindynamischesnetz.dyndns.org\u0026quot;\u0026lt;/span\u0026gt;)[0].ToString() \u0026lt;span style=\u0026quot;color:#606060;\u0026quot;\u0026gt; 3:\u0026lt;/span\u0026gt; $ip = $ip + \u0026lt;span style=\u0026quot;color:#006080;\u0026quot;\u0026gt;\u0026quot;/255.255.255.255,192.168.1.1/255.255.255.255\u0026quot;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#606060;\u0026quot;\u0026gt; 4:\u0026lt;/span\u0026gt; netsh advfirewall firewall set rule name=\u0026lt;span style=\u0026quot;color:#006080;\u0026quot;\u0026gt;\u0026quot;SQL-Server\u0026quot;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#0000ff;\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; remoteip=$ip  1: FLUSHDNS für einen frischen DNS Cache\n2: Ermitteln der ersten IP Adresse des DYNDNS Eintrags\n3: Hinzufügen der IP mit SubNetMask zu weiteren statischen IP Adressen\n4: Die REMOTEIP auf die Rule mit dem Namen “SQL-Server” setzen\nDie Lösung ist beliebig erweiterbar. Zum Beispiel, die aktuelle Konfig auszulesen und die IP zu ersetzen. Für mich hat es so erst mal gereicht… und vielleicht reicht es auch für andere mit einem selben Problem. Wichtig: Die SQL-Verbindung ist da durch nicht geschützt! Eine Lösung hierfür: Encypt=yes\nCiao Marco\nUPDATED: 2008-09-25 – FlushDNS ins Script mit aufgenommen\n","date":"2008-09-22","permalink":"https://marcoscheel.de/2008/09/22/powershell-netsh-windows-server-2008-advanced-firewall-und-dynamische-ip-adresse/","tags":["xArchived"],"title":"PowerShell, NETSH, Windows Server 2008 Advanced Firewall und Dynamische IP Adresse"},{"content":" Ich als Google Reader User komme nicht drum herum, den neuen Stern am Browser Himmel auszuprobieren. Ich gehe nicht davon aus, dass ich meine Standardbrowser (IE7 bald IE8) wechseln werde. Ein Teil meiner Meinung hat Guy Barrette bereits ausgedruckt:\nGuy Barrette - Google Chrome - Random Thoughts\nGoogle braucht eine bessere Browserplattform fur seine WEB 2.0 Applikationen. Generell gibt es die Chance, auch einen großeren Happen am Browsermarkt zu bekommen. Einen neuen Browserkrieg wird es aber denke ich nicht geben. Dem Web Entwickler wird es immer mehr egal unter welchem Browser seine Site lauft. Eine Konkurrenzsituation wie zu Zeiten von Netscape 4.0 wird es nicht mehr geben.\nThomas Bandt ist hell auf begeistert und findet es einfach Geil. Einer meine Kollegen hat schon den Standardbrowser gewechselt (Kopfschuttel). Und ein weitere Kollege ist der Meinung, man konne auch gleich alle personlichen Daten und Kreditkarteninformationen an Google senden.\nIch habe auf eine Applikation gehofft, die mir beim Lesen meiner Feeds mit dem Google Reader hilft. Chrome tut es leider nicht wirklich. Ich nutze den Reader schon seit mehr als einem Jahr unter dem IE. Ich hatte nie Probleme und bin sehr zufrieden mit der Kombination. Der Google Browser ist schneller beim Darstellen und der Applikationsmodus (sieht dann aus wie eine echte Windows Applikation ohne Browserknopfe etc.) ist schon anzusehen, aber leider habe ich Darstellungsfehler. Wie im Screenshot zu sehen, werde ab und an keine Bilder angezeigt. Das RSS Item ist dann schon als gelesen markiert und ein Reload des Browsers funktioniert also nicht. Ich kann die Seite nur in einem neuen Browser im Original (also vom echten Anbieter) laden. Es ist mir im IE noch nie passiert und zum aktuellen Stand ein K.O. Kriterium. Zusatzlich funktionieren die Pen Flicks auf meine Tablet PC nicht :-S Das ist doof aber verschmerzbar.\nIch denke nach der Beta sind die Bugs raus und man hat sich um die Security gekummert. Ich muss zugeben, das das Browser schon ein gewisse Leichtigkeit und Klarheit hat. Es reicht, aber (noch) nicht mich zum Wechsel zu bewegen.\nCiao Marco\n","date":"2008-09-03","permalink":"https://marcoscheel.de/2008/09/03/yacp-yet-another-chrome-post/","tags":["xArchived"],"title":"YACP – Yet Another Chrome Post"},{"content":"Meine Einschätzung hat sich bestätigt: Der Namics SharePoint Blog hat jetzt wohl Feedback von Microsoft zum Thema \u0026ldquo;SQL 2008 BLOB Support\u0026rdquo; in SharePoint bekommen. Den vollen Artikel findet iher hier.\nEs wird allerdings Hoffnung gemacht, das daran gearbeitet wird. Ich für meinen Teil würde lieber andere Features in den neuen Versionen sehen, als so eine Funktion. Ich würde mich auch einfach über noch mehr und schnellere Bugfixes freuen. Zum Beispiel: Der AAM Bug des \u0026ldquo;Infrastructure Updates\u0026rdquo; scheint zumindest behoben, wenn er auch nocht nicht offiziell zum Download steht. Über einen PSS Call zu einem anderen Thema kam der mit rein, sobald ich das gestetet habe gibt es Feedback und bestimmt den offiziellen Download :-)\nCiao Marco\n","date":"2008-09-01","permalink":"https://marcoscheel.de/2008/09/01/kein-sql-2008-filestream-support-fr-sharepoint-noch-nicht/","tags":["xArchived"],"title":"Kein SQL 2008 Filestream Support für SharePoint – noch nicht?"},{"content":"In meinem RSS Reader bin ich über einen Post von Namic gestolpert:\nhttp://sharepoint.namics.com/2008/08/moss_2007_auf_sql_2008_install.html\nEs werden Spekulationen geäußert, dass man mit Hilfe des SQL2K8 die BLOB\u0026rsquo;s also die Dateien in SharePoint auf die Platte auslagern könnte. Ich selber kann mit kein Out-Of-The-Box Funktionalität vorstellen. Es ist sicher wünschenswert, aber da es kein Transparentes Feature für die anwendenede Software (in diesem Fall SharePoint) ist, wären hierfür Codechanges notwendig. Microsoft bietet ja bereits eine Möglichkeit über die External BLOB Storage Funktion. Größtes Problem aus meiner Sichta: Es gibt keine Migration von einem Szenario (BLOB in DB und BLOB on DIsk) ins andere. Mehr Infos auch hier im Blog von Todd Carter. Es wäre schön, wenn MS es tatsächlich implementieren würde. Namics hat bereits einen Request an MS gestellt und ich bin sehr gespannt was das Ergebnis sein wird :-)\nWas aber SQL 2008 für SharePoint schon heute bringt hat Joel Oleson zusammengefasst:\nhttp://blogs.msdn.com/joelo/archive/2008/03/10/top-10-sql-2008-features-answer-why-plan-to-upgrade.aspx\n Ich habe meine ersten Erfahrungen während der Beta Phase gemacht. Es ist ein tolles Produkt, allerdings ändert sich erstmal für SharePoint in den meisten Installationen wenig (es sei denn die Performance ist bei gleicher Hardware wirklich besser). Interssant wird meiner Meinung nach, die Integration der Reporting Services mit den neuen Charts. Hat schon jemand gewechselt und kann für SharePoint da ne Aussage treffen?\nAchso: Nicht falsch verstehen\u0026hellip; ich finde SQL 2008 richtig gut und er ist für mich in jeder neuen SharePoint Installation gesetzt, solange der Kunde dieses Szenario supporten kann.\nCiao Marco\n","date":"2008-08-26","permalink":"https://marcoscheel.de/2008/08/26/microsoft-sql-server-2008-und-was-es-sharepoint-bringt/","tags":["xArchived"],"title":"Microsoft SQL Server 2008 und was es SharePoint bringt"},{"content":"Für die Installation verschiedener Komponenten im SharePoint Umfeld sind spezielle SQL Komponenten notwendig. Für den Project Server zum Beispiel werden folgende Features benötigt: * Microsoft SQL Server 2008 Native Client * Microsoft SQL Server 2005 Backward Compatibility Components * Microsoft SQL Server 2008 Analysis Services 10.0 OLE DB Provider\nWer die Reporting Services in SharePoint integrieren will, benötigt folgendes: * Microsoft SQL Server 2008 Reporting Services Add-in for Microsoft SharePoint Technologies\nFür den SQL Server 2005 gab es dazu ein Feature Pack. Für den SQL Server 2008 gibt es das Ganze nun ebenfalls zum Download:\nhttp://www.microsoft.com/downloads/details.aspx?FamilyId=C6C3E9EF-BA29-4A43-8D69-A2BED18FE73C\u0026amp;displaylang=en\n\nCiao Marco\n","date":"2008-08-26","permalink":"https://marcoscheel.de/2008/08/26/microsoft-sql-server-2008-feature-pack-august-2008-verfgbar/","tags":["xArchived"],"title":"Microsoft SQL Server 2008 Feature Pack, August 2008 verfügbar"},{"content":"Immer wieder habe ich Situationen, in denen ich beim Kunden sitze und zum Beispiel die SharePoint Datenbanken konfigurieren muss. Es kommt ab und an vor, dass ich 5 Minuten (mal mehr, mal weniger) auf das Öffnen des Management Studios warten muss. Der Grund war mir lange unbekannt\u0026hellip; bis eine Google Suche das Ergebnis brachte. Signierte Applikationen basieren wie sollte es anders sein auf Zertifikaten. Ob ein Zertifikat gültig ist oder nicht basiert auf verschieden Dingen. In unserem Fall der Microsoft Applikationen mit dem Start-Problem ist es aber meist der Check der \u0026ldquo;Certificate Revocation List\u0026rdquo;, ob das Zertifikat zurückgerufen wurde. Sollte der Server nicht über eine Internetverbindung zu der Microsoft CRL URL verfügen, dann kommt pro Check ein HTTP Request Timeout (default 30 Sekunden) zum Tragen. Die Wartezeit ist also von der Antwort der MS Server abhängig\u0026hellip; wenn diese überhaupt erreichbar sind.\nEin Workaround ist eine sicher nicht immer zu empfehlende Abschaltung der Funktion in den Internet Explorer Options:\n\nJetzt habe ich einen Microsoft SQL Server 2008 installiert und zur Zeit des Setups hatte er keinen Zugriff auf das Internet und siehe da\u0026hellip; man wird auf genau das Problem hingewiesen :-) Es sind die kleinen Dinge, die einem dem Tag verschönern.\n\nCiao Marco\n","date":"2008-08-26","permalink":"https://marcoscheel.de/2008/08/26/hilfe-mein-sql-management-studio-ist-langsam/","tags":["xArchived"],"title":"Hilfe mein SQL Management Studio ist langsam"},{"content":"Es ist soweit. Am Wochenende bin ich noch daran gescheitert meine SQL 2008 Client Tool zu installieren, weil das VS2008 eben noch nicht den SP1 Stand hat. Es gab es ja auch noch nicht final.\n\nFür heute angekündigt und tatsächlich über die MSDN Abo\u0026rsquo;s ausgeliefert, das fertige SP1 ist da.\n\nEndlich kann es los gehen.Wir haben gerade ein Projekt in der Pipeline, das einige Features aus dem SP1 nutzen soll und zusätzlich auf SQL2008 basiert (zumindest solange nichts besseres auf dem Markt exitiert).\nhttp://msdn.microsoft.com/en-us/subscriptions/default.aspx\nCiao Marco\n","date":"2008-08-11","permalink":"https://marcoscheel.de/2008/08/11/visual-studio-2008-sp1-verfgbar-sql-2008-ich-komme/","tags":["xArchived"],"title":"Visual Studio 2008 SP1 verfügbar – SQL 2008 ich komme"},{"content":" Es ist so weit. Der Microsoft SQL Server 2008 (SQL2K8) ist raus. Die Jungs von IT Service sind mal wieder super fix unterwegs und ich kann mir morgen mein x64 SQL Packet abholen :-)\nAnkundigungen auf:\n1. [http://www.microsoft.com/presspass/press/2008/aug08/08-06SQLServer2008PR.mspx](http://www.microsoft.com/presspass/press/2008/aug08/08-06SQLServer2008PR.mspx) 2. [http://bink.nu/news/microsoft-releases-sql-server-2008.aspx](http://bink.nu/news/microsoft-releases-sql-server-2008.aspx)  Folgende Seite sollte man gesehen haben:\n1. [Features Supported by the Editions of SQL Server 2008](http://msdn.microsoft.com/en-us/library/cc645993(SQL.100).aspx) 2. [How to buy](http://www.microsoft.com/sqlserver/2008/en/us/how-to-buy.aspx) \u0026lt;strike\u0026gt;(Fehlt noch) :-(\u0026lt;/strike\u0026gt; 3. NEW: [Pricing](http://www.microsoft.com/sqlserver/2008/en/us/pricing.aspx)  Es zeigt sich schnell, dass einige interessante Features nur in der Enterprise Edition vorhanden sind.\nCooles Logo, wenn ich das mal anmerken darf :-)\nCiao Marco\n","date":"2008-08-06","permalink":"https://marcoscheel.de/2008/08/06/microsoft-sql-server-2008-rtmrtw/","tags":["xArchived"],"title":"Microsoft SQL Server 2008 RTM/RTW"},{"content":"Eure Meinung ist gefragt!\n Ich bin kurz davor meinen Lenovo X61 Tablet PC neu zu installieren. Ich hatte einen 64-Bit Verusch vor einem Jahr direkt abgebrochen, weil einfach noch nichts da war\u0026hellip; zum Beispiel der Live Writer mit dem ich das hier schreibe. Gestern kam nun ein Softwareupdate fur mein USB2DVI Adapter von DisplayLink. Er ist erstmals auch als 64-bit fur Vista verfugbar. Was nun? Es gibt zwei favorisierte Losungen:\n1. Einfach bei 32-bit bleiben und die paar MB RAM der 4 GB einfach ungenutzt lassen. 2. Auf 64-bit wechseln und den vollen RAM nutzen, dafur aber einige Applikationen nur in VMWare oder Virtual PC betreiben  Jetzt zum Betreff meines Posts. Ist der Developer von heute in der Lage auf 64-bit zu arbeiten? Die Antwort kann nur ja lauten, aber die Details sind das was mich interessiert. Ich habe schmerzen sonst wurde ich einfach posten das ich auf x64 gelandet bin :-) Warum?\n* SharePoint: Mit dem aktuellen Release der [SharePoint Extensions fur Visual Studio 2008](http://www.microsoft.com/downloads/details.aspx?FamilyID=7BF65B28-06E2-4E87-9BAD-086E32185E68\u0026amp;displaylang=en) kann man diese Projekte auch auf [Vista direkt installieren](http://marcoscheel.de/sharepoint/windows-sharepoint-services-3-0-tools-visual-studio-2008-extensions-version-1-2/). Wenn ich schon 3 Monitore betreibe, dann will ich diese auch nutzen, also arbeite ich in meinem Vista fur 90% des Coding. Debugging passiert dann fur Sharepoint zwangslaufig in einer VM. * Tools: Print to OneNote, Office Scan Tool, ... alles Kleinigkeiten, die den Alltag als Entwickler erleichtern * MS Beta's: Leider kommen auch immer wieder MS Beta Produkte (wie damals der Live Writer) erst in der 32-Bit Version.  Keine Lange Liste aber gerade der SharePoint Punkt trifft mich hart. Eigentlich ist das entwickeln fur und mit 64.-bit als .NET Coder recht einfach, wenn da nicht die Details waren.\nAuf der Serverwelt sind wir heute in der x64 Welt angekommen: Exchange 2007. SharePoint kommt mit der Version 14 (13 gibt es nicht) als \u0026ldquo;x64 only\u0026rdquo;.\nJetzt meine Frage:\n1. Wer von euch entwickelt auf x64? 2. Was sind die Probleme? 3. Wie kann man diese umgehen? 4. Warum ist x64 fur euch ein Tabu?  Über eure Gedanken ware ich glucklich. Als Comment oder Trackback in eurem eigenen Blog :-)\nWeitere Gedanken aus dem Web:\n1. Paul Thurrott von WinSuperSite: [Comment on: Suddenly, 64-bit Windows is mainstream](http://community.winsupersite.com/blogs/paul/archive/2008/07/31/suddenly-64-bit-windows-is-mainstream.aspx) 2. Ed Bott von ZDNet: [Suddenly, 64-bit Windows is mainstream](http://blogs.zdnet.com/Bott/?p=506)  Ciao Marco\n","date":"2008-08-06","permalink":"https://marcoscheel.de/2008/08/06/64-bit-ist-die-developer-welt-schon-so-weit/","tags":["xArchived"],"title":"64-bit? Ist die \"Developer-Welt\" schon so weit?"},{"content":"Zuruck aus dem Urlaub (hab also noch nicht mit dem Bloggen aufgehort) offne ich den IE und einer meiner Default-Tabs ist neben Google Reader meine Site bei delicious. Es gab ein Layout Update. Die Seite hat bisher durch ihrer Einfachheit geglanzt. Mit dem Update wurde die Optik aufgebessert und ich bin hin und weg :-) Auf Wikipedia finde ich eine passende Umschreibung:\n   _**Delicious**_ is a term meaning \"highly pleasing or delightful\", normally in the context of [flavour](http://en.wikipedia.org/wiki/Flavour).    Quelle: Wikipedia\n\nCiao Marco\n","date":"2008-08-03","permalink":"https://marcoscheel.de/2008/08/03/wow-delicious-updated/","tags":["xArchived"],"title":"WOW – delicious Updated"},{"content":"\ne-office - Home\nvia Ton Stegeman\nCiao Marco\n","date":"2008-07-23","permalink":"https://marcoscheel.de/2008/07/23/ocs-presence-screensaver/","tags":["xArchived"],"title":"OCS Presence Screensaver"},{"content":"Nachdem ich tatsachlich einige Comments fur mein \u0026ldquo;Unit-Testing\u0026quot;-Beitrag bekommen habe, habe ich eine Funktion aktiviert, die ich besonders schon finde:\n\nBei einem Reply auf ein Comment gibt es eine Mail an den ursprunglichen Autor des Comments solange dieser seine Mailadresse (die nicht dargestellt wird) angegeben hat. Download der GraffitiExtras gibt es auf diesem CodePlex Projekt: GraffitiExtras.\n\nCiao marco\n","date":"2008-07-14","permalink":"https://marcoscheel.de/2008/07/14/graffiticms-comment-reply-notifier-aktiviert/","tags":["xArchived"],"title":"GraffitiCMS – Comment Reply Notifier aktiviert"},{"content":"Eben in dem Blog von Keyvan Nayyeri gelesen:\n   ##### **Unit Testing is Not Always Good**     You may be interested to know that how much of unit testing is done for Waegis. Actually I started the development with heavy unit tests for almost everything. Waegis has a core platform that I tried to develop with the best quality and accuracy so I unit tested it carefully. After 1-1.5 months of development I abruptly believed that this project is going to take much longer than my initial estimation. Normally taking longer wasn't bad but for a self-managed project this could cause to failures so I had to make a new decision. My decision was to limit my unit tests to some critical parts of code. Based on experience and background these parts are easy to recognize so I chose this approach and this helped me to speed up my development and bring it to the web finally! Unit testing is great and I'm a big fan of it but this time I had to reduce and limit it in order to get the point. All the proven methodologies, software development processes and practices are there to teach you how to choose the best way so don't restrict yourself with them! [more...](http://nayyeri.net/blog/lessons-that-i-learned-from-waegis/)     Ich lese den Blog von Keyvan schon eine Weile. Er ist nicht nur Autor von einigen Buchern im .NET Developmentbereich, sondern auch ein Tool-Developer fur verschieden Software. Ich selber nutze seine GraffitiCMS Extras auf diesem Blog.\nKeyvan schreibt in seinem Post uber seine Erfahrung im konsequenten Einsatz mit Unit-Testing. Er spricht mir aus der Seele. Es es wichtig es zu tun, aber es ist noch wichtiger es zu unterlassen, wenn es sinnvoll und vertretbar ist. Wie beim Designen von Datenbanken, kommt es nicht auf die tausendste Table an, um die vierte Normalvorm zu erreichen. Es ist wichtiger das Ziel vor Augen zu haben und dieses mit vertretbaren Mitteln zu erreichen.\nIch habe zu \u0026ldquo;.NET 1.1 Zeiten\u0026rdquo; mit nUnit und Plugins fur Visual Studio begonnen. Beim Wechsel auf die Visual Studio Version 2005 war dann schon Unit-Testing direkt mit an Board. Seit der Version 2008 konnen auch Nutzer, der nicht \u0026ldquo;Team Editionen\u0026rdquo; auf die eingebauten Funktionen zuruckgreifen. Es wurde also erkannt, das es ein wichtiges Feature ist, dass nicht zwingend an der Große und Professionalitat der Entwicklung gebunden ist.\nWofur nutzt man also Unit-Testing? Meine Antwort gleicht der von Keyvan. Die Core Elemente sollten im permanenten Testing stehen. Am \u0026ldquo;Core\u0026rdquo; sollte ab einem gewissen Zeitpunkt so oder so keine gravierenden Änderungen vorgenommen werden. Somit hat man je nach Projektgroße eine uberschaubare Anzahl an Test-Items. Fur diese Falle rentiert es sich auch die entsprechenden Aufwande zu betreiben, die Umgebung auf Unittesting vorzubereiten. Die Ausgangssituation solle idealerweise immer dieselbe sein, aber gerade im Fall von Datenbank getriebenen System nicht gerade trivial zu erreichen. Ein weiteres Szenario in dem sich Unit-Testing anbietet, ist das fruhe Stadium eine Applikation. Wann wird zum Beispiel das erste Mal der Datenlayer beansprucht? Wie testet man die Login-Methode von die ASP.NET Front noch nicht steht? In solchen Fallen behilft sich der Entwickler in der Regel mit einer WinForm Applikation: WindowsFormsApplication1. Nach drei Tagen hat der Knopf \u0026ldquo;Button 1\u0026rdquo; die sechste Funktion und ist von vier weiteren Knopfen mit ahnlichem Schicksal umgeben. Hoffentlich wird dann nicht im Team entwickelt. Chaos ist vorprogrammiert. An dieser Stelle einfach Unit-Testing angewandt and und dann einfach in Ruhe lassen. Das ist kein Code der gewartet werden muss. Bei Konflikten im Refactoring lieber auf eine Migration verzichten.\nIntensives Unit-Testing wird gerade in \u0026ldquo;schnellen\u0026rdquo; (kurzen wie auch schnell wechselnde Anforderungen) Projekten eher zum Hindernis. Bei einem kleinen Projekt auf einen Change zu reagieren, kann nicht (darf nicht) bedeuten tausend Zeilen Code furs Testing anpassen zu mussen. Bei einer vernunftigen Anzahl an Szenarien pro Test sind schnell mehr Zeilen Code im Unit-Test als in der getesteten Klasse gezahlt. Die Einfachheit einen Test fur eine Klasse mit den mitgelieferten Tools zu erzeugen, ist als trugerisch.\nFur was nutzt Unit-Testing? Und wie intensiv?\nCiao Marco\n","date":"2008-07-13","permalink":"https://marcoscheel.de/2008/07/13/unit-testing-und-die-projektanforderungen/","tags":["xArchived"],"title":"Unit-Testing und die Projektanforderungen"},{"content":"Ich hatte meine Erfahrungen bereits geschildert: Windows Server 2008 und der Default Zustand der Firewall\nLese eben meine Feeds und habe folgenden Artikel gefunden: SQL Server Security: SQL Server and the Windows Server 2008 Firewall\nvia Michael Howard\u0026rsquo;s\nCiao Marco\n","date":"2008-07-06","permalink":"https://marcoscheel.de/2008/07/06/nochmal-default-zustand-der-windows-server-2008-firewall/","tags":["xArchived"],"title":"Nochmal: Default Zustand der Windows Server 2008 Firewall"},{"content":"Eben in dem Blog von Benjamin Wegner gefunden:\nOnlineblog - Sharepoint und Riesentorte\nLeider alles nicht so witzig, wie es sich anhort. Die Installation alleine hilft nicht, die Webparts mussen entsprechend angepaßt werden. Was nicht richtig durch kommt, die Personensuche ist ebenfalls betroffen. Namen wie \u0026ldquo;Mustermann\u0026rdquo; wurden ebenfalls nicht als Treffer auftauchen.\n Ich hab einen der großeren Kunden betreut, der noch immer von dem Problem betroffen ist. Den Hotfix, den wir damals getestet haben (der hier nun offentlich ist), hat zwar das Problem gelost, an sich handelt es sich aber lediglich um einen Workaround. Die Losung ware, den Wordbreakter entsprechend zu korrigieren.\nIn den eigenen API Calls im C# Code muss genau auf so etwas geachtet werden, da es wirklich nur ein Workaround ist der an der Oberfache greift.\nVon uns kamen ubrigens die letzten beiden Worter\u0026hellip; fur mich sind es die \u0026ldquo;Unworter fur 2007 und 2008\u0026rdquo; geworden :-S\nMS KB Artikel 950439: No results are returned when you search for German compound words or for German compound names in SharePoint Server 2007\nCiao Marco\n","date":"2008-07-04","permalink":"https://marcoscheel.de/2008/07/04/kb950439-problem-bei-sharepoint-suche-mit-zusammengesetzten-deutschen-worten/","tags":["xArchived"],"title":"KB950439: Problem bei SharePoint Suche mit zusammengesetzten deutschen Worten"},{"content":"Heute auf Bink.neu gelesen und schon wieder ein Problem weniger. Ich war von folgendem Bug betroffen:\nThis update improves the reliability of Windows Vista SP1-based computers that experience issues in which large applications cannot run after the computer is turned on for extended periods of time. For example, when you try to start Excel 2007 after the computer is turned on for extended periods of time, a user may receive an error message that resembles the following:\nEXCEL.EXE is not a valid Win32 application\nInstalliert udn los gehts.\n\nQuelle: A reliability and performance update is available for Windows Vista SP1-based computers\nCiao Marco\n","date":"2008-07-04","permalink":"https://marcoscheel.de/2008/07/04/a-reliability-and-performance-update-is-available-for-windows-vista-sp1-based-computers/","tags":["xArchived"],"title":"A reliability and performance update is available for Windows Vista SP1-based computers"},{"content":"Ich habe eben versucht, auf dem Microsoft Demo Virtual PC (WSSv3 + SP1) eine SiteDefinition zu deployen. Es gab leider nur folgende Fehlermeldung:\n\u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 1:\u0026lt;/span\u0026gt; Error 1 Unable to generate a temporary class (result=1). \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 2:\u0026lt;/span\u0026gt; error CS0016: Could not write to output file 'c:Documents and SettingsAdministratorLocal SettingsTempxyqxz-fg.dll' -- 'Could not execute CVTRES.EXE.' \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 3:\u0026lt;/span\u0026gt; 0 0  Kurze Suche in Google brachte folgendes:\nhttp://forums.msdn.microsoft.com/en-US/msbuild/thread/b93ea3ff-8aa1-4893-a6bf-c4c064633528/\nDie folgenden Commands haben dann tatsachlich die LKosung gebracht:\ndevenv -resetsettings\ndevenv -resetskippkgs\ndevenv -safemode\nCiao Marco\n","date":"2008-06-30","permalink":"https://marcoscheel.de/2008/06/30/fehler-beim-deploy-mit-hilfe-der-vsewss-v-1-2-cvtres-exe/","tags":["xArchived"],"title":"Fehler beim Deploy mit Hilfe der VSeWSS V.1.2 – CVTRES.EXE"},{"content":"Ich hatte ja schon etwas uber die Zusammenarbeit von beiden Produkten geschrieben. Der grosse Installationspost der beiden Tools ist noch in der Mache :-) Vorab folgende Info, die mich wieder etwas Zeit gekostet hat.\nIch wollte eine Report Model aus dem Visual Studio 2008 in den SharePoint deployen. Visual Studio hat den Deploy aber ur mit folgender Meldung quitiert: \nDie Losung liegt in der Konfiguration der Deployment Pfade: \nNachzulesen ist das ganze hier in der MSDN: http://msdn.microsoft.com/en-us/library/bb283155(SQL.100).aspx\nCiao Marco\n","date":"2008-06-24","permalink":"https://marcoscheel.de/2008/06/24/deploy-report-model-microsoft-sql-2008-im-sharepoint-integration-mode/","tags":["xArchived"],"title":"Deploy Report Model (Microsoft SQL 2008) im SharePoint Integration Mode"},{"content":"In einer der Kundenumgebungen lauft neben der SharePoint Farm auch ein Microsoft Office Project Server 2007 (MOPS). Wir hatten in der Vergangenheit schon Proleme mit dem Einrichten der OLAP Cubes. Es fuhrte bis zu einem MS Support Case, der anschließend dafur sorgte, dass die MSDN Dokumentation entsprechend aktualisiert wurde. Leider ist es nicht der letzte Fehler in diesem Umfeld. Ich habe jetzt eine Anfrage bekommen, weil die Datenanalyse nicht funktioniert. Das ActiveX Control hinter dem Navigationspunkt ist ein Office Web Components (OWC) Element. Dieses Element baut im Hintergrund eine OLEDB Verbindung zum Microsoft SQL 2005 - Analaysis Sever (SSAS) auf. Der Kunde hatte nun das klassisch IT Probelm. Auf einem Rechner geht es noch und auf einem anderen Rechner funktioniert es nicht mehr. Es hat naturlich irgendwann mal auf allen Clients funktioniert und es wurde naturlich nichts verandert. Eine kurze Google Suche zur Fehlermeldung \u0026ldquo;Cannot connect to the server . The server is either not startet or too busy.\u0026rdquo; brachte mich auf die Spur der OLEDB Client Version fur SSAS. Es wurde schnell klar, dass die Fehlermeldung nicht unklarer sein konnte. Ich konnte mich auf zwei Citrix Server aufschalten, die jeweils einmal funktioniert und einmal nicht funktioniert. Offensichtlicher Unterschied: Die Version des \u0026ldquo;Microsoft OLEDB Provider for Analysis Services X.0\u0026rdquo;.\nDer Server mit der fehlerhaften Meldung hatte nur den OLEDB Treiber fur Version 8.0 ( = Microsoft SQL Server 2000):  \nDer Server mit der erfolgreichen Meldung hatte den OLEDB Treiber fur Version 9.0 ( = Microsoft SQL Server 2005):\n \nDie Installation des Treiber aus dem Microsoft SQL 2005 Feature Pack Feb 2007 (fur SP2) hat die notige Losung gebracht. http://www.microsoft.com/downloads/details.aspx?FamilyID=50b97994-8453-4998-8226-fa42ec403d17\u0026amp;displaylang=en\nCiao Marco\n","date":"2008-06-24","permalink":"https://marcoscheel.de/2008/06/24/project-server-datenanalyse-activex-zeigt-olap-fehler/","tags":["xArchived"],"title":"Project Server – Datenanalyse ActiveX zeigt OLAP Fehler"},{"content":"In meinem Vorgangerpost habe ich ja kurz beschrieben, was fur Hurden das Add-In des Report Servers (MS SQL 2008) aufstellt und wie man druber kommt. Grundsatzlich bleibt anzumerken, dass der Microsoft SQL Server 2008 ohne echten IIS auskommt. Seit 2005 konnte er so schon WebServices direkt ohne WebServer anbieten. Allerdings wurde fur die Reporting Services (SSRS) in der Version 2005 noch ein echter IIS benotigt. Mit dem aktuellen SQL 2008 CTP Feb ist dies nicht mehr notig. Der SSRS lauft komplett im SQL uber HTTP.SYS.\nDieser Post widmet sich kurz der Installation der Serverseite des SSRS. Es gibt namlich ein paar Änderungen\u0026hellip; sehr grundsatzliche Änderungen:\n   _\"To join a report server to a SharePoint farm, the report server must be installed on a computer that has an instance of a SharePoint product or technology. You can install the report server before or after installing the SharePoint product or technology instance.\" _BOL: Requirements for Running Reporting Services in SharePoint Integrated Mode [http://msdn.microsoft.com/en-us/library/bb283190(SQL.100).aspx](http://msdn.microsoft.com/en-us/library/bb283190(SQL.100).aspx)    Verstanden? Genau der Server, der den Report Server (WebService) anbietet, muss in die Sharepoint Farm! Hinweise dazu gibt es auch hier: Planning a Deployment Mode (Reporting Services) Deployment Topologies for Reporting Services in SharePoint Integrated Mode\nVorteile sind eine bessere und tiefere Integration (z.B. bessere Webparts), aber was sind weitere Folge?\n1. Auf dem SQL Server muss eine Sharepoint Server installiert werden 1. Extra Server der gepatched werden muss (HotFixes, Rollups, ...) 2. Extra Lizenzkosten fur einen SharePoint Server (nicht so schlimm) 3. IIS-Komponenten (nicht nur HTTP.sys) auf dem SQL 2. oder auf einem SharePoint Server der Farm (z.b. Maschine mit der Rolle: Indexer, ExcelServices, ...) 1. Wartung ist nicht so aufwendig wie anders herum (siehe 1.) aber muss gemacht werden 2. Extra Lizenzkosten und diese konnen extrem hoch werden, da in der Regel nach CPU lizensiert wird. Je nach Edition kommen hier schnell hohe 5 stellige Euro-Betrage zustande.  Ich habe mich entschieden fur meine Testumgebung, die Installation auf dem SQL Server zu machen. Ich habe alle Services \u0026ldquo;disabled\u0026rdquo; und so kaum einen \u0026ldquo;Overhead\u0026rdquo;. Allerdings muss ich den IIS auf dem Server installieren, weil das MOSS erzwingt.\n \nOhne SharePoint kommt zu Beispiel:\n\nNachster Fehler: \u0026ldquo;Report Server may not have joined the SharePoint farm, or Report Server service account may not have been granted access to farm.\u0026rdquo; Eine Losung habe ich noch nicht. Nur ein Workaround. Ich versuche, die ganze Umgebung mit Kerberos als Authentifizierung zu fahren. Im Windows Server 2008 hat sich hier ein wenig das Modell hinter dem Process Model geandert. Im Moment nur soviel: \u0026ldquo;KernelMode Authentication\u0026rdquo;. Ich lasse nun den ReportServer also nicht unter einem Domain Account laufen, sondern unter LocalSystem. Anders war es leider nicht zu realisieren. Im Moment ist die Umgebung mit W2K8 und SQL2K8 und dann noch SharePoint mehr als \u0026ldquo;unbekannt\u0026rdquo;\u0026hellip; und das geht nicht nur mir so\u0026hellip; Microsoft arbeitet ja auch noch am neusten SQL Server also ist im Moment noch sehr unklar, wo genau der Fehler liegt. Sharepoint an sich funktioniert ja sehr gut, auch mit Kerberos.\nBei weiteren Problemen mit der Authentifizierung sollte man mal in die Microsoft Books Online (BOL) fur den SQL 2008 rein schauen. Thema: Trouble Shot Authentication Report Server use Build in Account with Service type HOST \n**Fazit **Noch ist die Installation nicht ganz rund. Der RC0 ist glaube ich vor kurzem raus gegangen. Ich weiß nicht, ob ich die Zeit finde vor dem RTM mich naher damit zu beschaftigen, aber ich denke wir sind auf dem besten Weg. Die Architekturanderungen fur die Farm Integration findet ich etwas unglucklich, aber die Kundenakzeptanz wird zeigen, wie lange diese Lizenzauswirkungen bestand haben :-)\nCiao Marco\n","date":"2008-06-24","permalink":"https://marcoscheel.de/2008/06/24/install-microsoft-sql-server-2008-reporting-services-in-sharepoint-integration-mode-on-windows-server-2008/","tags":["xArchived"],"title":"Install Microsoft SQL Server 2008 Reporting Services in SharePoint Integration Mode on Windows Server 2008"},{"content":"Version 0.1 - 2008-05-07 Version 0.2 - 2008-06-24 Version 0.3 - 2009-06-24 (SP2, CU, IIS7, SQL2K8)\nVorbedingungen:\n* Betriebssystem und alles weitere Software ist in der Hauptsprache Englisch * OS und Software ist 64-Bit * Alle Server haben min. 4 GB RAM * Der Datenbankserver hat 8 GB RAM und min. 2 CPU's mit 2 Cores * Min. 2 Partitionen mit zwei getrennten Plattensystemen * PowerShell auf allen Sharepoint Servern für Scripting  Über die Punkte oben kann man streiten, aber das Sizing ist für mich min. wenn ein Kunde ein Farm mit zwei SharePoint und einem DB Server aufbaut. Die Hardware ist so oder so 64-bit fähig und 8 GB RAM sind heute günstig zu erwerben. Der Erfolg einer SharePoint Anwendung steht und fällt mit der Performance des SQL Servers. Also sind das meine Wunschrahmenparameter. Beim SQL mache ich nur Zugeständnisse, was die Platten angeht. Für alle Datentypen (SystemDB\u0026rsquo;s, tempdb, Datafiles und Logfiles) eigene getrennte Plattensystem zu haben, ist wünschenswert, aber das kann heute gerade bei großen Datenmengen teuer werden, besonders, da ich bisher auch nur wenige schnelle SAN Implementierungen gesehen habe :-)\n\n1. Deaktivierung der Virenscanner für die Installation = Beschleunigung des Setup (besonders wichtig beim Ausführen von PSCONFIG) 2. IIS Default Site stoppen und IIS Logsettings (Pfad und Details) konfigurieren 3. Installation \u0026lt;strike\u0026gt;MS SQL Server 2005 SP2\u0026lt;/strike\u0026gt; MS SQL Server 2008 SP1 1. Konfiguration der Default Datenbankpfade auf die entsprechenden Laufwerke 2. Einstellung für ein sinnvolle Defaultgröße (min. 100 MB Daten und 50 MB Log) 3. Wachstumsraten anpassen (min. 100 MB Daten und 50 MB Log) 4. Die Größen richten Sich nach dem geschätzten Volumen, je mehr man ausgeben darf desto besser. Ich habe auch schon Konfigurationen mit 20 GB (ja GB) Initialgröße gemacht, so verhindert man die Fragmentation auf der Platte und erhöht die Wahrscheinlichkeit, die Anwendung schnell zu halten 4. Installation MOSS 2007 mit SP1 5. Installation Language Packs falls notwendig 1. Install WSS Language Pack (e.g. x64, German) 2. Install MOSS Language Pack (e.g. x64, German) 3. \u0026lt;strike\u0026gt;Installation WSS/MOSS SP1 Language Packs \u0026lt;/strike\u0026gt; 6. \u0026lt;strike\u0026gt;Installation Post SP1 Rollups (\u0026lt;/strike\u0026gt;[\u0026lt;strike\u0026gt;http://www.harbar.net/articles/postsp1.aspx\u0026lt;/strike\u0026gt;](http://www.harbar.net/articles/postsp1.aspx)\u0026lt;strike\u0026gt;)  Installation SP2 (siehe auch mein SP2 Artikel) 1. WSS SP2 + WSS Language Pack SP2 2. MOSS SP2 + MOSS Language Pack SP2 7. Installation des letzten Cumulative Update (CU) 8. IIS Logverzichnis von Laufwerk C um konfigurieren auf ein anderes Laufwerk 9. (Optional) Für weitere Server in der Farm alle Schritte bis hier her wiederholen 10. DNS einrichten 1. Portal: portal.domain.local 2. MySite: my.domain.local 3. SSP: moss-prod-ssp.domain.local 4. Admin: moss-prod-admin.domain.local 11. Einrichten der Service Accounts 1. Farm: svcMossProd 2. SQL: svcMossProdSql 3. Crawl: svcMossProdCrawl 4. Content Access: svcMossProdContentAccess (Achtung: Innerhalb von SharePoint mur dann der Netbiosname verwendet werden, also die ersten 20 Zeichen = svcMossProdContentAc) 5. SSP: svcMossProdSsp 6. MySite + Portal: svcMossProdApp 12. Einrichten der Security Gruppen 1. Farm Admins: sec-MossProdFarmAdmin Alle User die Sharepoint administrieren sollen 2. Service Accounts: sec-MossProdSvcAccounts Alle Service Accounts svcMossProd* 13. Konfiguration der DCOM Config für alle AppPool und Service Accounts (sec-MossProdSvcAccounts) mit \u0026ldquo;Local Launch\u0026rdquo; und \u0026ldquo;Local Activation\u0026rdquo; für \u0026ldquo;IIS WAMREG admin service\u0026rdquo; und \u0026ldquo;OSearch\u0026rdquo; auf allen Servern (ohne SQL) 14. (Optional) Service Principal Names (SPN) und Delegation pflegen 1. Entscheidung für oder gegen IIS7 Kernelmode Authentication (Wenn NLB für die WFE geplant ist, dann auf jeden Fall gegen Kernelmode AUth entscheiden) 2. Setzen der SPN’s auf die Maschine(n) oder die Service Accounts (Kernelmode Auth = Off) 15. Erstellen einer Farm auf dem Applikationsserver (Crawl, Central Admin) 16. Joinen der Server (1 WebFrontEnd Server oder zwei bei Load Balancing bevorzugt Windows NLB) 17. Konfiguration der Rollen (WFE = WebApplication, Query / APP = CentralAmin, Index) 18. Konfiguration des Shared Service Provider 1. Anlegen einer eigene Applikation für den SSP (moss-ssp.domain.local) 2. Anlegen einer eigene Applikation für die MySite (my.domain.local) 3. Alle Applikationen bekommen einen eigene extra Domain User als AppPool Account (falls wir Kerberos machen wollen) 4. Alle Anwendungen werden mit HostHeader angelegt (unterstützt den Einsatz von Kerberos und vermeidet kryptische URL\u0026rsquo;s mit Port am Ende) 19. Alle Datenbanken, die angelegt werden, bekommen eindeutige Namen. Ausnahmen sind ConfigDB (bleibt beim Standard) und ContentDB der Central Administration (weil -\u0026gt; keine Relation zu Aufwand und Nutzen). In der Regel Kennzeichnung des Systembetriebs im Namen (MOSS_PROD_Portal_00). 20. Konfiguration des Operations Tab. Wichtige Optionen: 1. \u0026ldquo;Diagnostic logging\u0026rdquo;: Pfad vom Standard auf C umbiegen auf ein anderes Laufwerk. Wichtig: Dieses Setting wird in der Farm genutzt und alle beteiligten Server die SharePoint Software installiert haben, müssen über das Laufwerk und den Ordner verfügen. Es kann sonst zu unverständlichen Fehlern wie \u0026ldquo;Access Denied\u0026rdquo; kommen. 2. \u0026ldquo;Update farm administrator\u0026rsquo;s group\u0026rdquo;: Farm Administrators durch eine AD Gruppe für die MOSS Administration erweitern. Diese Gruppe verwenden wir auch weiter, da der Setup Account sonst der einzige ist der alle Rechte hat und die Pflege so sehr umständlich wird. 21. Konfiguration des Application Management Tabs. Wichtige Optionen: 1. \u0026ldquo;Create or extend Web application\u0026rdquo;: Erstellen er Anwendung für die Portal Applikation. Auch hier HostHeader für die URL, eigener AppPool Account (Alternative: Pool der MySite mitbenutzen), Pfad auf anderes Verzeichnis als C setzen, DB Namen vergeben. 2. \u0026ldquo;Web application general settings\u0026rdquo;: Einstellen der default Zeitzone für MySite und Portal 3. \u0026ldquo;Content databases\u0026rdquo;: Bei Anwendungen mit hohen userzahlen und vielen SiteCollection (MySite -\u0026gt; User = SiteCollection) erzeuge ich eine sinnvolle Anzahl an Datenbanken. Beim Anlegen von SiteCollections kannverteilt sich so die Datenmenge auf x Datenbanken was für Backup- und Restore Operatione gut ist. Es kann auch sinnvoll sein, dass wir die Portal CM S SiteCollection in eine eigene Content DB legen, diese dann offline nehmen, damit wir in der Datenbank nur eine SiteCollection haben. Der Rest an SiteCollections verteilt sich dann entsprechend. 4. \u0026ldquo;Self-service site management\u0026rdquo;: Für MySite einschalten 5. \u0026ldquo;Policy for Web application\u0026rdquo;: An dieser Stelle mache ich es so, dass ich auf alle Applications die Farm Administrator Group (sec-MossProdFarmAdmins) aus de AD mit Full Control berechtige. Es sollten sowieso nur hoch previligierte Accounts enthalten sein. 22. Konfiguration des Shared Service Provider. Wichtige Punkte: 1. \u0026ldquo;Site Settings - Advanced permissions\u0026rdquo;: Farm Admin AD Gruppe mit Full Control berechtigen (ist zwar per Policy auch schon drin, aber der Vollständigkeit halber sollte das pasieren) 2. \u0026ldquo;Personalization services permissions\u0026rdquo;: Farm Admin AD Gruppe auf alle Optionen berechtigen (ist bisher nicht durch Policy abgedeckt) 3. \u0026ldquo;User profiles and properties\u0026rdquo;: Import und Schedule konfigurieren 4. \u0026ldquo;Search settings\u0026rdquo;: Schedule konfigurieren 23. SQL Datenbanksettings 1. Check der MOSS Datenbanken auf: Initialgrösse, Wachstumsraten, Recovery Model (Simple oder Full) 2. Backup der Datenbanken. Wichtig: Bei Recovery Full müssen die Logs gebackuped werden, sonst läuft die Platte voll 3. Wartungsplan: Index Rebuild/Reorg nach MS Whitepapaer und Anforderungen 24. Backupscripte für IIS Metabase Schedulen 25. 12 Hive - BIN in den Enviroment Variables in den Pfad aufnehmen 26. STSADM PowerShell Kombi für Speichern der Ergebnisse von STSADM - o enumsites -URL \u0026lt;portal + mysite url\u0026gt; für Restore und Reportingzwecke 27. Screenshots nicht vergessen :-) Zur Orientierung: Bei mir wird es in der Regel ein Dokument mit ca. 150 Screenshots 28. (Optional) Bierchen trinken, Frau anrufen, dass es später geworden ist und tief Luft holen\nCiao Marco\n","date":"2008-06-24","permalink":"https://marcoscheel.de/2008/06/24/wie-installiere-ich-sharepoint-in-einer-farm/","tags":["xArchived"],"title":"Wie installiere ICH SharePoint in einer Farm?"},{"content":"Auf einer internen Fortbildung hatte ich mir den IIS7 angesehen. Eine der unscheinbarsten Änderungen trifft alle, die sich mit Kerberos auf einem IIS auseinander setzen mussen. Meine SharePoint Installationen sind also in Zukunft davon betroffen. Der Internet Information Server 7 (IIS7) kommt in der Default Konfiguration mit der Einstellung \u0026ldquo;Enable Kernel-mode authentication\u0026rdquo; gleich ON daher. Damit unterscheidet sich das Auflosen der Service Principlal Names (SPN) drastisch von der Art, die der IIS6 durchfuhrt. Normalerweise wurde der SPN auf dem Application Pool (AppPool) Acount gesucht. Also auf einem Domain Acount oder auf dem Machine Account, wenn es der \u0026ldquo;Network Service\u0026rdquo; oder das \u0026ldquo;Local System\u0026rdquo; ist. Die Konfiguration auf einem IIS7 sieht nun so aus, dass die Auflosung mit \u0026ldquo;Kernel-mode authentication\u0026rdquo; immer den Machine Account heranzieht. Es ist ein Detail welches man einfach kennen sollte. Wichtig: In einer SharePoint NLB Farm muss man naturlich dieses Feature abschalten, da hier der SPN auf einem Domain Account sitzen muss, da die virtuelle NLB Instanz anders als ein echter Cluster nicht im AD als Objekt vetreten ist und somit auch nicht korrekt fur Kerberos und die Delegation konfiguriert werden kann. Das Setting kann aber uber den neuen IIS Manager einfach gesetzt werden. Mehr Info\u0026rsquo;s gibt es auch bei dem Kollegen Spencer Harbar: Using Kerberos with SharePoint on Windows Server 2008\nCiao Marco\n","date":"2008-06-24","permalink":"https://marcoscheel.de/2008/06/24/windows-server-2008-iis7-kerberos-kernel-mode-authentication/","tags":["xArchived"],"title":"Windows Server 2008 – IIS7 – Kerberos Kernel Mode Authentication"},{"content":"Ich hatte jetzt bei einer SharePoint Intranet Installation ein Problem bei der Darstellung eines Content Query WebParts (CQWP). Auf der Startseite und einigen SubSites wurden uber ein CQWP alle Artikel (CMS Elemente) eines ContentTyps dargestellt. Ich habe mich an das Standardverfahren gehalten.\n1. Content Query WebPart in eine WebPartzone ziehen 2. Den WebPart exportieren 3. CommonViewField um die notigen Felder erweitern 4. WebPart importieren 5. Neuen XSLT Style anlegen fur die Darstellung (ItemStyle.xsl)   6. XSLT Funktion fur das entfernen der HTML Tags einsetzen und anwenden 7. WebPart auf den Startseiten plazieren\nWie nicht anders zu erwarten hat alles super funktioniert. Bei einer Anpassung an die Aggregation auf einer SubSite ist dann allerdings ein sehr unschones Phanomen aufgetreten. Die Aggregation sollte kein ItemLimit definieren. Der Custom Item Style mit der Vorschau des HTML Body war ausgewahlt. Nach dem Entfernen des Hakens fur das ItemLimit und dem Bestatigen mit OK, kam die Seite nicht mehr zuruck. Im Eventlog war zu sehen, dass ein .NET Runtime Error den IIS in die Knie gezwungen hatte. Etwas \u0026ldquo;Trial and Error\u0026rdquo; hat auf die Funktion zum Entfernen der HTML Tags gedeutet. Ich habe kurz uber Google gesucht und eine Alternative gefunden. Das Einsetzen des modifizierten XSLT hat die Aggregation korrekt dargestellt und fuhrte nun zu keinem Fehler mehr. Es muss wohl irgendwo ein HTML Konstrukt existieren, welches die erste Funktion zum Absturz bringt. Mich hat es allerdings sehr uberrascht, dass es gleich den ganzen WebServer mit runter gezogen hat. Erst ein IISRESET hat die Site wieder hoch gebracht.\nFunktionierendes XSLT zum Entfernen der HTML Tags:\n\u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 1:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:template\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;attr\u0026quot;\u0026gt;name\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;=\u0026quot;removeHtmlTags\u0026quot;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 2:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:param\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;attr\u0026quot;\u0026gt;name\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;=\u0026quot;html\u0026quot;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;/\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 3:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:choose\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 4:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:when\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;attr\u0026quot;\u0026gt;test\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;=\u0026quot;contains($html, '\u0026amp;lt;')\u0026quot;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 5:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:value-of\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;attr\u0026quot;\u0026gt;select\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;=\u0026quot;substring-before($html, '\u0026amp;lt;')\u0026quot;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;/\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 6:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;rem\u0026quot;\u0026gt;\u0026lt;!-- Recurse through HTML --\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 7:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:call-template\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;attr\u0026quot;\u0026gt;name\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;=\u0026quot;removeHtmlTags\u0026quot;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 8:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:with-param\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;attr\u0026quot;\u0026gt;name\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;=\u0026quot;html\u0026quot;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;attr\u0026quot;\u0026gt;select\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;=\u0026quot;substring-after($html, '\u0026amp;gt;')\u0026quot;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;/\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 9:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;/\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:call-template\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 10:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;/\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:when\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 11:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:otherwise\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 12:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:value-of\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;attr\u0026quot;\u0026gt;select\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;=\u0026quot;$html\u0026quot;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;/\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 13:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;/\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:otherwise\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 14:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;/\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:choose\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 15:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;/\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:template\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt;  Fehlerhaftes XSLT zum Entfernen der HTML Tags:\n\u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 1:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:template\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;attr\u0026quot;\u0026gt;name\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;=\u0026quot;removeMarkup\u0026quot;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 2:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:param\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;attr\u0026quot;\u0026gt;name\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;=\u0026quot;string\u0026quot;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;/\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 3:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:choose\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 4:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:when\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;attr\u0026quot;\u0026gt;test\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;=\u0026quot;contains($string, '\u0026amp;lt;')\u0026quot;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 5:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:variable\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;attr\u0026quot;\u0026gt;name\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;=\u0026quot;nextString\u0026quot;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 6:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:call-template\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;attr\u0026quot;\u0026gt;name\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;=\u0026quot;removeMarkup\u0026quot;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 7:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:with-param\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;attr\u0026quot;\u0026gt;name\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;=\u0026quot;string\u0026quot;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;attr\u0026quot;\u0026gt;select\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;=\u0026quot;substring-after($string, '\u0026amp;gt;')\u0026quot;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;/\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 8:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;/\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:call-template\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 9:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;/\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:variable\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 10:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:value-of\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;attr\u0026quot;\u0026gt;select\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;=\u0026quot;concat(substring-before($string, '\u0026amp;lt;'), $nextString)\u0026quot;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;/\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 11:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;/\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:when\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 12:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:otherwise\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 13:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:value-of\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;attr\u0026quot;\u0026gt;select\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;=\u0026quot;$string\u0026quot;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;/\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 14:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;/\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:otherwise\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 15:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;/\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:choose\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 16:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;/\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;xsl:template\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt;  ","date":"2008-06-24","permalink":"https://marcoscheel.de/2008/06/24/content-query-webpart-bringt-w3p-zum-absturz/","tags":["xArchived"],"title":"Content Query Webpart bringt W3P zum Absturz"},{"content":"Meine SharePoint Installationen sind in der Regel Intranet Installationen. Je nach Szenario ist Kerberos uber NTLM als Authentifizierung zu beorzugen. Es ist schwer mit Pauschalen zu arbeiten, aber in der Regel empfehle ich den EInsatz von Kerberos. Die Intrastruktur laßt sich so schnell an wachsende Aufgaben (Reporting, OTP, etc) anpassen. Beim Setzen der Service Principal Names (SPN) ist es wichtig, das der Service nur auf einm einzigen Objekt im Active Directory (AD) liegt. Zum Setzen und Auslesen der SPN\u0026rsquo;s gibt es SETSPN.exe aus dem Ressource Kit (oder in Windows Server 2008 bereits enthalten). In der W2K8 Version von SETSPN kann ich beim Hinzufugen gleich auf doppelte Eintrage prufen lassen. Nur wenn es den Service Name noch nicht auf einem anderen Objekt gibt, wird er auf das gewuschte Element auch gesetzt. Die W2K8 Version lauft (wie immer) nicht auf den Vorganger-Versionen. Es kommt hinzu, dass ich so Doubletten vermeide, aber wenn ich den entsprechenden Eintrag setzen will, muss ich irgendwie an die Information kommen, wo der Service registriert wurde. Ich habe im Internet gegooglet und eine passende Losung gefunden:\nhttp://blogs.dirteam.com/blogs/carlos/archive/2006/04/21/812.aspx\n\u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 1:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;GC_Server_Name\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; = CMD -\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; SET -\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; value of LOGONSERVER \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 2:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;Service\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; = e.g. HOST or HTTP or ... \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 3:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;URI\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; = e.g. URL of portal -\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; http://portal.ms.local \u0026lt;span class=\u0026quot;lnum\u0026quot;\u0026gt; 4:\u0026lt;/span\u0026gt; ldifde -s \u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;GC_Server_Name\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt; -f c:\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;My_SPN_Dump_File\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt;.txt -d \u0026quot;\u0026quot; -r \u0026quot;(serviceprincipalname=\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;Service\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026lt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;html\u0026quot;\u0026gt;URI\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;kwrd\u0026quot;\u0026gt;\u0026gt;\u0026lt;/span\u0026gt;)\u0026quot; -p subtree -t 3268 -l dn,serviceprinciplename  Ciao Marco\n","date":"2008-06-19","permalink":"https://marcoscheel.de/2008/06/19/doppelte-spns-finden/","tags":["xArchived"],"title":"Doppelte SPN’s finden"},{"content":"Ich versuche gerade mein Leben im WEB neu zu ordnen. Die Ausrichtung auf Web 2.0 Dienste (wenn es so was überhaupt gibt) geht heute ein Schritt weiter. Kurz die Historie: * Ich nutzt Google Reader für RSS und Share dort interessante Posts anderer Blogger * Ich habe einen Blog, den ich mit Windows Live Writer mit Posts und Tags füllen kann * Ich analysiere mein WebSite-Traffic mit Google Analytics * Ich geschäftlich bei Xing registriert * Ich nutze privat Wer-kennt-wen * Ich habe ein Flickr Account (ohne ihn bisher zu nutzen) für Bilder, die ich über die Windows Live Photo Gallery Software online stellen kann * Ich speichere meine Bookmarks bei del.icio.us * Ich habe ein SkyDrive (ohne es wirklich zu nutzen) für meine Onlinedateien * Ich nutze Windows Live Messenger für Instant Messaging und ab und an Skype für Video Telefonie * Ich nutze Windows Live Mail für mein Hotmail Account * Mein RSS Feed wird über Feedburner aggregiert und aus weiteren Quellen angereichert\nHeute kommt noch ein Service, der mir persönlich noch etwas unbekannt ist, hinzu:\nTechnorati Profile\n Dieser Post dient also primär dazu mein Blog bei Technorati zu \u0026ldquo;claimen\u0026rdquo;. Was genau der Dienst bringt, werde ich bestimmt noch raus finden.\nMan kann nun auf jeden Fall zwei Dinge erkennen: 1. Ich habe x Accounts bei allen großen Anbietern (wie der Rest der internet User) 2. Ein paar meiner wichtigsten Dienste gehören zu Google (Google Reader und Feedburner welches seit kurzem zum Google Imperium gehört)\nDem Account durcheinander ist leider nicht bei zu kommen. Die Sammlung der Dienste bei Google macht mir schon mehr Angst. Google ist auch mein primärer Suchdienst und so können die Jungs doch recht viel Daten über mich sammeln. Ich liebe es, die Vorteile der angebotenen \u0026ldquo;Software\u0026rdquo; zu nutzen. Die Zeit wird es zeigen, wie Google damit umgeht und wie die Öffentlichkeit darüber berichtet (wie also die öffentliche Meinung geformt wird).\nEs bleibt noch anzumerken, dass es trotz der Menge an Services nur ein Bruchteil der Dienste ist, die sich in dem Bereich tummeln. Bisher nicht genutzte Dienste: * StumbleUpon * Digg * YouTube * iGoogle * GMail * Facebook * MySpace\nAngeschaut habe ich auch fast alle einmal, aber genutzt habe ich keins länger als ein Tag :-)\nIch habe mir vorgenommen, über die verscheidenen Dienste nochmal im Detail zu berichten, zumindest die dich ich für interessant empfinde. Mal sehen, ob das klappt.\nCiao Marco\n","date":"2008-06-16","permalink":"https://marcoscheel.de/2008/06/16/ein-schritt-weiter-in-die-web-2-0-welt/","tags":["xArchived"],"title":"Ein Schritt weiter in die Web 2.0 Welt"},{"content":"Ab heute will ich es mit einem neuen Zuhause mit meinem Blog versuchen. Ein paar Posts werde ich nich auf beiden aktiv werden, aber ich will möglichst schnell den Live Space Blog schließen.\nNeues Heim: http://marcoscheel.de\nIch habe auf Basis von GraffitiCMS meinen Blog neu aufgezogen. Schaut doch auch mal auf die Seite. Das Layout ist ein angepaßtes Theme. Mehr zum Thema GraffitiCMS vielleicht in den nächsten Posts.\nCiao Marco\n","date":"2008-06-15","permalink":"https://marcoscheel.de/2008/06/15/angekommen-neues-zuhause-fr-meinen-blog/","tags":["xArchived"],"title":"Angekommen: Neues Zuhause für meinen Blog"},{"content":"Wir implementieren gerade eine SiteCollection in einem Portal in dem es darum geht Video\u0026rsquo;s die aus einer Document Library kommen in einem Flash Movie darzustellen. Um die Performance und die Wartbarkeit zu halten haben wir folgende Dinge eingerichtet:\n Eigene ContentDB  Wir haben die SiteCollection uber STSADM angelegt und einer leeren ContentDB auf dem SQL Server zugewiesen. Anschließend haben wir die ContentDB im Central Admin \u0026ldquo;Offline\u0026rdquo; genommen, so das die Datenbank nur fur Video Content genutzt wird. Fur den Backup Prozess kann man so einen anderen Schedule hinterlegen, da hier nicht so viel \u0026ldquo;Bewegung\u0026rdquo; existiert und es primar um grosse Dateien geht. 2. Upload Limit erhohen\nDie Konfiguration wird Leider fur die gesamte WebApplication vorgenommen. Sollte es also notig sein, dass nur die neue SiteCollection von dem Limit beroffen ist, dann wurde eine eigene Application benotigt, was den Aufwand naturlich deutlich erhoht.\n 3. Disk Based Blob Cache einschalten\nHier ist der Artikel von MS:\nMSDN: Disk-based Caching for Binary Large Objects\nTechnet: Caching in Office SharePoint Server 2007\nEMC Team Blog: How can I make my web site faster with caching?\nSo sieht unsere Web.Config dann auf den beiden WFE Servern aus:\n\u0026lt;BlobCache\nlocation=\u0026ldquo;D:MOSS-BlobCache\u0026rdquo;\npath=\u0026quot;.(flv)$\u0026quot;\nmaxSize=\u0026ldquo;1\u0026rdquo;\nmax-age=\u0026ldquo;86400\u0026rdquo;\nenabled=\u0026ldquo;True\u0026rdquo; /\u0026gt;\nIm Moment werden also nur Flash Videos fur 24 Stunden mit einer gesamt Cachegroße von 1 GB gecached.\nDas erzielte Ergebnis hilft zum einen die Datenbank zu entlasten, sendet allerdings auch gleich an den Client per HTTP-Header die folgenden Cache Control einstellungen:\nCache-Control: public, max-age=86400\nSomit wurde auch der Client erst in 24 Stunden nach einem neuen File fragen. Die Konsequenz ist also das egal, ob ich den Cache leere oder den Content auf dem Server austausche, der Client wurde bei korrekter einstllung erst in 24 Stunden nach einem neuen File fragen. Ich hatte schon Situationen, wo so etwas zu Problemen gefuhrt hat. Eventuell setzen wir den Cache auf 15 Minuten, was die Server bei vielen gleichzeitigen Zugriffen noch immer deutlich unterstutzt, aber bei einem fehlerhaften File (Video) den notigen Spielraum fur ein kurzfristiges Update schafft.\nSollte man in die verlegenheit kommen den Cache fruher wieder los werden zu wollen, dann gibt es diese folgende potentielle Moglichkeiten:\n **STSADM  **stsadm -o setproperty -propertyname blobcacheflushcount -propertyvalue 11 -url http://mywebapp:port 2. SiteSettings - Site Collection Administration - Site collection object cache\n\nAchtung: Diese Aktion muss fur jeden Zone auf der eine Applikation \u0026ldquo;Extended\u0026rdquo; wurde durchgefuhrt werden. Sollte zum Beispiel also noch ein HTTPS Site mit laufen, mußte das auch dort gemacht werden. 3. del .\nEinfach aud den WFE Servern den Verzeichnisinhalt loschen und ein IISRESET durchfuhren\nCiao Marco\n","date":"2008-06-13","permalink":"https://marcoscheel.de/2008/06/13/sharepoint-disk-based-blob-cache/","tags":["xArchived"],"title":"SharePoint Disk Based Blob Cache"},{"content":"Aus der Woche in der ich mich intensiver mit dem Windows Server 2008 und dem IIS7 beschäftigt habe, hat sich ein Detail eingebrannnt:\nAchtung: Firewall ist by default eingeschaltet!\nTotal sicher, aber auch total ungewohnt. Ich bin gleich drei mal in den Fehler gelaufen: 1. IIS7 FTP Server von einem Vista Client aus testen 2. SharePoint Server in die Farm joinen 3. Reporting Services Integration auf dem Shareoiint aktivieren\n\nIch habe die Firewall nicht abgeschaltet, sondern einfach die Ports, die ich jetzt benötigte, eingetragen als Exception. Es ist einfach Gewohnheitssache. Ich habe nicht ganz so viel Zeit verloren, da ich nicht so viele Ports benötige, aber Kollegen in deutlich komplexeren Szenarien, hat es einiges mehr an Zeit gekostet.\nCiao Marco\n","date":"2008-06-13","permalink":"https://marcoscheel.de/2008/06/13/windows-server-2008-und-der-default-zustand-der-firewall/","tags":["xArchived"],"title":"Windows Server 2008 und der Default Zustand der Firewall"},{"content":"Das wird einen lange Nacht in Offenbach.\n\nCiao Marco\n","date":"2008-06-11","permalink":"https://marcoscheel.de/2008/06/11/21-fr-die-trkei/","tags":["xArchived"],"title":"2:1 für die Türkei"},{"content":"Ich bin jetzt ein Mitglied der FeedBurner Familie. Ab sofort ist mein RSS Feed über folgende URL zu erreichen:\nhttp://feeds.feedburner.com/marcoscheel\nDie Idee dahinter ist den Mehrwert, den so ein Dienst bringt, auszunutzen. Ich bin mir nicht sicher, ob ich der Microsoft Windows Live Space Plattform für Blogging treu bleibe. Ich bin sehr begeistert über die Funktionen und die Integration in den Windows Live Writer. Allerdings fehlen doch viele Dinge: 1. Statistiken\nSie sind schlicht nicht vorhanden:\n 2. Kommentare\nDas Handling ist mehr als suboptimal. 3. Integration von weiteren Diensten\nSolange ich meinen Feed bei Feedburner Manage, bin ich in der Lage meinen Blog so oft umzuziehen wie ich möchte! Auch wenn ich es vielleicht nie tun sollte. Feedburner biete eine Menge funktionen. Kurz angemerkt (ich kenne es auch erst seit eben):\n 1. del.icio.us Aggregation\nEinmal am Tag werden meine Bookmarks \u0026ldquo;geblogget\u0026rdquo; 2. Flickr Aggregation\nSobald ich bei Flicker Bilder mit einem Tag versehe, werden sie in meinem Feed Aggregiert. 3. Flares - Email This \u0026amp; Save to del.icio.us\nMinifunktionen in jedem Feed item 4. Statistiken\nNoch nicht viel zu sehen, aber mehr als nur ein Punkt:\n\nAlso lieber Leser, wer mehr von mir wissen will, sollte auf den Feedburner Link wechseln.\nDanke und Ciao Marco\n","date":"2008-06-10","permalink":"https://marcoscheel.de/2008/06/10/mein-rss-feed-ndert-sichsoll-sich-ndern/","tags":["xArchived"],"title":"Mein RSS Feed ändert sich/soll sich ändern"},{"content":"Auf dem Microsoft SharePoint Team Blog gab es eben die Ankündigung, dass es nun die SharePoint Extenions für Visual Studio 2008 zum Download gibt.\nhttp://blogs.msdn.com/sharepoint/archive/2008/06/04/announcing-the-vsewss-version-1-2.aspx\nDie fehlende Verfügbarkeit hat mich schon oft genervt. Endlich ist es vorbei. Es gibt keine neuen Features. Aber das beste Feature ist folgendes:\n\nEs läßt sich unter Vista installieren. Allerdings\u0026hellip; eben in den Comments gelesen\u0026hellip; es gibt kein 64-Support :-S Naja man kann nicht alles haben.\n\nCiao Marco\n","date":"2008-06-05","permalink":"https://marcoscheel.de/2008/06/05/windows-sharepoint-services-3-0-tools-visual-studio-2008-extensions-version-1-2/","tags":["xArchived"],"title":"Windows SharePoint Services 3.0 Tools: Visual Studio 2008 Extensions, Version 1.2"},{"content":"Ich habe gestern beim Babysitten etwas Zeit gehabt und mal wieder meine Feeds gelesen. Dabei bin ich über zwei meiner Meinung nach wichtigen Artikel gestoplert: 1. Kerberos, SharePoint und Windows Server 2008\nIch wollte ja selber was zum Thema schreiben, aber dieser Blog nimmt mir die Arbeit ab. Alle die SharePoint auf W2K8 installieren wollen und ernsthaft kerberos betreiben wollen, müssen diesen Artikel lesen\nUsing Kerberos with SharePoint on Windows Server 2008 2. Sharepoint Post SP1 Hotfixes\nEs ist nicht einfach da den Überblick zu behalten. der Kollege Spencer Harbar (ist eine RSS Subscripion mehr als Wert) macht im Moment zumindest den Versuch den Überblick zu behalten:\nSharePoint 2007 Post SP1 Hotfixes\nWer selber noch auf der Suche nach dem ultimativen Bookmarktool ist, dem würde ich einen Blick auf del.icio.us empfehlen.\n\nCiao Marco\n","date":"2008-06-04","permalink":"https://marcoscheel.de/2008/06/04/sharepoint-links-zum-bookmarken/","tags":["xArchived"],"title":"SharePoint Links zum Bookmarken"},{"content":"Ich wollte jetzt einmal kurz (auch für mich als reminder) runter schreiben, was passiert, wenn ich verschiedene Dinge an einem ContentType ändere: 1. **Vorbedingung\n**\nAlle folgenden Aktionen haben eine Einstellung für die Aktualisierungsweitergabe )\u0026ldquo;Update List and Site Content Types\u0026rdquo;). Ob bestehende Listen aktualisiert, werden hängt von der Wahl der entsprecheden Option ab. Alle folgenden Aktionen werden auf dem ContentType gemacht! 2. Hinzufügen einer Spalte\nBeim Einfügen werden alle Listen, die den ContentType verwendet, entsprechend der Einstellung aktualisiert. Der Liste wird eine neue Spalte hinzugefügt. Der Wert der Spalte für bestehende Einträge wird **nicht **auf den definierten Default-Wert gesetzt! Die neue Spalte ist also ungefüllt. 3. Verändern des Datentyp\nEinfache Spaltentypen wie \u0026ldquo;Single line of text\u0026rdquo; oder \u0026ldquo;Number\u0026rdquo; lassen sich in einander umwandeln. Ob die Werte erhalten bleiben, hängt natürlich vom Inhalt ab. 4. Löschen einer Spalte\nLösche ich eine Spalte aus dem ContentType wird Sie aus den entsprechenden Listen nicht entfernt sondern die Definition wird in eine ganz normale Spalte ohne ContentType abhängigkeit geändert. Alle Werte und Viewdarstellungen bleiben also unberührt. Achtung: Bei ContentQueryWebParts (CQWP) kann es entsprechenden Fehlern geben. 5. **Erneutes Hinzufügen einer zuvor gelöschten Spalte\n**Die Spalte wird quasi zurückgemappet und es entsteht kein Konflikt und kein doppelter Spalteneintrag. Hat sich der Datentyp geändert, dann kommt zusätzlich, das Verhalten aus Punkt 3. zum Tragen. 6. **Hinzufügen einer Spalte mit dem selben Namen (auch mit anderem Datentyp)\n**bei gleichem Namen wird einfach eine weitere Spalte eingefügt. Unterschied liegt in dem internen Mamen. An der Oberfläche ist nichts zu erkennen.\nVor dem Ändern (Typ=Number):\n\nNach dem Hinzufügen (Typ=Single Line of text):\n\nCiao Marco\n","date":"2008-06-02","permalink":"https://marcoscheel.de/2008/06/02/contenttypes-und-was-passiert-bei-nderungen/","tags":["xArchived"],"title":"ContentTypes und was passiert bei Änderungen"},{"content":"Ich habe eben versucht die Reporting Integration für den SQL2008 auf dem MOSS Server zu aktivieren. Die Installation ist ohne Fehler abgebrochen. Die Lösung gibt es in der Readme von MS zum Add-In:\nReadme:\nhttp://download.microsoft.com/download/0/9/4/09438108-4f1d-4734-8100-4ff73d549082/Readme_rsaddin.htm\nDownload aus dem February CTP Feature Pack des SQL 2008:\nhttp://www.microsoft.com/downloads/details.aspx?FamilyId=D68DE3C9-60A9-49C9-A28C-5C46BBC3356F\u0026amp;displaylang=en\nDie Installation läuft anders als das Add-In aus dem SQL 2005 SP2 Feature Pack. Es muss kein Cab per STSADM registriert werden, sondern das MSI erledigt die ganze Arbeit. Mein Testenviroment hat wenig RAM, da alles auf meinem Lenovo x61T mit 4 GB unter Vista läuft. Das Setup schlägt fehl und die Readme hat mir dann weiter geholfen. Es bleibt nur anzumerken, dass die Beschreibung ein falschen MSI Namen verwendet. Die Installation läuft wirklich lange, da alle Locals (LCID) installiert werden. Aber es lohnt sich.\nCiao Marco\n","date":"2008-05-30","permalink":"https://marcoscheel.de/2008/05/30/sharepoint-und-microsoft-sql-server-2008-reporting-add-in-installation/","tags":["xArchived"],"title":"SharePoint und Microsoft SQL Server 2008 Reporting Add-in Installation"},{"content":"Ich beschäftige mich gerade mit dem Windows Server 2008 (wird ja auch Zeit) und schaue mir speziell den IIS an. Einen Post dazu wird es sicher in Zukunft geben. Meinem Interesse am W2K8 Server gilt natürlich auch speziell der SharePoint-Fähigkeit. Erstes Fazit ist toll, aber auch dazu später mehr. Negativ aufgefallen ist nun der fehlende POP3 Service. Ob die Mehrheit ihn benötigt, ist jetzt nicht die Frage. Für ein (schlanke) SharePoint Testumgebung war es allerdings die einfachste Möglichkeit, eine Szenario samt Mail zu realisieren.\nIch bin natürlich nicht der einzige, der ihn vermißt:\nhttp://patrickyong.net/2008/05/15/pop3-server-for-windows-server-2008/\nDie Lösung habe ich nicht getestet, da ich überlege, ob ich nicht den hMailServer für so was nehme. Wenn jemand ein Vorschlag hat, der ohne Exchange auskommt, bin ich für alle Berichte bankbar.\nCiao Marco\n","date":"2008-05-30","permalink":"https://marcoscheel.de/2008/05/30/windows-server-2008-und-fehlender-pop-service/","tags":["xArchived"],"title":"Windows Server 2008 und fehlender POP Service"},{"content":"Ich mache viele reine Intranet-Konfigurationen/Installationen. Einen grossen Teil davon versuche ich, mit Kerberos Authentifizierung zu realisieren. Es ist etwas an Vorarbeit nötigt, aber der Mehrwert, wenn es um mehr als reinen CMS Content geht, ist doch recht gross. Szenarien sind folgende: * Zugreifen auf die Mailbox der Users (Unread-Items, Kalenderabfragen, \u0026hellip;) * Zugreifen auf Datenbanken (der Loginname steht für personalisierte Abfragen zur Verfügung, etablierte Security auf dem SQL Server kann genutzt werden, \u0026hellip;) * Zugreifen auf KPI\u0026rsquo;s aus dem Analysis Server (MS SQL 2005) * Zugreifen auf dem Fileserver aus SharePoint Code * RSA Authentifizierung ohne Passwort (nur mit Contrained Kerberos Delegation) * \u0026hellip;\nWas man vorher tun muss: * DNS (Jede Anwendung mit eigenem AppPool sollte seine eigene URL haben, der Central Admin ist der einzige der auf einem MachineName läuft) * AppPoolAccounts aus dem Active Directory Domain * SPN (ServicePrincipalName) auf die Accounts zu den entsprechenden Services setzen * Delegation für den Account konfigurieren (Tab im AD ist erst nach dem Setzen der SPN\u0026rsquo;s verfügbar)\nDie Konfiguration des IIS übernimmt dann SharePoint selbst. Beim Anlegen der Applications wird man gefragt, wie man Authentifizieren will. Später kann man die Option über den Central Admin im Tab Application Management unter dem Punkt Authentication Provider finden. Sollte das Umstellen mit einem Fehler quitiert werden, ist Vorsicht geboten. Ein erneutes Aufrufen zeigt zwar den gewünschten Wert an, aber dieser wird nicht \u0026ldquo;live\u0026rdquo; ausgelesen, sonder kommt aus der ConfigDB. Ich hatte schon Installationen in denen ein Bug im IIS (OWS Timer Bug) dazu führte, dass die Metabase nicht zu schreiben war. Merkt man auch daran das das IIS Management Tool nicht auf geht.\nWeitere Informationen: * http://blogs.msdn.com/solutions/archive/2008/02/28/enterprise-portal-kerberos-delegation-for-connecting-to-reporting-analysis-services-on-a-different-box.aspx * http://blogs.msdn.com/selvar/archive/2007/11/10/kerberos-overview.aspx * http://blogs.msdn.com/gregmcb/archive/2008/03/08/kerberos-fails-when-using-cname.aspx * http://feeds.feedburner.com/~r/TonStegeman/~3/181218254/ViewPost.aspx\nCiao Marco\n","date":"2008-05-16","permalink":"https://marcoscheel.de/2008/05/16/sharepoint-moss-und-kerberos/","tags":["xArchived"],"title":"SharePoint (MOSS) und Kerberos"},{"content":"Ich komme immer wieder in die Situation, dass ich die System oder zumindest die tempdb einer MS SQL Server Installation umlegen muss. Anweisungen für das Vorgehen findet man hier:\nhttp://support.microsoft.com/kb/224071/en-us\nAm einfachsten und wichtigsten ist das Umkonfigurieren der tempb.\nCiao Marco\n","date":"2008-05-13","permalink":"https://marcoscheel.de/2008/05/13/move-sql-server-database-files/","tags":["xArchived"],"title":"Move SQL Server Database Files"},{"content":"Einfach mal so in den Raum gestellt: * Chris O\u0026rsquo;Brien\u0026rsquo;s blog: Using CustomAction to modify system pages * Free SQL Server tools that might make your life a little easier\nCiao Marco\n","date":"2008-05-13","permalink":"https://marcoscheel.de/2008/05/13/interessante-links/","tags":["xArchived"],"title":"Interessante Links"},{"content":"Das Tool der Wahl heißt SharePoint Designer und wie es geht erklärt MS selbst:\nCreate HTML Editor styles - SharePoint Designer - Microsoft Office Online\n\nCiao Marco\n","date":"2008-05-12","permalink":"https://marcoscheel.de/2008/05/13/eigene-style-informationen-im-wysiwyg-editor-von-sharepoint/","tags":["xArchived"],"title":"Eigene Style Informationen im WYSIWYG-Editor von SharePoint"},{"content":"Ich habe jetzt schon einige Installation mit dem Microsoft Office SharePoint Server 2007 (MOSS) gemacht. In der Regel sind es komplexere Installationen. Es sind also neben dem SQL Server noch zwei weitere Server in der SharePoint Farm.\nBei der Konfiguration gehe ich immer Schritt für Schritt vor. Installation der Software, Konfiguration auf dem Server \u0026ldquo;Application Server\u0026rdquo; für Central Admin Host, den Rest der Farm joinen, SSP anlegen (SSP Application und MySite Application anlegen), alle Optionen unter \u0026ldquo;Operations\u0026rdquo;, alle Optionen unter \u0026ldquo;Application Management\u0026rdquo; und zum Schluss alle Optionen des SSP. So weit hat sich Routine eingestellt und es ist mit keiner Komplikation zu rechnen. Allerdings kommt dann der Punkt für den \u0026ldquo;Profile Import\u0026rdquo; oder \u0026ldquo;Search - Content Source\u0026rdquo; zum Erstellen der Schedules. Und auch mit SP1 (noch keine Rollups) bekomme ich hier immer mal wieder einen Fehler \u0026ldquo;Access Denied\u0026rdquo;.\n\nGrund für den Fehler ist wohl eine fehlerhafte Konfiguration des \u0026ldquo;%windir%tasks\u0026rdquo; Ordners und der Sicherheitseinstellungen. Für die AppPool Accounts muss hier die korrekte Security eingestellt werden. Details wie man das auch bei dem speziellen Ordner der \u0026ldquo;Scheduled Tasks\u0026rdquo; macht findet man hier:\nhttp://support.microsoft.com/kb/926959/en-us\nIch mache bald eine Installation direkt mit SP1 Rollups (3 an der Zahl zum heutigen Stand) und werde berichten, was passiert.\nCiao Marco\n","date":"2008-05-12","permalink":"https://marcoscheel.de/2008/05/13/fehler-beim-erstellen-eines-zeitplans-im-ssp/","tags":["xArchived"],"title":"Fehler beim Erstellen eines Zeitplans im SSP"},{"content":"Folgende Artikel behandeln das Thema oder die Auswirkung: * Best Practices: Using Disposable Windows SharePoint Services Objects\nhttp://msdn2.microsoft.com/en-us/library/aa973248.aspx * Best Practices: Common Coding Issues When Using the SharePoint Object Model\nhttp://msdn2.microsoft.com/en-us/library/bb687949.aspx * Stefan Gossner: Troubleshooting SPSite/SPWeb leaks in WSS v3 and MOSS 2007\nhttp://blogs.technet.com/stefan_gossner/archive/2008/05/07/troubleshooting-spsite-spweb-leaks-in-wss-v3-and-moss-2007.aspx\n Es ist leider noch immer so, dass hinter den .NET Schichten eine Menge (D)COM(+) liegt. In der .NET Welt ist man \u0026ldquo;leider\u0026rdquo; ja nicht mehr gezwungen, sich gezielt um speicher zu kümmern. Leider wird das unerfahrenen aber auch erfahrenen Entwicklern immer wieder zum Stolperstein. Genau wie beim Zugriff auf Datenbanken (Thema: Connection Pool Limit) und beim Lesen und Schreiben von Dateien, kommt es immer wieder zu Problemen. In einer WebAnwendung kommen diese Effekte in der Regel deutlich stärker zum Tragen, als in Windows Anwendungen, da das Beenden der Anwendung in den meisten Fällen auch die genutzten Ressourcen wieder frei gibt. So kommen die SQL Connections wieder in den Pool, sobald zum Beispiel die CommandLine Applikation nach dem Ausführen automatisch geschlossen wird. In einer WebAnwendung leben die Objekte im W3P (seit dem IIS6, also pro ApplicationPool). Ob und wann dieser beendet wird, ist von vielen Parametern abhängig. In der Regel lebt der Prozess allerdings länger als seine Verwandten auf dem Desktop. Es kommt hinzu, dass die Useranzahl in der Regel höher ist also bei einem Windows Programm.\nEin guter SharePoint Programmierer muss also wissen, was hinter seinen Objekten steckt. Leider ist es es auch nicht so einfach immer ein Dispose aufzurufen, wo es angeboten wird :-S Ein Sharepoint Objekt das direkt aus dem SPContext kommt, wurde zum Beispiel nicht von dem Programmierer erstellt und somit ist er auch nicht für das schließen verantwortlich. Er darf es sogar nicht schlichen, da noch anderer Programmcode nach ihm das Objekt benötigt. Es wird also deutlich, dass es darauf ankommt, wo ich meine Objekte her bekomme. In der Regel ist man gut beraten, die MS Guides zu lesen und diese zu befolgen. Ein MS SharePoint Objekt, welches das IDisposable Interface implementiert, sollte auf jeden Fall geprüft werden, ob es geschlossen werden muss.\nWenn man nun solche Probleme hat, sollte man sich einfach den Artikel von Stefan Grosser ansehen. Es wird schnell klar, dass es nicht trivial ist, die Quelle zu finden. Selbstgeschriebener oder eingekaufter Code kann die Quelle für Speicherprobleme sein, allerdings gibt es auch genug andere Ursachen, die von MS direkt kommen. Zumindest läßt der eine oder andere KB Artkel darauf schließen.\nAlso dann happy coding.\nCiao Marco\n","date":"2008-05-12","permalink":"https://marcoscheel.de/2008/05/13/sharepoint-coding-und-speicherverwaltung/","tags":["xArchived"],"title":"SharePoint (Coding) und Speicherverwaltung"},{"content":"Zum Nachschlagen für die Rechte, die ein User haben kann, die für Filterung von Elementen (z.B. Navigationspunkten in Systemmenüs) zuständig sind (Audience Targeting):\nhttp://office.microsoft.com/en-us/sharepointtechnology/HA101001491033.aspx\nHintergrund ist das Ein- und Ausblenden von Controls, wenn der User zum Beispiel Listen erstellen kann. Ein Beispiel in dem MS das selber macht ist die MasterPage des Intranet Collaboration Portals. Hier wird der Punkt \u0026ldquo;View all site content\u0026rdquo; für User ohne entsprechende Rechte versteckt. Somit ist es für eigene Solutions und Menüeinträge ebenfalls relevant.\nCiao Marco\n","date":"2008-05-12","permalink":"https://marcoscheel.de/2008/05/13/sharepoint-und-permissions/","tags":["xArchived"],"title":"SharePoint und Permissions"},{"content":"Ich habe mich vor kurzem etwas intensiver mit dem MS SiteDirectory Template beschäftigt. Es ging darum, hier automatisch MetaDaten zu aktualisieren. Die \u0026ldquo;Kindsite\u0026rdquo;, die im SiteDirectory einen Eintrag in der \u0026ldquo;Sites\u0026rdquo; Liste hat, sollte unter speziellen Bedingungen aktualisiert werden. Nun ist die Frage wie kommt man ran? Die URL irgendwie speichern? Aber wer macht das? Wann macht man das? Ein Griff zur PowerShell hat schnell zu Tage gefördert, dass alle Anstrengungen überflüssig sind. Zumindest für Sites, die über das SiteDirectory erstellt wurden, findet man in den Properties des SPWeb Objektes die nötigen Informationen: * SPWeb.AllProperties[\u0026ldquo;DefaultSiteDirectorySiteId\u0026rdquo;]\nDamit kann ich mir die SiteCollection öffen * SPWeb.AllProperties[\u0026ldquo;DefaultSiteDirectoryWebId\u0026rdquo;]\nDamit kann ich aus der SiteCollection das richtige SPWeb holen * Dann muss ich nur noch die Liste \u0026ldquo;Sites\u0026rdquo; öffnen und kann die URL\u0026rsquo;s mit der aktuellen URL vergleichen und somit Aktualisierungen vornehmen\nCiao Marco\n","date":"2008-05-12","permalink":"https://marcoscheel.de/2008/05/13/arbeiten-mit-dem-sharepoint-site-directory/","tags":["xArchived"],"title":"Arbeiten mit dem Sharepoint \"Site Directory\""},{"content":"In zwei Installationen hatte ich jetzt schon einen Fehler beim Erzeugen von MySites, obwohl das bisher ohne Problem funktioniert hat. Es wird wohl an SP1 oder einem anderen MS OS Update liegen, da auch bisher Farmen, die über 1 Jahr ohne Probleme liefen, betroffen sind. Bei der bereits laufenden Farm hat mir der System Center Operations Manager (SCOM oder OpsMgr) den Gefallen getan, mich darüber zu informieren, da nicht unbeingt jeder User sich darüber beschwert, wenn etwas nicht funktioniert, was er bisher noch nciht kennt :-)\nDie Lösung:\nhttp://forums.microsoft.com/TechNet/ShowPost.aspx?PostID=1196717\u0026amp;SiteID=17\u0026amp;pageid=0\nBei mir hat auch das Aufnehmen in die locale Admingruppe geholfen, irgendwie unschön, aber solange ich nicht mehr Zeit habe, das bei MS zu verifizieren und ein Hotfix anzufordern (wenn er denn existiert), wird es erstmal so bleiben.\nCiao Marco\n","date":"2008-05-12","permalink":"https://marcoscheel.de/2008/05/13/mysite-creation-fehler/","tags":["xArchived"],"title":"MySite Creation Fehler"},{"content":"Es kommt immer mal wieder vor, dass man mit den Standard Properties des Profile Imports (im Shared Service Provider zu finden) nicht auskommt. Zum Beispiel ist das AD Feld für das Unternehmen (Company) nicht gemapped. Eine schnelle Suche hat mich hier her geführt:\nhttp://www.computerperformance.co.uk/Logon/LDAP_attributes_active_directory.htm\nVielleicht hilft es euch ja auch weiter. Mir hat es geholfen, mein Property aus dem AD zu fischen.\nCiao Marco\n","date":"2008-05-12","permalink":"https://marcoscheel.de/2008/05/13/ad-properties-fr-profile-import/","tags":["xArchived"],"title":"AD Properties für Profile Import"},{"content":"Ich als alter Tablet PC user bin ein totaler Microsoft Office OneNote 2007 Fan. In Kombination mit meiner MySite (als Ablageort meiner OneNote Files) ist es das Produktivitätstool schlecht hin. Ich zeichne beim Kunden oft die Start und Endzeiten meines Aufenthalts auf. Somit eignet sich das ganze auch als gedächnisstütze für die Zeiterfassung die ich zu erledigen habe. Bei der zeitlichen Darstellung hilft ein Tool von Josh Einstein (Freeware):\n\nDas Tools heißt \u0026ldquo;OneNote Calendar\u0026rdquo; gibt es hier zum Download.\nCiao Marco\n","date":"2007-12-04","permalink":"https://marcoscheel.de/2007/12/04/onenote-als-kalender-darstellen/","tags":["xArchived"],"title":"OneNote als Kalender darstellen"},{"content":"Ich habe fur einen Kunden vor einiger zeit auf der testumgebung die damalige Beta fur die Citrix Integration installiert. Es geht hautsachlich darum, den AppDelivery Webpart zu nutzen. In der Beta hat das auf einer Maschine auch wunderbar funktioniert. In der Produktion haben wir aber 2 WFE Server und einen APP Server. Die Administration hat immer einen Fehler geworfen:\n\nIch liebe es: Unknown Error. Da wir in der Produktion sind, kann ich kein erweitertes Fehlerhandling einschalten.\nLosung:\nEss fehlte folgender Eintrag in der WebConfig (system.web/compilation/expressionBuilders):\nAlternative:\nDie Farm ist so aufgebaut, dass vom APP Server deployed wird. Der AppServer selber aber nicht die die WebApplication Rolle hat. Ein Activate Feature auf einem der WFE hat ebenfalls zum Erfolg gefuhrt (farmweit):\nstsadm -o activatefeature -name CitrixAccessCore -url http://myserver\nVielleicht bin ich der einzigste mit so einem Fehler, aber vielleicht hilft der Post auch jemand weiter :-)\nCiao Marco\n","date":"2007-12-04","permalink":"https://marcoscheel.de/2007/12/04/citrix-und-sharepoint/","tags":["xArchived"],"title":"Citrix und Sharepoint"},{"content":"Unglaublich. Da ich gerade noch nicht sicher bin, ob mein aktuelle Installtion auf dem Lenovo lange lebt, habe ich mir nicht die Mühe gemacht und einige Office 2007 Tools erstmal ohne Key, also als Trial (25x kann man alles öffnen) installiert. Bisher habe ich Sie auch nicht gebraucht\u0026hellip; und nun das! Beim Download Versuch des PDF/XPS Add-Ins werde ich abgewiesen\u0026hellip;\n\nNaja, dann doch über den internen Fileserver :-S Einfach nur ärgerlich, ob das wirklich MS oder dem Kunden weiter hilft.\nCiao Marco\n","date":"2007-11-28","permalink":"https://marcoscheel.de/2007/11/28/genuine-validation-failed/","tags":["xArchived"],"title":"Genuine Validation failed"},{"content":"Endlich ist man am \u0026ldquo;Ende\u0026rdquo; des Tags angekommen und möchte zumindest noch das fertig heruntergeladene VS2008 installieren\u0026hellip; und dann so was\u0026hellip; keine Files da? Irgendwie hat der Download Manager was von \u0026ldquo;Virtualized\u0026rdquo; oder ähnlichem gesagt\u0026hellip;. hätte ich den Folder doch nicht zu gemacht :-(\nEin kurzer Weg zu Google hat folgenden Link hervorgebracht:\nhttp://kb.adobe.com/selfservice/viewContent.do?externalId=kb400534\u0026amp;sliceId=1#FindVFolder\nIch würde sagen es macht es im ersten Schritt nicht \u0026ldquo;sicher\u0026rdquo;, sondern es verunsichert.\nDanke Google und Danke Adobe\n","date":"2007-11-19","permalink":"https://marcoscheel.de/2007/11/20/vista-vs2008-download-akamai-und-wo-sind-meine-dateien/","tags":["xArchived"],"title":"Vista, VS2008 Download, Akamai und wo sind meine Dateien?"},{"content":"Wie bei jedem Release einer neuen Visual Studio Version stellt sich erneut die Frage, ob die neue und die alte Version zusammen leben können. Der Mann der es wissen muss, hat dazu eine klare Aussage:\nNote that VS 2008 runs side-by-side with VS 2005 - so it is totally fine to have both on the same machine (you will not have any problems with them on the same box). Source: [ScottGu](http://weblogs.asp.net/scottgu/archive/2007/11/19/visual-studio-2008-and-net-3-5-released.aspx)    Neben dem Kommentar gibt es noch andere Interessante Info\u0026rsquo;s im Überblick. Durch das neue Multi-Targeting Feature hoffe ich, nach der nächsten Neu-Installation nur noch ein Microsoft Visual Studio installieren zu müssen.\nCiao Marco\n","date":"2007-11-19","permalink":"https://marcoscheel.de/2007/11/19/vs-2008-und-vs-2005-side-by-side/","tags":["xArchived"],"title":"VS 2008 und VS 2005 Side-By-Side"},{"content":"Eben wollte ich eigentlich nur einen Download aus dem MSDN Subscriber Bereich machen, da ich heute Nachmittag Probleme hatte auf die Seiten zu kommen. Ich wurde mit folgender Fehlermeldung begrüßt:\n\nUnd eben hatt es dann geklappt, als über bink.nu dann die Nachricht kam, dass das .NET Framework 3.5 Released wurde, was eigentlich nur bedeuten kann, dass auch das zugehörige Visual Studio online sein muss. Siehe da\u0026hellip; heute ist Release Day :-) Und die Fehler waren eventuell Vorboten des Go-Live.\n\nFür den Download benötigt man den Downloadmanager von Akamai\u0026hellip; die wohl dafür sorge tragen, dass selbst an so einem Tag ich es schaffe meine 6MBit voll auszulasten :-) Und man sollte seinen Popup-Blocker (IE7) mal eben ausschalten.\n\nSchade\u0026hellip; beinahe hätte ich sogar Bink.nu überholt - Link.\nIch hatte leider wenig Zeit die Beta mitzumachen, also freue ich mich um so mehr, das Ganze gleich produktiv einzusetzen. So jetzt aber zurück an die Arbeit :-(\nCiao Marco\n","date":"2007-11-19","permalink":"https://marcoscheel.de/2007/11/19/die-nchste-generation-ist-da-visual-studio-2008/","tags":["xArchived"],"title":"Die nächste Generation ist da – Visual Studio 2008"},{"content":"Für das Abrufen der Zugriffzaheln auf einen SharePoint Farm gibt es erstmal zwei Möglichkeiten: 1. Die eingebauten \u0026ldquo;Usage Statistics\u0026rdquo;: Diese sind allerdings immer auf die SiteCollection (SPSite) oder die Site (SPWeb) bezogen. Eine Farm übergreifendes Reporting ist so nicht möglich. 2. Die IIS Logfiles: Die IIS Logfiles können über klassische Tools ausgwertet werden. Sobald die Farm aus mehr als einer Application und mehr als einem Server besteht, muß man die Daten aggregieren.\nBeide Varianten haben also ihre Vor- und Nachteile. Es bleibt noch anzumerken, daß es für die SiteCollection (SPSite) übergreifenden Situationen meist um Intranet Konfigurationen geht sind die klassischen Webreportings selten ausrechend. Zum Beispiel werden User nicht anhand der IP als \u0026ldquo;Unique\u0026rdquo; erklärt sondern anhand der AD-Kennung. Auswertungen anhand von GeoIP-Daten ist in der Regel ebenfalls nicht möglich.\nMein Ansatz: 1. Ein .NET Service holt die IIS Logs von allen Servern ab und läßt Sie mit Hilfe des www.logparser.com in eine Datenbank laufen. Wenn man schon dabei ist, kann man die verarbeiteten Logs auch löschen oder wie in meinem Fall komprimieren. 2. Die Auswertung übernehmen die Microsoft SQL 2005 - Reporting Services (SSRS). Es gibt von Microsoft bereits einige Report Samples zum Thema IISLogs. Diese Vorlagen werden auf Intranetparameter getrimmed oder in einigen Fällen komplett durch eigene ersetzt.\nNachteil: Die Lösung versendet Nachts die gewünschten Reports. Die entstehende Last ist stark vom Report und von den Datenmengen abhängig. Eine Umsetzung als Microsoft SQL 2005 - Analysis Services (SSAS) wäre wünschenswert, um zeitbasierte Reports und generell Ad-Hoc Abfragen zu ermöglichen/beschleunigen.\nVorteil: Kostengünstige Lösung, die stark anpassbar ist.\nAlternative gibt es zum Beispiel folgendes Produkt (selbst nicht getestet):\nIDevFactory Universal SharePoint Manager USPM 2007\n\nhttp://www.idevfactory.com/products/uspm2007/features/sharepoint%20reporting%20farm.aspx\nDas Tools bietet zu dem Preis auch noch mehr an. Bisher gibt es nur wenige Tools die sich darum kümmern. Es geht darum, die Security innerhalb von SharePoint reporten zu können. Also wo in welcher Ebene wurde die Security unterbrochen? Bin mal gespannt, ob ich zu einem Test komme.\nAus das Tools bin ich durch den Post von Joel Oleson\u0026rsquo;s SharePoint Land.\nCiao Marco\n","date":"2007-11-17","permalink":"https://marcoscheel.de/2007/11/18/sharepointmoss-statistiken-securityberblick/","tags":["xArchived"],"title":"SharePoint/MOSS Statistiken + Securityüberblick"}]